---
title:  "Dx toolkit development - ddPCR for contamination assessment"
subtitle: "Assay Biostatistics"
date: '`r format(Sys.time(), "%a %b %d", tz="America/Los_Angeles")`'
always_allow_html: yes
output:
  bookdown::html_document2:
    code_folding: hide
    code_download: true
    toc: true
    toc_depth: 2
    # does this have an effect
    fig_caption: yes
    # this has no effect
    number_sections: yes
    # css: ['../_css/pandoc3.css', '../_css/myMargins.css']
bibliography: [../_bib/_Normalization.bib, ../Refs/ruv/_RUV.bib, ../Refs/ruvm/_RUVM.bib, ../Refs/ddPCR/_ddPCR.bib, ../Refs/Contam/_contam.bib, ../Refs/LoD/_LoD.bib, ../../Refs/AI/_AI.bib]
csl: ../_csl/cell-numeric.csl
link-citations: true
---

../Refs/ruv/_RUV.bib


`r DT_DATATABLE <- T`

<!-- Blah, Blah, Bklah -->


<style>
@import url('https://fonts.googleapis.com/css?family=Raleway');
@import url('https://fonts.googleapis.com/css?family=Oxygen');
@import url('https://fonts.googleapis.com/css?family=Raleway:bold');
@import url('https://fonts.googleapis.com/css?family=Oxygen:bold');

.main-container {
  max-width: 1400px !important;
}

body{
  font-family: 'Oxygen', sans-serif;
  font-size: 16px;
  line-height: 24px;
}

h1,h2,h3,h4 {
  font-family: 'Raleway', sans-serif;
}

.container { width: 1400px; }

caption {
  font-size: 20px;
  caption-side: top;
  text-indent: 30px;
  background-color: lightgrey;
  color: black;
  margin-top: 5px;
}

g-table-intro h4 {
  text-indent: 0px;
}
</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = NA,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      fig.width = 8,
                      fig.height = 4)
```

```{r m5a-GlobalOptions, results="hide", include=FALSE, cache=FALSE}

knitr::opts_knit$set(stop_on_error = 2L) #really make it stop
options(knitr.table.format = 'html')

options(stringsAsFactors=F)

 #knitr::dep_auto()

```
<!-- ######################################################################## -->


```{r m5a-Prelims,  include=FALSE, echo=FALSE, results='hide', message=FALSE} 

FN <- "_M5A-cf_ddPCR_seq"
if(sum(grepl(FN, list.files()))==0) stop("Check FN")

 suppressMessages(require(rmarkdown))
 suppressMessages(require(knitr))

 suppressPackageStartupMessages(require(methods))
 suppressPackageStartupMessages(require(bookdown))

 suppressPackageStartupMessages(require(data.table))
 options(datatable.fread.datatable=F)

 suppressPackageStartupMessages(require(plyr))
 suppressPackageStartupMessages(require(dplyr))
 suppressPackageStartupMessages(require(magrittr))

 # Shortcuts for knitting and rendering while in R session (Invoke interactive R from R/Scripts folder)
 kk <- function(n='') knitr::knit2html(paste("t", n, sep=''), envir=globalenv(),
       output=paste(FN,".html", sep=''))

 rr <- function(n='') rmarkdown::render(paste("t", n, sep=''), envir=globalenv(),
       output_file=paste(FN,".html", sep='')) ##, output_dir='Scripts')

 bb <- function(n='') browseURL(paste(FN,".html", sep=''))

 # The usual shortcuts
 zz <- function(n='') source(paste("t", n, sep=''))

 WRKDIR <- '..'
 if(!file.exists(WRKDIR)) stop("WRKDIR ERROR", WRKDIR)

 # Shotcuts for knitting and rendering while in R session

 # file rmarkdown file management options: cache, figures
 cache_DIR <- file.path(WRKDIR,'Scripts', 'cache/M5A/')
 suppressMessages(dir.create(cache_DIR, recursive=T))
 opts_chunk$set(cache.path=cache_DIR)

 figures_DIR <- file.path(WRKDIR,'Scripts', 'figures/M5A/')
 suppressMessages(dir.create(figures_DIR, recursive=T))
 opts_chunk$set(fig.path=figures_DIR)

 #tables_DIR <- file.path(WRKDIR,'Scripts', 'tables/M5A/')
 #suppressMessages(dir.create(table_DIR, recursive=T))
 #opts_chunk$set(fig.path=table_DIR)
 
 # need a local copy of help_DIR
 #help_DIR <- file.path(WRKDIR,'Scripts', 'help_files')
 help_DIR <- file.path('.', 'help_files')
 suppressMessages(dir.create(help_DIR, recursive=T))
 
 temp_DIR <- file.path(WRKDIR,'Scripts', 'temp_files')
 suppressMessages(dir.create(temp_DIR, recursive=T))

```
<!-- ######################################################################## -->

*** 

```{r m5a-utilityFns, echo=FALSE}
 # Here we define some utility functions
source('utilityFns.r')

```

<!-- ######################################################################## -->

## Synopsis {-}

### Truth {-}

<p><p/>
* Assessment of performance of a data analysis tool to be used for a
particular task requires **truth**.

<p><p/>
* The dataset studied here has two forms of truth:
   - samples with no contamination
   - samples with a ladder of contamination levels 


   - Question: *how do we ensure that there is no contamination in these samples?**
      - this affecgts the truth level


#### Background Contamination {-}

* Batches 1-11 produce results for samples not affected by contamination.
   - how do we know that?
   - Results for samples with no contamination can be summarized to establish
a baseline or background level of contamination
   - Results show that background contaminations are:
      -

<p><p/>
* Baseline samples from batches 1-11 are useful for estimating
the background distributution of metrics in these particular samples
   - if we can interpret these as providing reliable assessments of level of blank -
counts in the absence of contamination, that would put the limit of blank at **XXX**.


<br/>

### Salient Data Features {-}

* Understanding the salient features of the data is the critical first step
in making the best use of the data 

<p><p/>
* In the absence of high contamination, the relationship
between **n_contam_frag** and **n_hom_frag_adj** is flat,
   - ie there is no association between n_contam_frag and n_hom_frag_adj 
   - in this range, the identified contam fragments must come from the host.
   - it looks like up to **20 n_contam_frag** may come from the host,
independent of the amount of fragments from the host. 
      - See pairs plots in Section \@ref(exam-seq-data).
   - for batches 12 and 13, relationships or associations between metrics
may be enhanced or obscured by the varying titration level.
      - plot stratified by titration level?

* ... add more here

 <!--
   - we can examine the titrations in Batches 12 and 13; the contam
fragments at the very low  end might respond to changes in contam levels
but if the counts are below the limit of blank (?) it will have no clear
evidence of a response.
 -->


<br/>
<br/>

### Inference {-}

* At some point, sometimes from the very beginning, but more wisely after
absorbing what we can learn from this one dataset, we will want to think
about making inference to future data.


* At that point we need to consider  some statistical framework.
* Giving the classical concepts a new look, Yu and Kumbier (2018) [@Yu:2018aa] write 

> Artificial intelligence (AI) is intrinsically data-driven.
> It calls for the application of statistical concepts through 
> human-machine collaboration during generation of data, development
> of algorithms, and evaluation of results. This paper discusses how
> such human-machine collaboration can be approached through the
> statistical concepts of population, question of interest, representativeness
> of training data, and scrutiny of results (PQRS).^[
See mention by [Andrew Gilman](https://statmodeling.stat.columbia.edu/2017/12/07/bin-yu-karl-kumbier-artificial-intelligence-statistic/)].


<p><p/>
* With the data we are studying here, we also need to think of PQRS:
   - where do the data come from?
   - what sampling mechanism can we assume to be not too implausible?
   - what generalizations can we entertain?
<!-- that are not too far fetched? -->

<p><p/>
* We have more than one run so that inference to other runs is possible,
but these other runs would be just like the ones in this data set.
   - ie. we can make statistical inference to a hypothetical larger (ie. population)
dataset, or to future datasets of the same sort
   - beyond the hypothetical population from which the study dataset
was drawn, there is no statistical inference possible, but generalizability can be
postulated if there is ground to do so.


<br/>
<br/>

### TODO {-}

* LoD needs to be defined and agreed on.  Two different concepts of LoD
are in use in relation to conta: Section \@ref(conta-detect-lod).
* Simulating ddPCR data according to the partitioning model
does not produce counts which follow the expected binomial and 
Poisson probability laws. See Sections \@ref(simg-partg) and 
\@ref(part-stats).
   - See Loskyll et al. (2023) [@Loskyll:2023aa] and references therein for
example simulation setups.

<p><p/>
* This task may provide a tremendous opportunity to learn about the character of conta,
if we were interested in learning more about the character of conta.

<br/>
	

## Parameter Setting {-}


```{r set-params}

params <- list(
 conta_ddPCR_root = "../../../extData/conta_ddPCR",
 analysis_dir = "20241017 - Data summaries",

 design_file  = "2024SepOct_all runs to compare ddpcr and seq_.xlsx",
 cp_design_file  = "Cp2024SepOct_all runs to compare ddpcr and seq_.xlsx",
 seq_data_file = "20241017_seq data for conta.xlsx", 
 ddpcr_data_file = "20241017_ddpcr data for conta.xlsx",
 seq_ddpcr_metadata_file = "20241017_metadata_conta_ddpcr_seq.xlsx",
 seq_ddpcr_metadata_sheet = "data w B002new",

 # previous work - GPS Conta assessment (From Ting-Chun Liu)
 # https://docs.google.com/presentation/d/170thhwNkniTo73xlW1uNRQP5eeEV_9dvCoCJCyTHCMg/edit#slide=id.gf9cd69f3f6_0_264
 seq_gps_conta_file = "Misc/20211014_control_titration_conta.csv",

 # Dorazio
 Dorazio_2015_supp_dir = "../Refs/ddPCR/Dorazio_2015_supp",
 Dorazio_dilution_file = "dilutionData.csv",
 Dorazio_CNV_file = "BioRadData-CNV.csv",
 Dorazio_geneExpress_file = "geneExpressData.csv",
 
 
 analysis_Prefix  = FALSE,

 empty = "Last Spot"
)

```

```{r m5a-input-params-all, echo = FALSE, results = "asis", eval=T, echo=F}

 # print out original input params
data.frame(
param_name=names(params),
param_value=unlist(params),
param_class=sapply(params, class), row.names=NULL) %>%
  knitr::kable(caption="Input Parameters")

```


<br/>
<br/>

# Introduction  {#intro}

<p><p/>
* The goal of this task is to evaluate the capabilities of the `ddPCR` platform in
detecting low levels of contamination so that it can be used to identify
contaminating events by assaying samples stopped early in the processing pipeline.
See Figure \@ref(fig:m5a-design-img0).


`r CAPTION <-
paste(   
"contam ddPCR is run on cfDNA coming out of EXTR.  See arrow in the diagram which also shows the locations of 15 stopping points."
)`       
       
```{r m5a-design-img0, fig.cap = CAPTION}
knitr::include_graphics("img/MCED-I workflow with 15 Safe Stopping Points.png", dpi=100)
 
``` 

<br/>
<br/>

## Analysis Plan

* If we had an analysis plan, the primary analysis section would
describe how the collected data will be used to characterize
each platform, seq and ddPCR, in terms of attributes
which can play a role in contamination detection.

<p><p/>
* Given the data that have been collected, the following summaries/analyses
should be part of the plan:
   - use the background batches, batches 1 to 11, to characterize properties
of the data from each platform in the contexts represented by these batches[
Batches 1-11 are a selected set, with different batches being representative
of different and specific contexts.]
      - the level and variability of candidate conta-seq and conta-ddPCR
contamination indexes or metrics
  - use the titration batches to assess each platform's ability to
distinguish among the low titration levels.
     - we could also look at the precison of estimated titration levels by
calibration, keeping in mind that the end of the range of contamination levels
are or primary interest.

* These analyses assess each platform by itself.  
   - it's not clear how we can interpret differences between the two
plaform without having a good grasp of the variability and bias within
each platform.

<br/>
<br/>

## Report Outline 

* Section \@ref(conta-algorithm) describes the conta algorithm and
features of the conta data.  The limit of detection for contamination
detection is an important feature that **needs clarification** - See Section
\@ref(conta-detect-lod)

* Section \@ref(ddpcr-platform) provides references for an introduction to ddPCR
and describes the partitioning statistics and data model which provide a framework
for the analysis of ddPCR data.

<p><p/>
* In Section \@ref(ddpcr-seq-dilution) a curated dataset which combines
ddPCR and Seq data is analyzed.
The primary end-points of the analyses in the section are characterizations
of the data form each platform as they relate to contamination detection:
   - results from batches 1 to 11 provide data about levels of background;
the level and variability of reported contamination levels when there is
no contamination.
      - <span style="color:blue;"> How do we ensure that there
is no contamination in these runs? </span/>
   - results from batches 12 and 13 provide data about the sensitivity
and specificity of contamination estimates, or how closely to 
estimates track known differential levels of contamination.
      - of particular interest is the region which crosses over
the upper boundary of the region spanned by the background levels
of estimated contamination.

* The next two sections, \@ref(data-collect-design) and \@ref(sample-qc),
describe and load the data collected for the task of evaluating
ddPCR data for contamination assessment.  Some QC metrics are collected
but no filtering of samples is done at this point.

* In Section \@ref(exam-seq-data)  and  \@ref(seq-data-analysis-dil-series)
we load and examine the sequence data and analyze the dilution series data.

* Sections \@ref(exam-ddpcr-data) and \@ref(ddpcr-data-analysis-dil-series)
do the same with the ddPCR data.



<br/>
<br/>

# The conta algorithm  { #conta-algorithm .tabset}

## Home {-}

<br/>
<br/>

## Algorithm Desctiption


<p><p/>
* What does Conta V3 do?
   - contamination is quantified by tracking 
contaminants among a selected set of target regions - 
the **Conta V3 Variants/Regions**.

   -  For a carefully selected subset of targets - 1000 specially selected
targets on the seq platform - we can identify and count contaminants 
among all fragments in the selected target subset.
the primary metric in the Conta V3 methodology is CF:

\begin{equation}
CF = \frac{\text{num_contamination_fragments}}{\text{num_hom_fragments_adj}}
(\#eq:cf-def)
\end{equation}

   -  The **denominator** is the number of fragments from potential contaminating samples
(ie. adjacent) which are associated with tracked targets; the fragments
must be associated with targets containing 2 SNPs and have a homozygous 
genotype.

   - The **numerator** is the number of fragments known to  be contaminants.
To be identified as a contaminant, the fragment must be associated with one
on the tracked targets, have the same genotype as the tracked adjacent target,
and the sample must have the opposite allele genotype as the adjacent.

<p><p/>
* What is the Conta V3 performance?
   - Through target selection the sensitivity is increased 10-100X.
      - **link to data here**
   - The increased sensitivity might come at a price in terms of accuracy.
      - can a similar analysis be run on a representative set of targets?
      - **link to data here**

<p><p/>
* How was conta calibrated?
   - how to we go **from** *contamination in the Conta V3 Variant Targets*
**to** contamination in the full set of abnormal targets, and to an inference
on the effect of contamination on test sample results?
      - **include reference to data here**

<br/>
<br/>


## Background 

<p><p/>
* Almost everything there is to know should be on or linked on the
[V3 Conta Confluence page](https://confluence.ti-apps.aws.grail.com/x/_iYuBg) 
<p><p/>
* In particular, the following presentation slide decks are included in the confluence page:
   - [22_03_18_Intro to Conta-Source V&V and Rgt Dev Inter-Team Meeting](https://docs.google.com/presentation/d/1XoNaj-sJiMOKESpKx_40agquTm0rK9BCp8lfzCV1YMc/edit#slide=id.g11e1f47718c_0_25).  Slide 11 illustrates the standard notion of LoD  (see below)
   - [ContaV3 Update (Part I) (2021)](https://docs.google.com/presentation/d/1Hr0XL50yPqQ0d1PsV19gig0IHL-MXXAs5vSCtqU2EYo/edit#slide=id.p)
   - [Conta Update and Development Plan (2019)](https://docs.google.com/presentation/d/13du1tVf60R2BTcjCgJQpJvpGSO0hcnqwTabo-k7Q8ZU/edit#slide=id.g224d66e4ca_0_305)
   - [200910_Doppler_Conta_Assessment](https://docs.google.com/presentation/d/1SL1hAwJ2pmtMVOd_PKoKrpDVqZWq32BCpm7RvVvovyE/edit#slide=id.g224d66e4ca_0_305)

<p><p/>
   - [Conta: Methods for detecting trace amounts of contamination](https://grail.com/wp-content/uploads/2020/12/HiTSeq_2017_Sakarya_Conta_POS_Final.pdf)
      - the publication about Conta (and associated [R package](https://github.com/grailbio/conta))
      - This method is conta V1 (see below)

	<p><p/>
* Several Rmarkdown reports accessible from RSconnect report on analyses relating to
contamination detection
   - Reports on CF estimation in the contexrt of the Doppler panel:
   - [Doppler Contamination Estimate with Singletons Phase 2 - Split Bags](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/5dc3f9f7-62be-428d-82e8-70b6b6b54868/access)
   -  [Doppler Contamination Estimate with Singletons Phase 2](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/90a50c84-14f2-44df-b437-9dc61507386d/access)  
   - [Doppler Contamination Validation](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/a86ac37c-f8b9-4314-9324-5f2390c15a5c/access)
   - [Contamination Estimate with Singletons](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/6cb4b757-a0c7-48be-b8ee-2a5e3aee239d/access)
     * Brief exploration of CF in TDG 1 and 2
        - [RTP Scaled Integration - TDG2 Conta Fraction Data](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/ef47d0e6-5565-4471-9415-72c15637f38d/access) 
           - should the negative correlation between CF and n_hom be corrected?  Should be
hom_adj...
        - [RTP Scaled Integration - TDG1 Conta Fraction Data](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/47e20cad-069b-4351-836e-a5a72ebfad72/access) 


<br/>
<br/>

## Conta V1, V2

This meterial from  [ContaV3 Update (Part I) (2021)](https://docs.google.com/presentation/d/1Hr0XL50yPqQ0d1PsV19gig0IHL-MXXAs5vSCtqU2EYo/edit#slide=id.p)

<!--
* Contamination Detection == Find unexpected haplotypes at a frequency
which exceeds what is expected from sequencing error alone.
   - the detection algorithm.../

-->

<p><p/>
* Conta V1
   - Based on the standard approach of using SNPs
   - Accounts for sequencing error through explicit modeling
      - See poster: [Conta: Methods for detecting trace amounts of contamination](https://grail.com/wp-content/uploads/2020/12/HiTSeq_2017_Sakarya_Conta_POS_Final.pdf) - the publication about Conta (and associated [R package](https://github.com/grailbio/conta))
<p><p/>
* Conta V2
   - Same variants as V1 but uses machine learning to model sequencing error

<p><p/>
* Limitations of these earlier versions
   - Sequencing error rate is ~1e-3
      - => contamination LoD is ~1e-3
   - MM Classifier has an LoD ~ [1e-5, 1e-4]

<p><p/>
* As contamination can affect the classifier training process,
we  need to have a lower contamination LoD!

<br/>
<br/>




## Conta V3 


This tab and the next are based on the
[V3 Conta](https://confluence.ti-apps.aws.grail.com/x/_iYuBg)
Confluence page.


> Conta V3 uses variants that are more complex than SNPs to avoid being limited by the sequencing error rate for single nucleotides. In the panels Doppler, Echo, Floodlight and GPS-Large, these are 500 Indels, and 500 pairs of SNPs within 10 bp of each other, and in GPS-Small, there is a subset of 500 of these.
> 
> For each of these variants, the haplotypes are called for each fragment, and then **any deviations are counted as contamination in sites that are called as homozygous for the sample.**
> 
> For cases where we want to only quantify the contamination and we expect the contamination to be less than 3% (low enough that they won't interfere with calling a variant site as homozygous), we use a simple method of counting such contamination fragments and then computing a point estimate of the contamination fraction. The graphic below illustrates the method for counting contamination fragments and estimating contamination fraction.
> 

`r CAPTION <-
paste(   
"Conta V3 method for counting contamination fragments and estimating contamination fraction."
)`       
       
```{r m5a-conta-v3-method-fig, fig.cap = "ContaV3", out.width = "800px", fig.align = 'center'}
knitr::include_graphics("img/V3 Contamination Estimate Example") ###dpi=100)
 
``` 

* Several issues to consider:
   - When do we need to bring the population allelic fractions into the picture?
      - The population description is not needed when we are interested in
instances of contamination in a set of processed samples.
   - The inference that we need to consider is how we generalize 
from a contamination pattern observed  in the Conta V3 variant targets 
to the expected contamination  in all of the fragments and the
impact the contamination will have on classification results.
   - Making this inference requires collecting more data so that
contamination rates in the Conta V3 fragments can be translated into 
contamination rates in the remaining fragments.

<br/>
<br/>

## CF - Estimation


>
> For any given sample, detection **depends on the number of unique molecules 
> that overlap with ContaV3 variants**.  Mathematically, contamination 
> events are given by a binomial distribution with n = num_hom_fragments_adj 
> and p = contamination_fraction, where num_hom_fragments_adj is a value
> available as a metric (see below) for the sample.
> A rule of thumb is that **95% confident Limit of Detection (LoD95) 
> is given by 3 / num_hom_fragments_adj**. For a typical GPS sample,
> LOD95 is expected to be around 5E-5.

* The primary metric in the Conta V3 methodology is CF:

\begin{equation}

CF = \frac{\text{num_contamination_fragments}}{\text{num_hom_fragments_adj}}


(\#eq:cf-def-2)
\end{equation}

* The denominator is the number of fragments from potential contaminating samples
(ie. adjacent) which are associated with tracked targets; the fragments
must be associated with targets containing 2 SNPs and have a homozygous 
genotype.

* The numerator is the number of fragments known to  be contaminants.
To be identified as a contaminant, the fragment must be associated with one
on the tracked targets, have the same genotype as the tracked adjacent target,
and the sample must have the opposite allele genotype as the adjacent.

<br/>
<br/>

## Conta Detection - LoD  {#conta-detect-lod}


* In order to properly evaluate the value of ddfPCR for contamination related
tasks, we need a clear and useful definition of `Contamination Detection`
on the sequencing platform. Armbruster and Pry (2008) [@Armbruster:2008aa] 
describe the concepts of LoD, LoQ and limit of blank, clearly snd succintly. 

<p><p/>
* The [V3 Conta](https://confluence.ti-apps.aws.grail.com/x/_iYuBg)
Confluence page) states that Limit of Detection (LoD95)
is given by `LoD = 3/ num_hom_fragments_adj`.  
   - **this reqires some explaining**.


<p><p/>
* `3/num_hom_fragments_adj` is the 95% upper bound for the population
proportion of SNP fragments that have contaminated the analysis sample^[
Refer to the sample for which we are seeking to assess the 
contamination level  as the `analysis sample`]
given that no contaminant is detected in the analysis sample.
   - This construct does not tell us where the true rate of contamnation is;
the only claim made for the confidence interval is in relation to the
procedure used to collect data and construct an interval from the results,
according to the specified procedure.  A "95% confidence interval"
is a claim about the frequency with which these intervals will
include the true value.  

<p><p/>
* CLSI EP17-A2 (2012) [@EP17A2:2012aa] definition of limit of detection (LoD)
   - measured quantity value, obtained by a given measurement procedure,
for which the probability of falsely claiming the
absence of a measurand in a material is $\beta$ (ie. power is 1 - $\beta$),
given a probability $\alpha$ of falsely claiming its presence.


<p><p/>
* Other LoD definitions:
   - [LOD_LOQ_guidance document_food contaminants_2016](https://food.ec.europa.eu/system/files/2017-05/animal-feed-guidance_document_lod_en.pdf):
Limit of detection: Analyte content which can be distinguished from the blank with an error probability of (1-ß)
      - The concentration at which the analyte can distinghuished from blank 95% of the time.
      - This notion requires the limits of blank be established.
   - Klymus et al. (2020) [@Klymus:2020aa] 
LOD:  The lowest standard concentration of template DNA that produces at least 
95% positive replicates. 
   - Wang from FDA (2021) [@Wang:2021aa]
The LOD of a qualitative microbiological assay is defined as the level of contamination that leads to a positive result with a specified value of the probability of detection.


These definitions are more standard and in line with the procedure
implictly specified in [Slide 11 of 22_03_18_Intro to Conta-Source V&V and Rgt Dev Inter-Team Meeting](https://docs.google.com/presentation/d/1XoNaj-sJiMOKESpKx_40agquTm0rK9BCp8lfzCV1YMc/edit#slide=id.ge800820d1a_0_467).

<br/>

* Vaks (2018) [@Vaks:2018aa] proposes an alternative to the costumary
probit analysis method of estimating an the LoD in the context of qPCR

<br/>
<br/>



# The ddPCR Platform {#ddpcr-platform .tabset}

* The results of the simulations show that the binomial model
does not lead to appropriate estimated standard errors -
ie. confidence intervals computed based on these SEs do not have
the correct coverage frequencies.
   - Return to this later

## Home {-}

<br/>
<br/>

## ddPCR Bibliography  {.tabset}


### New {-}

<p><p/>
* The first set of ddPCR references listed below  -  
Basu et al. (2017), Pinheiro et al (2012), Dube et al (2008),
and Kreutz et al (2011) [@Basu:2017aa;@Pinheiro:2012aa;@Kreutz:2011aa; @Dube:2008aa]  -
all use the fundamental partitioning statistics 
as the basis of analyses customized to two problems: 
   - the estimation of the concentration of a target in solution, and 
   - the estimation of the ratio of the concentration of
two analytes in solution.

<p><p/>
* Dorazio et al. (2015), Burdukiewicz et al. (2016) and Uhlig et al. (2018) 
[@Dorazio:2015aa; @Burdukiewicz:2016aa; @Uhlig:2018aa]
extend the framework of partitioning statistics and describe methods which
support a more comprehensive set of analyses.
Dorazio et al. (2015) [@Dorazio:2015aa] illustrate the use of the
GLM methodology for the analysis of ddPCR data to address questions that go beyond
comparing two concentrations. Example analyses from Dorazio et al. (2015) [@Dorazio:2015aa]
are included in  Section \@ref(ddpcr-analysis-glm). 

<p><p/>
* Simulated partitioning in Section \@ref(simg-partg) indicate that the
estimated uncertainty derived from the partitioning statistics from Basu et al.
are incorrect, or there is a mismatch between the simulated data and the data to which
the partitioning statistics apply to.  

<p><p/>
* Loskyll et al. (2023) [@Loskyll:2023aa] also indicate that the binomial and Poisson
models may not faithfully capture the variability in qqPCR data.  The paper
details how to derive statistics which are  more appropriate for modeling the 
variability in ddPCR data.

<p><p/>
* Loskyll et al. (2023) [@Loskyll:2023aa]  provide a number of references to
articles addressing issues relating to the analysis of ddPCR data:
[@Jacobs:2014aa; @dMIQE:2020aa; @dMIQE:2020ab; @Debski:2016aa; @Majumdar:2015aa; @Jacobs:2014aa]

<p><p/>
* Chen et al. (2023) [@Chen:2023aa] also point out shortcomings of the use of
models which assume binomial variability and propose some 
bootstrap estimates of variability instead.

<p><p/>
* The European Network of GMO Laboratories (ENGL) technical report
*Overview and recommendations for the application of digital PCR*
([@ddPCR_ENGL:2019aa]) is a good reference assembled by a consortium.
  - See Section 6.4.7  
**Limit of Detection (LOD), Limit of Quantification (LOQ)**.

<p><p/>
* Tellinghuisen (2020) [@Tellinghuisen:2020aa] goes through the details of how
Poisson variability dictates the accuracy of estimates obtained by qPCR and dPCR
so that at very low concetrations, a higher volume qPCR assay may be a better option.



<br/>
<br/>

### First pass {-}


<p><p/> 
* Basu (2017) [@Basu:2017aa], Whale et al (2013) [@Whale:2013aa], 
Kreutz et al (2011) [@Kreutz:2011aa], Pinheiro et al (2012) [@Pinheiro:2012aa],
and Dube et al (2008) [@Dube:2008aa] all discuss some statistical 
aspects of ddPCR data.
   - Basu's Partitioning Statistics is reproduced in Section \@ref(part-stats).
   - Dube derives a mathematical framework to calculate the true concentration of 
molecules from the observed positive reactions in a panel and find the 95% 
confidence intervals of the true concentrations and the ratio of two concentrations in a CNV experiment using the digital array with multiplex PCR.


<p><p/>
* From BioRad
   -  [Qx600 Droplet Digital PCR System](https://www.bio-rad.com/sites/default/files/2023-08/Bulletin_3557.pdf)
      -  Absolute Quant with 0.1% sensitivity = 1/1000 droplets  
   -  [QX600 AutoDG Droplet Digital PCR System](https://www.bio-rad.com/en-us/product/qx600-autodg-droplet-digital-pcr-system?ID=0d9e49a4-cf92-f9a9-5779-ef412180f1ed)

<p><p/>
* Pinheiro et al. (2012) [@Pinheiro:2012aa]  is a showcase article written jointly
by the National Measurement Institute of Australia and Bio-Rad.
   - Experimental data from independent sets (such as independent gravimetric dilutions) 
and replicate measurements was combined using the pooled relative standard deviations:

\usepackage{amsmath,xcolor}
$$\frac{SD_{M_{prec}}}{\bar{M}} = \sqrt{\frac{\left(\frac{SD{M_1}}{M_1}\right)^2(n_1-1)+
\dotsb + \left(\frac{SD{M_L:1}}{M_L}\right)^2(n_L-1)}{n_1 + n_2 + \dotsb + n_L - L}}$$

   - similartly for CNR (copy number ratio SD)
   - The relative standard uncertainty of the precision data was then determined
by dividing the pooled relative standard deviation by the square root of the
number of data sets as shown in eqs 6 and 7, respectively:

\begin{equation}
 \frac{u_{M_\text{prec}}}{\bar{M}} = \frac{SD_{M_\text{prec}}}{\bar{M} \sqrt{i}}
\end{equation}


<br/>

* Quan et al (2018) [@Quan:2018aa]
   -  compares the fundamental concepts behind the quantification of nucleic acids
by dPCR and quantitative real-time PCR (qPCR).
   - provides some **detail the underlying statistics of dPCR** and 
explains how it defines its precision and performance metrics and 
discuss design decicions relating to droplet size and number.
   -  discusses hoe to determine whether the theoretical advantages of dPCR over qPCR hold true
by pursuing studies that directly compare assays implemented with both methods.


<p><p/> 
*  Taylor (BioRad) et al (2017) [@Taylor:2017aa]   
   - Droplet Digital PCR (ddPCR) and qPCR platforms are directly compared
for gene expression analysis using low amounts of purified,
synthetic DNA in well characterized samples under identical reaction conditions. 
   -  for sample/target combinations with low levels of nucleic acids (Cq ≥ 29) 
and/or variable amounts of chemical and protein contaminants,
ddPCR technology will produce more precise and reproducible results
* main difference ddPCR vs qPCR:
   - partitioning of the PCR reaction into thousands of individual reaction vessels 
   - acquisition of data at reaction end point
<p><p/>
   - Experimental design
      - A single reaction mix was for each sample and for 
all experiments which was split (20 μL each) for data acquisition 

<p><p/>
*  Stephens et al (2018) [@Stephens:2018aa] 
   - Describes a Droplet Digital PCR (ddPCR) protocol and a 
bioinformatic solution for the detection of contamination in patient’s
samples and derived sequencing data, which are based on the same principle:
      - detection of alternative alleles for single-nucleotide polymorphisms (SNPs)
that are homozygous according to the control (germline) sample.


<p><p/>
* Attali et al (2016), Brink et al (2018) [@Attali:2016aa; @Brink:2018aa] 
   -  Two R packages
      -  [ddpcr](https://cran.r-project.org/web/packages/ddpcr/vignettes/overview.html)
      -  [ddPCRclust](https://www.bioconductor.org/packages/release/bioc/html/ddPCRclust.html)  

<br/>
<br/>

### Chen et al. (2023)  {-}

*  Variance estimation of a ratio between targets is 
particularly problematic in dPCR experiments.

<p><p/>
* Dube et al (2008) [@Dube::2008aa]  provided two methods for 
constructing confidence intervals for CNV. One way is to build 
a histogram of the ratio by binning and adding up the joint probabilities 
of all possible combinations of concentrations in each bin of the sampling distributions of the target and reference concentration

* Another method is to use explicit formulas for standard errors

* Whale et al. (2013) [@Whale:2013aa] first log transformed the ratio and
derived the confidence interval, then the estimates are backtransformed to the
original scale. This method makes the assumption that the distribution of
the log-ratio is (approximately) normal

* Vynck et al. (2016) [@Vynck:2016aa] 
provides a flexible method to account for between-replicate variability
by fitting a generalized linear mixed model. This method incorporates the
between replicate variability as a random effect in the statistical model. 

* A limitation of some of the above mentioned methods is that they rely
on the assumption of independence of the two variables in the ratio.
<!--
However, when there is a correlation between the variables, the independent
sampling or approximation formulas will be incorrect. For example,
dPCR methods aiming to quantify linkage disequilibrium or DNA quality,
e.g. the DNA shearing index (DSI), DNA fragmentation [13] will result
in a correlation between the measured variables, as these assays are
intended to quantify the ratio of linked versus unlinked sequences.
-->

* This paper will focus on methods for variance (or standard error) estimation
and confidence intervals for the target parameter of a dPCR experiment
(e.g. absolute quantifiction, CNV, …). To tackle the above problems,
we propose three methods:
   - 2.1. A Conditional Bootstrap Method for Standard Errors
   - 2.2. Binomial Bootstrap for Standard Errors
   - 2.3. A Simple Nonparametric Estimator of the Variance






<br/>
<br/>

## Partitioning Statistics {#part-stats .tabset}

Partitioning statistics are the basis of ddPCR data analysis.
Basu et al. (2017) [@Basu:2017aa] provide a comprehensive introduction to the topic which
is summarized below.  

Also see Chen et al. (2023) and Dube el al. (2008) [@Chen:2023aa; Dube:2008aa].



### Home {-}

<br/>
<br/>

### PS Fundamentals {#basu}

* In this section we summarize the fundamentals of Partitioning Statistics 
presented  in Basu et al. (2017) [@Basu:2017aa].
These remarks are applicable to the context of measuring a single analyte held in solution
at a fixed concentration.


`r CAPTION <-
paste(   
 "Principles of digital PCR.  Each sample is 
partitioned into many independent aliquots or partitions, with each
containing a few or no target sequence. (Source [@Quan:2018aa])"
)`
 
```{r m5a-Quan-2018-fig-3, fig.cap = CAPTION,  out.width = "800px", fig.align = 'center'}

knitr::include_graphics("img/Quan_2018_fig_3.png")
 
``` 


<p><p/>
* Let 
   - m  = total number of targets in the sample 
   - C  = sample concentration = $\frac{m}{\text{units of Volume}}$  
   - n  = number of partitions or droplets  
   - $V_p$  = partition volume
   - $V_s$  = sample volume
   - $\lambda$  = expected number of targets per partition  

***

<p><p/>
* In digital assays, the number of targets in the sample (**m**)
is typically less than or on the order of the number of partitions (**n**).
and there is significant statistical variation in the number 
of molecules per partition. 
   - in what follows we asssume the all partitoions have the same size
which gives rise to the `1/n` term in the equations.

* The **average number of targets per partition** ($\lambda$) depends on 
the sample concentration (C), the number of partitions (n) and sample volume.
The following equalities hold:

\begin{equation}
\lambda = \frac{m}{n}
(\#eq:lambda-m-n)
\end{equation}

\begin{equation}
m = C \cdot V_s
(\#eq:m-C-Vs)
\end{equation}

\begin{equation}
V_p  = V_s/n
(\#eq:m-C-Vs)
\end{equation}

where $V_s$ is the sample volume and $V_p$ is the targeted partition volume
(which will have some variability).


$$\lambda = C \cdot V_p$$ 

* Finally, to estimate concentration:

\begin{equation}

C = \frac{\lambda}{V_p}   
(\#eq:conc-est-1)
\end{equation} 


<p><p/>
* To get at the distribution of the number targets per partition,
we note that by definition when partitioning is done at random,
each target can be assigned to each partition with equal probability
and the assigments are independent between targets.  

* For a fixed partition, the probability that `k` targets were assigned
to that partition is given by the binomial distribution with
`p = 1/n` and `N = m`:

\begin{equation}

p(k) =  \binom{m}{k} \left(\frac{1}{n}\right)^k 
                       \left(1 - \frac{1}{n}\right)^{m-k} 


(\#eq:binomial-1-over-n-m)
\end{equation}

$$ p(k) = \binom{m}{k} \left(\frac{\lambda}{m}\right)^k 
                       \left(1 - \frac{\lambda}{m}\right)^{m-k}  $$

The second parametrization is used to emphasize the relationship with the mean
number of tatgets per droplet, **$\lambda$**, and the total number of targets,  **m**.


* The binomial paremeters are `p = 1/n`, which is small, and
 `N = m`, which is large, so that the binomial distibution
given by \@ref(eq:binomial-1-over-n-m) is well approximated
by a Poisson distributoion with parameter
$\lambda = E(X) = N \cdot p = m/n$:

$$ p(k) = \frac{\lambda^k e^{-\lambda}}{k!} $$
 
with mean and variance both equal to $\lambda$.  

* Let E denote an empty partition^[
E is also used to denote the proportions of empty partitions]: 

$$E = p(0) = e^{-\lambda}$$


So that $\displaystyle \lim_{\lambda \to \infty} E = 0$.
In other words, quantitation becomes impossible  when $\lambda$ gets
to be too large.

<p><p/>
* From $\lambda = -ln(E)$, putting $\lambda \approx m/n$ and
$E \approx c/n$ where $c$ is the number of observed empty
partitions, we have:

\begin{equation}
m = n\lambda = -n \cdot ln(E)     \\
m \approx n \cdot (ln(n) - ln(c))     \\
c \ge 1 \Longleftrightarrow m < n \cdot ln(n)
(\#eq:target-upper-range)
\end{equation}

<p><p/>
* In other words, to statistically have at least
one empty partition, the number of targets 
must be less than $n \cdot ln(n)$.

* Substituting `-ln(E)` for $\lambda$ in Equation \@ref(eq:conc-est-1) yields this
result:

\begin{equation}
 C =  - \frac{ln(E)}{V_p}
(\#eq:conc-est-2)
\end{equation} 


<br/>
<br/>

### Sources of Error  {.tabset}

The modeling and analysis of ddPCR data must properly account for
the known sources of variability:

* variability due to subsampling from the source sample (donor tube, for example)
   - we can only observe this with replicated subsamplings. ie. draw 
several aliquots from the source
   
<p><p/>
* variability due to the random allocation of targets to partitions which has 
two sources of variability:
   - targets are randomly allocated to a bin or partition 
   - the bin size may differ from bin to bin



#### Home {-}

<br/>
<br/>

#### Subsampling Error  {-} 


* Subsampling errors arise in any biological assay (digital or analog),
which does not analyze the full volume of the sample, but rather a subsample of it, 
resulting in statistical variation between replicate tests. 

<p><p/>
* When the measured quantity depends on a number of targets and
the number is low, a considerable amount of variability can be 
introduced by subsampling.  
   - if the measurement is the number of targets, then the
standard deviation is $\sqrt{m}, $   where m is the expected
number of targets in the processed subsample.
   - there is further variability to consider when the biological sample
is part of a subset collected to make inference to population parameters.  

   
<p><p/>
* When subsampling a fraction of a larger sample, the standard deviation of the
targets per subsample is $\sqrt{m}$.  The normalized measurement uncertainty  due to 
subsampling is given by the following equation: 

\begin{equation}

u_s = z_c \frac{\sigma_m}{m} = z_c \frac{\sqrt{m}}{m} = \frac{z_c}{\sqrt{m}}

(\#eq:subsampling-error)
\end{equation}

   - We can estimate the subsampling variability from the estimate for m, if
subsampling is like selecting targets at random from a large set.
   - Subsampling variability can also be estimated from replicates of the right 
sort,  ie.  resample the tube or source

<!-- 
in more complicated cases SD = sqrt(sum(r^2)/m)
sum(r^2) ~ m * meanR2
SD = sqrt(meanR2)
does not depend on m
-->

<br/>

#### Partitioning Error {-}

*  In a set of replicate experiments, there is a variance in the number of
empty partitions E that propagates to a corresponding variance in the 
calculated concentration, $\lambda$.


* $\sigma_E = \sqrt{E(1-E)/n}$

* The normalized uncertainty due to partitioning is given by

\begin{align}

u_p  & =  \frac{\Delta m}{m}     \\
     & =  \frac{n(\lambda_{max} - \lambda_{min})}{\lambda n} \\
     & =  \frac{1}{\lambda} ln \left( \frac{E + z_c \sigma_E}{E - z_c \sigma_E}\right)

(\#eq:partitioning-error)
\end{align}

Note that $\lambda_{max} = -ln(E -z_c\sigma_E).


<br/>

#### Partition Volume Error {-}

See Pinheiro et al. (2012) [@Pinheiro:2012aa] for a discussion of partition volumes
variability.
   - 1. It's complicated
   - 2. In the data the were  analyzed, the partition volume variability can be ignored.


\begin{equation}

u   = C \sqrt{ (u_p)^2 + \left(\frac{u_{V_d}}{\bar{V_d}}\right)}

(\#eq:part-vol-error)
\end{equation}

* Here, $\bar{V_d}$ is the mean partition volume and C is a coverage factor 
between 2.05 to 2.18, which
*provides a 95% confidence level that the measurement
falls within the calculated uncertainty*^[
Should read `95% confidence that the calculated uncertainty covers the
measurement, or true value being estimated by measurement.].

* As the number of partitions is increased from $10^2$ to $10^5$,
$u_p$ becomes insignificant, and the expanded uncertainty drops from >10% to about 1%. 

* Beyond this, error bars are limited by uncertainty in the partition volume. 



`r CAPTION <-
paste(   
"Poisson encapsulation statistics (Source [@Basu:2017aa])"
)`
 
```{r m5a-Basu-2017-fig3, fig.cap = CAPTION,  out.width = "800px", fig.align = 'center'}

knitr::include_graphics("img/Basu_2017_fig3.png")
 
``` 


`r CAPTION <-
paste(   
"Subsampling and Partitioning Errors (Source [@Basu:2017aa])"
)`
 
```{r m5a-Basu-2017-fig4, fig.cap = CAPTION,  out.width = "800px", fig.align = 'center'}

knitr::include_graphics("img/Basu_2017_fig4.png")
 
``` 

<br/>
<br/>

## Simulating Partitionings {#simg-partg .tabset}

* The assumed binomial variability is used as a model to explain the variability
in the occurence of empty partitions, the key measure in ddPCR. 

* It is based on the assumption that targets
are binned or partitioned at random.  We can simulate this context by distributing
`m targets` over a unit interval using a uniform distribution to select the target 
locations on the interval, and overlaying a bin grid over the interval.

<p><p/>
* Let 
   - N = 10K - the number of partitions  
   - $V_s$ = 10K - the sample number of units of volume (same as N for convenience)
   - Conc = .1, .01, .001  
   
<p><p/>
* In this context, m is fixed and the only variability is in the partitioning
which we assume either has no volume variability, or varies by 
v = 5, 10 or 20%
  - instead of uniform, make the distribution have density
(this only controls the occurence of volium values +/- v%): 

$$f(x)  = 1 - v/2 + v \cdot x  \text{   for x in [0, 1]}$$

### Home {-}

<br/>
<br/>

### No Volume Variability {- .tabset}


#### Home {-}

<br/>
<br/>

`r CONC <- 0.1; N <- 10000`

#### Conc = `r CONC`

```{r m5a-no-vol-var-conc-0p1-example, cache=T, cache.vars='', fig.cap = paste0("No Volume Variability, Conc =", 100*CONC, '%'), fig.height=6, fig.width=8}

 # draw N
Conc = CONC
n_pos <- runif(n=N*Conc) * N
Conc_hist <- hist(n_pos, breaks=seq(from=0, to = N, len = N),main='', plot=F,
        ylab="Target Count", xlab = "Partition")

 # THIS IS NOT ENOUGH VARIABILITY
 # n_pos <- runif(n=N*Conc) * N

 # Should incorporate the subsampling variability
 # N is partition and sample size
 n_target <- rpois(n=1, lambda = N)
 n_pos <- runif(n=n_target*Conc) * N
 Conc_hist <- hist(n_pos, breaks=seq(from=0, to = N, len = N), plot=F)

#empty_ndx <- which(Conc_hist$counts==0)
#rug(empty_ndx, col="red")

#table(Conc_hist$counts)


E_hat <- mean(Conc_hist$counts == 0)
E_low <- E_hat - 2 * sqrt(E_hat * (1 - E_hat)/N)
E_hi <- E_hat + 2 * sqrt(E_hat * (1 - E_hat)/N)

lambda_hat <- - log(E_hat)
lambda_low <- - log(E_hi)
lambda_hi <- - log(E_low)

M_est <-  - N * log(E_hat)

plot(Conc_hist, main='',  ylab="Target Count", xlab = "Partition (showing first 1K out of 10K",
    xlim=c(1, 1000))

title(paste0("Simulating Partitioning: N = ", N, " Conc = ", 100*CONC, '%'))
mtext(side = 3, paste0(expression(hat(lambda)),  ' = (', 
  round(100*lambda_low,1), ', ', round(100*lambda_hi,1), ')%',
  "  hat(E) = ", round(100*E_hat, 1), '%'))

```


```{r m5a-no-vol-var-conc-0p1-run-sim, cache=T, cache.vars="novolvar_conc_0p1_frm", fig.cap = paste0("No Volume Variability, Conc =", 100*CONC, '%')}

set.seed(101)
novolvar_conc_0p1_frm <- do.call("rbind", lapply(1:5000,
function(K)
{
 # THIS IS NOT ENOUGH VARIABILITY
 # n_pos <- runif(n=N*Conc) * N

 # Should incorporate the subsampling variability
 # N is partition and sample size
 n_target <- rpois(n=1, lambda = N) 
 n_pos <- runif(n=n_target*Conc) * N
 Conc_hist <- hist(n_pos, breaks=seq(from=0, to = N, len = N), plot=F)


 #empty_ndx <- which(Conc_hist$counts==0)
 #rug(empty_ndx, col="red")

 #table(Conc_hist$counts)

 E_hat <- mean(Conc_hist$counts == 0)
 E_low <- E_hat - 2 * sqrt(E_hat * (1 - E_hat)/N)
 E_hi <- E_hat + 2 * sqrt(E_hat * (1 - E_hat)/N)

 lambda_hat <- - log(E_hat)
 lambda_low <- - log(E_hi)
 lambda_hi <- - log(E_low)

  # Interval according to ddPCR_ENGL_2019
  n_tot <- length(Conc_hist$counts)
  n_neg <- sum(Conc_hist$counts == 0)
  n_pos <- n_tot - n_neg
  lambda_low2 <- lambda_hat - 2 * sqrt(n_pos/(n_tot*n_neg))
  lambda_hi2 <- lambda_hat + 2 * sqrt(n_pos/(n_tot*n_neg))


 data.frame(sim=K, E_hat = E_hat, E_low, E_hi, lambda_hat, lambda_low, lambda_hi,
      lambda_low2, lambda_hi2)
}))

```

```{r m5a-no-vol-var-conc-0p1-plot-sim, cache=T, cache.vars="novolvar_conc_0p1_frm", fig.cap = paste0("No Volume Variability, Conc =", 100*CONC, '%'), fig.heigth=6, fig.width=8}

par(mfrow=c(1,1), mar=c(2, 5, 2, 1),oma=c(2,0,2,0))

with(novolvar_conc_0p1_frm[1:100,],
{
plot(1:100, 100*lambda_hat, pch=20, ylim = c(min(100*lambda_low), max(100*lambda_hi)), xlab="")
segments(x0=1:100, x1=1:100, y0=100*lambda_low, y1=100*lambda_hi)
abline(h=100*Conc, col='green')
}
)
lambda_coverage_prob <- with(novolvar_conc_0p1_frm,
mean(lambda_low < Conc & Conc < lambda_hi))

title(paste("CI for Conc (100 of 5000) - Coverage prob =",
   round(lambda_coverage_prob, 2)))

SKIP_E_HAT <- function() {
with(novolvar_conc_0p1_frm[1:100,],
{
plot(1:100, E_hat, pch=20, ylim = c(min(E_low), max(E_hi)), xlab="")
segments(x0=1:100, x1=1:100, y0=E_low, y1=E_hi)
abline(h=1-Conc, col='green')
}
)
E_coverage_prob <- with(novolvar_conc_0p1_frm,
mean(E_low < (1-Conc) & (1-Conc) < E_hi))

title(paste("CI for Proportion Empty Titrations (100 of 5000) - Coverage prob =",
   round(E_coverage_prob, 2)))
}# SKIP_E_HAT

mtext(side=1, outer=T, "Partition No.")
mtext(side=3, outer=T, paste0("Simulating Partitions N =", N, " Conc =", 100*CONC, '%'))

```

<br/>
<br/>

`r CONC = 0.001; N = 10000`

#### Conc = `r CONC`


```{r m5a-no-vol-var-conc-0p001-example, cache=T, cache.vars='', fig.cap = paste0("No Volume Variability, Conc =", 100*CONC, '%'), fig.height=6, fig.width=8}

 # draw N
Conc = CONC
 # THIS IS NOT ENOUGH VARIABILITY
 # n_pos <- runif(n=N*Conc) * N

 # Should incorporate the subsampling variability
 # N is partition and sample size
 n_target <- rpois(n=1, lambda = N)
 n_pos <- runif(n=n_target*Conc) * N
 Conc_hist <- hist(n_pos, breaks=seq(from=0, to = N, len = N), plot=F)


E_hat <- mean(Conc_hist$counts == 0)
E_low <- E_hat - 2 * sqrt(E_hat * (1 - E_hat)/N)
E_hi <- E_hat + 2 * sqrt(E_hat * (1 - E_hat)/N)

lambda_hat <- - log(E_hat)
lambda_low <- - log(E_hi)
lambda_hi <- - log(E_low)

M_est <-  - N * log(E_hat)

plot(Conc_hist, main='',  ylab="Target Count", xlab = "Partition",
    xlim=c(1, 10000))

title(paste0("Simulating Partitioning: N = ", N, " Conc = ", 100*CONC, '%'))
mtext(side = 3, paste0(expression(hat(lambda)),  ' = (', 
  round(100*lambda_low,1), ', ', round(100*lambda_hi,1), ')%',
  "  hat(E) = ", round(100*E_hat, 1), '%'))

```


```{r m5a-no-vol-var-conc-0p001-run-sim, cache=T, cache.vars="novolvar_conc_0p001_frm", fig.cap = paste0("No Volume Variability, Conc =", 100*CONC, '%')}

set.seed(101)
novolvar_conc_0p001_frm <- do.call("rbind", lapply(1:5000,
function(K)
{
 # THIS IS NOT ENOUGH VARIABILITY
 # n_pos <- runif(n=N*Conc) * N

 # Should incorporate the subsampling variability
 # N is partition and sample size
 n_target <- rpois(n=1, lambda = N)
 n_pos <- runif(n=n_target*Conc) * N
 Conc_hist <- hist(n_pos, breaks=seq(from=0, to = N, len = N), plot=F)

 #table(Conc_hist$counts)

 E_hat <- mean(Conc_hist$counts == 0)
 E_low <- E_hat - 2 * sqrt(E_hat * (1 - E_hat)/N)
 E_hi <- E_hat + 2 * sqrt(E_hat * (1 - E_hat)/N)

 lambda_hat <- - log(E_hat)
 lambda_low <- - log(E_hi)
 lambda_hi <- - log(E_low)

  # Interval according to ddPCR_ENGL_2019
  n_tot <- length(Conc_hist$counts)
  n_neg <- sum(Conc_hist$counts == 0)
  n_pos <- n_tot - n_neg
  lambda_low2 <- lambda_hat - 2 * sqrt(n_pos/(n_tot*n_neg))
  lambda_hi2 <- lambda_hat + 2 * sqrt(n_pos/(n_tot*n_neg))


 data.frame(sim=K, E_hat = E_hat, E_low, E_hi, lambda_hat, lambda_low, lambda_hi,
      lambda_low2, lambda_hi2)
}))

```

```{r m5a-no-vol-var-conc-0p001-plot-sim, cache=T, cache.vars="novolvar_conc_0p001_frm", fig.cap = paste("No Volume Variability, Conc =", CONC), fig.heigth=6, fig.width=8}

par(mfrow=c(1,1), mar=c(2, 5, 2, 1),oma=c(2,0,2,0))

with(novolvar_conc_0p001_frm[1:100,],
{
plot(1:100, 100*lambda_hat, pch=20, ylim = c(min(100*lambda_low), max(100*lambda_hi)), xlab="")
segments(x0=1:100, x1=1:100, y0=100*lambda_low, y1=100*lambda_hi)
abline(h=100*Conc, col='green')
}
)
lambda_coverage_prob <- with(novolvar_conc_0p001_frm,
mean(lambda_low < Conc & Conc < lambda_hi))

title(paste("CI for Conc (100 of 5000) - Coverage prob =",
   round(lambda_coverage_prob, 2)))

SKIP_E_HAT <- function() {
with(novolvar_conc_0p001_frm[1:100,],
{
plot(1:100, E_hat, pch=20, ylim = c(min(E_low), max(E_hi)), xlab="")
segments(x0=1:100, x1=1:100, y0=E_low, y1=E_hi)
abline(h=1-Conc, col='green')
}
)
E_coverage_prob <- with(novolvar_conc_0p001_frm,
mean(E_low < (1-Conc) & (1-Conc) < E_hi))

title(paste("Confidence intervals for Proportion Empty Titrations (100 of 5000) - Coverage prob =",
   round(E_coverage_prob, 2)))
}# SKIP_E_HAT

mtext(side=1, outer=T, "Partition No.")
mtext(side=3, outer=T, paste0("Simulating Partitions N =", N, " Conc =", 100*CONC, '%'))

```

<br/>
<br/>

## ddPCR Data Analysis by GLM {#ddpcr-analysis-glm .tabset}

In this section we carry out data analyses outlined in
Dorazio et al. (2015) [@Dorazio:2015aa].

### Home {-}

<br/>
<br/>

### Dilution Data Analysis {- .tabset}

#### Home {-}

<br/>
<br/>


#### Load the data {-}


```{r m5a-rd-dorazio-dilution-data, cache=T, cache.vars = "dorazio_dilution_frm"}

dorazio_dilution_frm <- with(params, 
data.table::fread(file.path(Dorazio_2015_supp_dir, Dorazio_dilution_file)))

dorazio_dilution_frm %<>% 
dplyr::mutate(concF = paste0("C", 
   as.numeric(as.factor(conc)))
 ) %>%
dplyr::group_by(concF) %>%
dplyr::mutate(rep = paste0('r', formatC(dplyr::row_number(), width=2, flag='0'))) %>% 
dplyr::arrange(concF, rep) %>%
dplyr::relocate(concF, rep) %>% as.data.frame()

dorazio_dilution_frm %<>% 
dplyr::rename(
   npos = npositive,
   ntot = ntotal) %>%
dplyr::mutate(
   nneg = ntot - npos
)

 # also add vol, which is needed later
 # constant volume (microliters) per droplet (physical constant)

dorazio_dilution_frm %<>% 
dplyr::mutate(
  Vp = 0.91 / 1000,   
  E = nneg / ntot,
  C = -log(E)/Vp
)

saveObj("_M5A-dorazio_dilution_frm", "dorazio_dilution_frm")
```

Annotate ...


```{r m5a-annot-dorazio, cache=F, echo=T}

 ########################################
 # Annotation 
 ########################################

 # Option 3
grail_color <- c("#3B1C52","#94724F","#628592","#666666","#546741","#B73D27",
               "#D29A22","royalblue4","plum4","maroon4", "turquoise4", "tomato4")
col_vector <- rev(grail_color)


 # Option 2
suppressPackageStartupMessages(require(RColorBrewer))
qual_col_pals <- brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector <- unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
col_vector[4] <- 'red'
col_vector[8] <- 'darkgreen'

 # Option 1
KellyColors.vec <- c(
  "#222222", "#F3C300", "#875692", "#F38400", "#A1CAF1",
  "#BE0032", "#C2B280", "#848482", "#008856", "#E68FAC", "#0067A5",
  "#F99379", "#604E97", "#F6A600", "#B3446C", "#DCD300", "#882D17",
  "#8DB600", "#654522", "#E25822", "#2B3D26"
)
col_vector <- KellyColors.vec

 # recycle for more levels
 col_vector <- rep(col_vector, 3)

 Pch_v <- setdiff(1:50, 26:31)


 # concF
concFLegend_vec <- sort(unique(dorazio_dilution_frm$concF))
concFCol_vec <- col_vector[1:length(concFLegend_vec)]
names(concFCol_vec) <- concFLegend_vec
  
concFPch_vec <- Pch_v[1:length(concFLegend_vec)]
names(concFPch_vec) <- concFLegend_vec


 # rep
repLegend_vec <- sort(unique(dorazio_dilution_frm$rep))
repCol_vec <- col_vector[1:length(repLegend_vec)]
names(repCol_vec) <- repLegend_vec
  
repPch_vec <- Pch_v[1:length(repLegend_vec)]
names(repPch_vec) <- repLegend_vec


```

* Unequal slopes indicate that the relationship between
`npositive` and `ntotal` cannot be reduced to the ratio of the two.


```{r m5a-plot-dorazio-dilution, cache=T, cache.vars='', fig.height= 6, fig.width = 8, fig.cap ="Dorazio Dilution Data"}

par(mar=par("mar") + c(0, 0 , 0, 4))

with(dorazio_dilution_frm,
plot(x = log(ntot), y = log(npos),  log='',
     col = concFCol_vec[concF],
     pch = repPch_vec[rep],
    xaxt = 'n', yaxt = 'n', xlab='ntot', ylab='npos' )
)
with(dorazio_dilution_frm,
{
axis(side=1, at = pretty(log(ntot)), label = round(exp( pretty(log(ntot))), 0))
axis(side=2, at = pretty(log(npos)), label = round(exp( pretty(log(npos))), 0))
})

concF_dil_frm_lst <- split(dorazio_dilution_frm, dorazio_dilution_frm$concF)
concF_dil_lm_lst <- lapply(concF_dil_frm_lst, function(frm)
  lm(log(npos) ~  log(ntot), data = frm))


for(CC in names(concF_dil_lm_lst)) {
  dil_lm <- concF_dil_lm_lst[[CC]]
  abline(dil_lm, col = concFCol_vec[CC])
}

slopes_vec <- sapply(concF_dil_lm_lst, function(LM) coefficients(LM)[[2]])

old_par =  par(xpd = NA)

legend('topright', inset = c(-.2, 0), title = "ConcF - slope",
legend = paste0(names(slopes_vec), '=', round(slopes_vec, 1)),
text.col = concFCol_vec[names(slopes_vec)], bty='n')

par(old_par)

title("Dilution Data from Dorazio et al. (2015) ")

```

<br/>
<br/>

#### rePlot the data {-}

* Replot the data in the way that reflects the fitted model.

```{r m5a-plot-ddpcr-seq-counts, cache=T, cache.vars='dorazio_byDil_frm', fig.height= 6, fig.width = 8, fig.cap ="Dorazio Dilution Data - Counts: total, pos, neg"}

dorazio_byDil_frm <- dorazio_dilution_frm %>%
dplyr::group_by(concF) %>%
dplyr::summarize(
  Vp = median(Vp),
  conc = round(median(conc), 2),
  ntot_med = median(ntot),
  npos_med = median(npos),
  nneg_med = median(nneg),
  E_q1 = round(quantile(100*E, prob=1/4), 2),
  E_q3 = round(quantile(100*E, prob=3/4), 2),
  E_cv = (E_q3 - E_q1)/(E_q3 + E_q1),
  C_q1 = round(quantile(C, prob=1/4), 1),
  C_q3 = round(quantile(C, prob=3/4), 1),
  C_cv = (C_q3 - C_q1)/(C_q3 + C_q1),
  #E_iqr = paste0( E_q1, ', ', E_q3),
  #C_iqr = paste0( C_q1, ', ', C_q3)
) %>% as.data.frame()


dorazio_byDil_frm %>%
knitr::kable(
caption = "Dorazio Dilution Data - Summarized over reps"
) %>%
kableExtra::kable_styling(full_width = F)
  

par(mar=par("mar") + c(0, 0 , 0, 4))

with(dorazio_dilution_frm,
{
plot(x = log(conc), y = log(ntot),  log='', ylim=range(log(c(ntot, npos, nneg))),
     col = 1, ###concFCol_vec[concF],
     pch = repPch_vec[rep],
    xaxt = 'n', yaxt = 'n', xlab='conc', ylab='count')
points(log(conc), log(nneg),
       col = 2, ###concFCol_vec[concF],
       pch = repPch_vec[rep]
      )
points(log(conc), log(npos),
       col = 3, ###concFCol_vec[concF],
       pch = repPch_vec[rep]
      )
axis(side=2, at = pretty(log(c(ntot, npos, nneg))), 
label = round(exp( pretty(log(c(ntot, npos, nneg)))), 0))
axis(side=1, at = log(unique(conc)), label = round(unique(conc), 1))

}
)

with(dorazio_byDil_frm, lines(log(conc), log(ntot_med), col = 1, lwd=2))
with(dorazio_byDil_frm, lines(log(conc), log(nneg_med), col = 2, lwd=2))
with(dorazio_byDil_frm, lines(log(conc), log(npos_med), col = 3, lwd=2))

old_par =  par(xpd = NA)

legend('topright', inset=c(-.15, 0), legend = c("ntot", "nneg", "npos"), text.col = 1:3,
bty='n', title="Counts", title.col = 1)

par(old_par)


title("Dilution Data from Dorazio et al. (2015) ")

```

<br/>
<br/>

#### reRePlot the data {-}

* Replot the data in the way that reflects the fitted model.
   -  only show negative and total counts as posiitive counts
are at a different level and compress the figure.


```{r m5a-rereplot-dorazio-dilution, cache=T, cache.vars='dorazio_byDil_frm', fig.height= 6, fig.width = 8, fig.cap ="Dorazio Dilution Data"}

dorazio_byDil_frm <- dorazio_dilution_frm %>%
dplyr::group_by(concF) %>%
dplyr::summarize(
  Vp = median(Vp),
  conc = round(median(conc), 2),
  ntot_med = median(ntot),
  npos_med = median(npos),
  nneg_med = median(nneg),
  E_q1 = round(quantile(100*E, prob=1/4), 2),
  E_q3 = round(quantile(100*E, prob=3/4), 2),
  E_cv = round(100*(E_q3 - E_q1)/(E_q3 + E_q1), 3),
  C_q1 = round(quantile(C, prob=1/4), 1),
  C_q3 = round(quantile(C, prob=3/4), 1),
  C_cv = round(100*(C_q3 - C_q1)/(C_q3 + C_q1), 3),
  #E_iqr = paste0( E_q1, ', ', E_q3),
  #C_iqr = paste0( C_q1, ', ', C_q3)
) %>% as.data.frame()


dorazio_byDil_frm %>%
knitr::kable(
caption = "Dorazio Dilution Data - Summarized over reps"
) %>%
kableExtra::kable_styling(full_width = F)
  

par(mar=par("mar") + c(0, 0 , 0, 4))

with(dorazio_dilution_frm,
{
plot(x = log(conc), y = log(ntot),  log='', 
     #ylim=pmax(c(log(500), 0), range(log(c(ntot, npos, nneg)))),
     ylim=range(log(c(ntot, nneg))),
     col = 1, ###concFCol_vec[concF],
     pch = repPch_vec[rep],
    xaxt = 'n', yaxt = 'n', xlab='conc', ylab='count')
points(log(conc), log(nneg),
       col = 2, ###concFCol_vec[concF],
       pch = repPch_vec[rep]
      )
axis(side=2, at = pretty(log(c(ntot, nneg))), 
label = round(exp( pretty(log(c(ntot, nneg)))), 0))
axis(side=1, at = log(unique(conc)), label = round(unique(conc), 1))

}
)

with(dorazio_byDil_frm, lines(log(conc), log(ntot_med), col = 1, lwd=2))
with(dorazio_byDil_frm, lines(log(conc), log(nneg_med), col = 2, lwd=2))

old_par =  par(xpd = NA)

legend('topright', inset=c(-.15, 0), legend = c("ntot", "nneg"), text.col = 1:3,
bty='n', title="Counts", title.col = 1)

par(old_par)


title("Dilution Data from Dorazio et al. (2015) ")

```

<br/>
<br/>

#### cloglog model fit {-}

* The complimentary log-log link:  
Cloglog regression is an extension of the logistic regression model and is 
particularly useful when the probability of an event is very small or very large.
   - used in Dorazio et al.

```{r m5a-dorazio-dilution-glm-fit, cache=T, cache.vars=c('cloglog_fit', "dorazio_dilution_glm_frm")}

cloglog_fit <- glm(cbind(npos, nneg) ~ log(conc), family=binomial(link='cloglog'),
   offset = log(Vp), data = dorazio_dilution_frm)

summary(cloglog_fit)

  # for manually extracting informantion from fit objects
beta.mle = cloglog_fit$coefficients
beta.vcv = vcov(cloglog_fit)
beta.se = sqrt(diag(beta.vcv))
alpha = 0.05  # significance level for confidence intervals
zcrit = qnorm(1-alpha/2)
beta.lowerCL = beta.mle - zcrit*beta.se
beta.upperCL = beta.mle + zcrit*beta.se
deviance = cloglog_fit$deviance
GOF = 1 - pchisq(deviance, df=nrow(dorazio_dilution_frm)-length(beta.mle))


 # Summarize results
beta.out = cbind(beta.mle, beta.se, beta.lowerCL, beta.upperCL)
dimnames(beta.out)[2] = list(c('MLE', 'SE', '2.5%', '97.5%'))

if(F)
print(beta.out)

 # Estimate concentration of each dilution
X <- with(dorazio_dilution_frm, model.matrix( ~ log(conc)))

X.pred <- unique(X)

lambda.est = rep(NA,dim(X.pred)[1])
lambda.lowerCL = rep(NA,dim(X.pred)[1])
lambda.upperCL = rep(NA,dim(X.pred)[1])
for (i in 1:dim(X.pred)[1]) {
    Xvec = matrix(X.pred[i,], ncol=1)
    loglambda.est = t(Xvec) %*% beta.mle
    loglambda.var = t(Xvec) %*% beta.vcv %*% Xvec
    lambda.est[i] = exp(loglambda.est)
    lambda.lowerCL[i] = exp(loglambda.est - zcrit*sqrt(loglambda.var))
    lambda.upperCL[i] = exp(loglambda.est + zcrit*sqrt(loglambda.var))
}

data.frame(lambda_est  = lambda.est,  
           lambda_lcb = lambda.lowerCL,
           lambda_ucb = lambda.upperCL) %>%
knitr::kable(
caption = "cloglog fit estimates"
) %>%
kableExtra::kable_styling(full_width = F)

 # Alternatively, use model extraction methods

if(F)
cloglog_preds <- predict(cloglog_fit, newdata = new_data_frm, type = "response",
   se.fit = T)

 ### Return to this later


```

Residual plot

```{r m5a-cloglog-residuals, cache=T, cache.vars='', fig.height= 6, fig.width = 8, fig.cap ="Dorazio Dilution Data"}

par(mfrow= c(2, 1), mar = c(2, 4, 2, 1), oma=c(2, 0, 2, 4))

with(dorazio_dilution_frm,
{
plot(x = log(conc), y = log(ntot),  log='', 
     #ylim=pmax(c(log(500), 0), range(log(c(ntot, npos, nneg)))),
     ylim=range(log(c(ntot, nneg))),
     col = 1, ###concFCol_vec[concF],
     pch = repPch_vec[rep],
    xaxt = 'n', yaxt = 'n', xlab='', ylab='count')
points(log(conc), log(nneg),
       col = 2, ###concFCol_vec[concF],
       pch = repPch_vec[rep]
      )
axis(side=2, at = pretty(log(c(ntot, nneg))), 
label = round(exp( pretty(log(c(ntot, nneg)))), 0))
axis(side=1, at = log(unique(conc)), label = round(unique(conc), 1))
}
)

npos_vec <- with(dorazio_dilution_frm, sapply(split(npos, conc), sum))
nneg_vec <- with(dorazio_dilution_frm, sapply(split(nneg, conc), min))
with(dorazio_byDil_frm,
text(log(conc), log(nneg_vec), labels=npos_vec)
)
legend('bottom', legend = "npos", bty='n')

with(dorazio_byDil_frm, lines(log(conc), log(ntot_med), col = 1, lwd=2))
with(dorazio_byDil_frm, lines(log(conc), log(nneg_med), col = 2, lwd=2))

old_par =  par(xpd = NA)

legend('topright', inset=c(-.15, 0), legend = c("ntot", "nneg"), text.col = 1:3,
bty='n', title="Counts", title.col = 1)

par(old_par)

mtext(side=3, outer = T, "Dilution Data from Dorazio et al. (2015) ")

with(dorazio_dilution_frm,
{
plot(x = log(conc), y = residuals(cloglog_fit))
})
abline(h=0, col='red')
title(paste("GOF = ", GOF))


```

* A GOF of zero indicates that the model does not fit the data which is seen in the pattern
of the residuals.

* In this case a one-group-at-a-time analysis may be preferable.



<br/>
<br/>

#### optim() model fit {-}

**Come back to this later**

<br/>
<br/>

### CNV Analysis {-}

**Come back to this later**

<br/>
<br/>

### Gene Expression Analysis {-}

**Come back to this later**

<br/>
<br/>

# ddPCR & Seq Merged Data {#ddpcr-seq-data .tabset}


<br/>
<br/>



## Home {-}

<br/>
<br/>

## Intro 

* In this section we summarize data from a dataset which has already been
curated (not documented) and combines the ddPCR and Seq data.  The data from each platform
are more carefully analyzed in the sections which follow^[
It should be made clear that statistical analysis does not begin
with curated data.  It actually begins when the data collection is 
being planned.  When the statistician is not involved from
the beginning, when they do get involved, they will have to go
back and start working with the raw data, and go through
all of the data cleaning and preprocessing steps needed to
convert the raw data to an analysis dataset.  Even when
the scientist knows exactly what to do, the steps have to be repeated
by the statistician because the preprocessing of the data is 
an integral part of the study data analysis process.  With that
in mind, the scientist must organize the data in such a way that
the raw data files are readily accessible.  Giving the statistician
access to a curated file does not accelerate the process, and
may add quite a bit of time if the path from raw data to curated dataset
is not perfectly clear, as is rarely the case.].

<p><p/>
* For each platform the are two choices for contamination indices:
   - the number of contaminants detected in the host sample
   - the rate of contaminant detection on the host sample
      - for ddPCR the 


<p><p/>
* The dilution data collected with batches 12 and 13 form a rich set for evaluating
each platform as well as for comparing the two^[
this section is based on data obtained from a table created by merging seq and ddPCR
data.  The seq data included in the merge are examined in the previous section,
while the ddPCR data are examined in the next section.]
   - The richness comes from the truth built into the design and the replication included in the
dataset.  
      - the dilution has built-in differences between groups, or dilution levels - we can examine 
how each platform does at detecting these differences
      - additionally, we can assess variability with the replication
      - the two parts are necessary to have an interpretable assessment of performance

<p><p/>
* Detecting contamination requires that a contamination index or
statistic be able to distinguish, or discriminate,
among samples with different levels of contamination.  A good index must
have the following attributes (in the appropriate range of contamination):
   - the index must be sensitive to different levels of contamination
   - the index must have low variability

<p><p/>
* In other words, when examining the values of a candidate contamination
detection indext for samples separated into two groups with distinct
contamination levels (same contamination level in each group but different 
between groups), the ideal index would be observed **at a different level for
each group, with little variability within each group**. 
   - Note that the goal of detecting differences in contamination within
a given range of inrerest does not require the index to produce
accurate estimates of contamination level - the index just needs to be
sensitive and specific in the contamination raneg of interest.
      - the fact that a contamination index is unbiased and has low
vriablity when the contamination rate is 20% or 30% is immaterial. 

<!--
<p><p/>
* These considerations suggests scoring candidate contamination 
detection indices for each pair of titration groups using a z-score, or 
a difference in standard units:

$$z_{A, B} = \frac{\hat{F_{A, B}}}{\text{se}_F}$$

where  

* A and B refer to two groups corresponding to 
two titration levels
* F is the expected log fold-change in contamination index
between levels A and B
* $se_F$ is the standard error of the estimated log fold-change

-->

<!--
<p><p/>
* There are several options to choose from to quantify contamination on the ddPCR platform:
   - use one (YRS) or two (+HBB) channels?
      - HBB would be used as a normalizer
      - we need to examine whether that helps and we probably need more than one dateset
   - for each channel, what statistic to use?  $\hat{\lambda}$ or $\hat{C}$?  
      - the diffence is just a constant so it only matters when the constant varies 
from run to run
-->


<p><p/>
* The contamination indices under consideration include, for each platform:
   - Seq:
      - S1 = n_contam_frag 
      - S2 = cf $= \frac{\text{n_contam_frag}}{\text{n_hom_frag_adj}}$
         - at the level of contamination of interest, it is not clear that
scaling n_contam_frag by n_hom_frag_adj helps
<p><p/>
   - ddPCR   
      - D1 = nPositive,  the number of droplets for which a (contam) target was detected  
      - D2 = posRate = nPositive/nAccepted 
      - D3 = pctConta_byYRS 
         - (these can be negative - how can we get negative contamination?)^[
Having negative contamination is an indication that there is a problem
in the way the index is computed.  Since the contamination levels the we want 
to estimate are low, it seems that it would be worthwhile to calibrate the 
process such that negative values, which are invalid estimates, do not occur.]
      - D4 = pctConta_byRatio
      - D5 = cf_ddpcr 

<br/>
* Statistically D2-D5 are equivalent (corr = 1.0)

***

<p><p/>
*  The contamination indices, S1 and S2 on the seq platform, and D1 and D2
on the ddPCR platform, are similar but the counts have different meanings 
on the two platforms:
   -  It may be counter intuitive but when working with ddPCR data,
concentrations are estimated based on the distribution of empty partitions - 
not the content of the filled partitions.
   - on the ddPCR platform, 
      - `npositive` is the number of non-empty partitions and `nnegative` is the 
number of empty partitions.
      - the expected number of targets partitioned across all partitions (ie. in the sample) can 
be estimated from these
   - on the NGS platform,
      - `n_contam_frag` is the number of contaminating fragments detected in the test sample,
and `n_hom_frag_adjacent` is the number potential contaminants compiled from
adjacent samples.
         - the correlation between `n_hom_frag` and `n_hom_frag_adjacent` is close to 1.  This
implies that statistically `n_hom_frag_adjacent` does not buy you anything over `n_hom_frag`.
      - If each of the `n_hom_frag_adjacent` potential contaminants is equally likely
to be detected, then the number of contaminating fragments has a binomial distribution 
with parameters p, the unknown
proportion of `n_hom_frag_adjacent` which have contaminated the test sample,  and N is 
`n_hom_frag_adjacent`.

* The fact that contamination is mearures differently on the two platforms is not
of any concern at this point.
   - We want each of the platforms to discriminate
between **samples with** and **samples without** contamination.  
   - The two platforms do not have to use the same measurement or speak the
same language even.
   - The fact that the fraction positive on the ddPCR platform is not the same 
as the fraction of contaminant fragments is also irrelevant because **the ddPCR output 
could be transformed to agree with the seq ratios** 

***

<!--
<p><p/>
   -  Careful considerations should be given to the modeling task - a good model will produce reliable
estimates and add to our understanding of the platform.
   - models have assumptions and or implications which should be examined and verified - even
if we don't have a Plan B^
-->

***

<!--
* First get the dilution data without exclusions.   After verifying the structure,
filter out samples which would induce bias or unnecessary variability.
-->

Assemble the analysis dataset   ...

```{r m5a-seq-dil-series, cache=T, cache.vars='ddpcr_seq_dilution_frm', fig.cap="B012-13: Seq Dilution Series Data", results = "asis"}

ddpcr_seq_dilution_frm <-  conta_seq_frm %>%
  dplyr::filter(batch %in% c("B012", "B013")) %>%
  dplyr::mutate(cf = pmax(1e-6, cf)) %>%
  dplyr::select(sample, sampleName, batch, Plate_24, Well_96, abn_cov, bin_cov,
                cf, n_contam_frag, n_hom_frag_adj, n_hom_frag, pctMale, 
                Positives, Negatives, Accepted_droplets,
                cf_ddpcr, pctConta_byRatio, pctConta_byYRS,
                chPP, chPM, chMP, chMM) %>%
  dplyr::mutate(
    dil_level = 2^as.numeric(sub("L",'',sample)),
    seq_tot = n_hom_frag_adj,    # keep both seq_tot and n_hom_frag_adj
    seq_pos = n_contam_frag      # keep both seq_pos and n_contam_frag
  ) %>%
  dplyr::rename(
    dd_tot = Accepted_droplets,
    dd_pos = Positives,
    dd_neg = Negatives
    #seq_tot = n_hom_frag_adj,
    #seq_pos = n_contam_frag
  ) %>%
  dplyr::mutate(
    seq_neg = seq_tot - seq_pos
   )

```

Annotate ...


```{r m5a-annot-2, cache=F, echo=T}

 ########################################
 # Annotation 
 ########################################

 # Option 3
grail_color <- c("#3B1C52","#94724F","#628592","#666666","#546741","#B73D27",
               "#D29A22","royalblue4","plum4","maroon4", "turquoise4", "tomato4")
col_vector <- rev(grail_color)


 # Option 2
suppressPackageStartupMessages(require(RColorBrewer))
qual_col_pals <- brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector <- unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
col_vector[4] <- 'red'
col_vector[8] <- 'darkgreen'

 # Option 1
KellyColors.vec <- c(
  "#222222", "#F3C300", "#875692", "#F38400", "#A1CAF1",
  "#BE0032", "#C2B280", "#848482", "#008856", "#E68FAC", "#0067A5",
  "#F99379", "#604E97", "#F6A600", "#B3446C", "#DCD300", "#882D17",
  "#8DB600", "#654522", "#E25822", "#2B3D26"
)
col_vector <- KellyColors.vec

 # recycle for more levels
 col_vector <- rep(col_vector, 3)

 Pch_v <- setdiff(1:50, 26:31)

 # These legends are for the diltuions series only

 # batch
batchLegend_vec <- sort(unique(ddpcr_seq_dilution_frm$batch))
batchCol_vec <- col_vector[1:length(batchLegend_vec)]
names(batchCol_vec) <- batchLegend_vec

batchPch_vec <- Pch_v[1:length(batchLegend_vec)]
names(batchPch_vec) <- batchLegend_vec

 # Plate_24
Plate_24Legend_vec <- sort(unique(ddpcr_seq_dilution_frm$Plate_24))
Plate_24Col_vec <- col_vector[1:length(Plate_24Legend_vec)]
names(Plate_24Col_vec) <- Plate_24Legend_vec

Plate_24Pch_vec <- Pch_v[1:length(Plate_24Legend_vec)]
names(Plate_24Pch_vec) <- Plate_24Legend_vec

 # sample
sampleLegend_vec <- sort(unique(ddpcr_seq_dilution_frm$sample))
sampleCol_vec <- col_vector[1:length(sampleLegend_vec)]
names(sampleCol_vec) <- sampleLegend_vec

samplePch_vec <- Pch_v[1:length(sampleLegend_vec)]
names(samplePch_vec) <- sampleLegend_vec
 

```

<br/>
<br/>

<!--
## Verify Structure 

* 2 paltforms, 7-8 dilution levels, with replicates
   - ddPCR: should have dd_pos, dd_tot data for each
   - seq: should have cf, n_contam_frag, n_hom_frag_adj, pctMale, 
-->

```{r m5a-verify-structure, eval=F}

ddpcr_seq_dilution_frm %>%
dplyr::group_by(batch, dil_level) %>%
dplyr::summarize(
  n = dplyr::n(),
  n_valid_dd_pos = sum(!is.na(dd_pos)), 
  n_valid_dd_tot = sum(!is.na(dd_tot)), 
  n_valid_seq_pos = sum(!is.na(seq_pos)),
  n_valid_seq_tot = sum(!is.na(seq_tot)),
  n_valid_bin_cov = sum(!is.na(bin_cov) & bin_cov > 50)
) %>%
knitr::kable(
caption = "DDPCR Seq Dilution Data Structure"
) %>%
kableExtra::kable_styling(full_width = F)

```
   
* When exploring the data  we should keep all samples in
the alalysis.  Only when we comptute stats and assess performance do we need
to remove samples which have some missing value.

* For now just drop samples with missing dil_level, which are controls

```{r}

ddpcr_seq_dilution_frm %<>% 
  dplyr::filter(!is.na(dil_level))

ddpcr_seq_dilution_frm %<>% 
dplyr::mutate(
  dil_levelF = paste0("L", formatC(as.numeric(as.factor(dil_level)), width=2, flag='0')),
  pctDummy = 1/dil_level,
  Vp_d = 1 / 1000,   
  E_ddPCR = dd_neg / dd_tot,
  #C_ddPCR_d = -log(E_ddPCR)/Vp_d,

  E_seq = seq_neg/seq_tot,
  #C_seq_d = E_seq/Vp_d
 )

 # dil_levelF
dil_levelFLegend_vec <- sort(unique(ddpcr_seq_dilution_frm$dil_levelF))
dil_levelFCol_vec <- col_vector[1:length(dil_levelFLegend_vec)]
names(dil_levelFCol_vec) <- dil_levelFLegend_vec

dil_levelFPch_vec <- Pch_v[1:length(dil_levelFLegend_vec)]
names(dil_levelFPch_vec) <- dil_levelFLegend_vec
 

saveObj("_M5A-ddpcr_seq_dilution_frm", "ddpcr_seq_dilution_frm")
```

<br/>
<br/>

## Background Samples - Batches 01-11 {#ddpcr-seq-background .tabset}

<p><p/>
* In this section we characterize sequencing reads for NA12878 and NA24631
by Batch in 1-11.
   - for summaries of all the samples in batches 1 to 11 see Section \@ref(exam-seq-data).


<p><p/>
* Statistics of interest (all on log10 scale): 
   - cf = n_contam_frag/n_hom_frag_adj
      - mean, sd, q1, q3
   - n_contam_frag
      - mean, sd, q1, q3
   - n_hom_frag_adj
      - mean, sd, q1, q3
   - cor(n_contam_frag, n_hom_frag_adj)

<p><p/>
* Question of interest: how does sd(cf) compare with sd(n_contam_frag)?
   - **does scaling by n_hom_frag_adj  reduce the variability in n_contam_frag?**

<p><p/>
* In order to have comparable statistics, we will summarize all the measures
on log10 scale.
   -  $log10(10^n) = n$ - a factor of 10 increases log10 by 1
   - log10(2) = 0.30 - doubling increases log10 by 0.30

<br/>

### Home {-}

<br/>
<br/>

### Pairs By Batch {- .tabset}

```{r m5a-}

panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
}


panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    #from help of pairs
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y,use="pairwise.complete.obs"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}




```

<br/>
<br/>


### Batch 1-11 Summary {-}








<br/>
<br/>


## Dilution Series Samples {#ddpcr-seq-dilution .tabset}

### Count Plots  {.tabset}

* Pairs dd & seq 

<!--
* Apply quality filters to get better resolution and color by pctDummy to see tot vs pos 
better.
-->

<p><p/>
* At the low end of the mixture/contamination range: 
   - dd_tot and dd_pos are uncorrelated
      - variability in the number of positive droplets does not depend on the
total number of droplets
   - seq_tot and seq_pos are weakly correlated


```{r m5a-ddpcr-seq-counts-pairs-dd-seq, cache=T, cache.vars='', fig.height= 8, fig.width = 11, fig.cap ="ddPCR and Seq Counts: total, pos"}

 # get corr between positive (conta) and total counts
seq_tot_pos_cor_vec  <- 
with(ddpcr_seq_dilution_frm %>%  dplyr::filter(bin_cov > 50),
sapply(split(data.frame(seq_tot, seq_pos), dil_levelF), 
function(M) round(cor(M[,1], M[,2], use="pair"),2)))

dd_tot_pos_cor_vec  <-  
with(ddpcr_seq_dilution_frm %>%  dplyr::filter(bin_cov > 50),
sapply(split(data.frame(dd_tot, dd_pos), dil_levelF),
function(M) round(cor(M[,1], M[,2], use="pair"),2)))

with(ddpcr_seq_dilution_frm %>% 
     dplyr::filter(bin_cov > 50),
pairs(cbind(dd_tot, dd_pos = dd_pos+.25, seq_tot, seq_pos=seq_pos+0.25, pctDummy),
    log='xy', cex.labels = 1.5,
    lower.panel=NULL,
    panel=function(x,y){
      na.flg <- is.na(x) | is.na(y)
      points(x[!na.flg], y[!na.flg],
      pch=batchPch_vec[batch[!na.flg]],
      col=dil_levelFCol_vec[dil_levelF[!na.flg]])
    lines(lowess(x[!na.flg],y[!na.flg]), col='green', lwd=2)
  }
 ))


old_par =  par(xpd = NA)

legend(0.9, 5, legend = names(dil_levelFCol_vec), ncol = 2, 
text.col = dil_levelFCol_vec, title = "dil_levelF", title.col = 1, bty = 'n')


legend(1.3, 4.0, legend = names(batchPch_vec), ncol = 1,
pch = batchPch_vec, title = "batch", bty = 'n')

par(old_par)

dd_seq_tot_pos_cor_frm <- rbind(
seq = rev(seq_tot_pos_cor_vec), dd = rev(dd_tot_pos_cor_vec))

dd_seq_tot_pos_cor_mtx <- matrix(c(
" ", colnames(dd_seq_tot_pos_cor_frm),
"seq", dd_seq_tot_pos_cor_frm[1,],
"dd", dd_seq_tot_pos_cor_frm[2,]), 
byrow=T, ncol = ncol(dd_seq_tot_pos_cor_frm)+1)


old_par =  par(xpd = NA)

legend(1.0, 2.3, cex = 0.8, bty='n',
  title = "correlation between number of positives and total", 
  legend=as.vector(dd_seq_tot_pos_cor_mtx), 
  ncol = ncol(dd_seq_tot_pos_cor_frm)+1)

par(old_par)

dd_seq_tot_pos_cor_frm %>%
knitr::kable(
caption = "pos vs tot corr by dil_levelF"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>

## Fold-Change Summaries 

### HERE 

<p><p/>
* To compare to two platforms, and to compare competing contamination indices,
we will use fold change confidence intervals as a score -
larger point estimates of fold change between two groups at different dilution levels
with narrower confidence intervals are desirable.
   - to combine the two we can use the LCB of the fold change estimates.

```{r m5a-dilution-fc-estinmates, cache=T, cache.vars = 'XXX', fig.height= 8, fig.width = 11, fig.cap ="ddPCR and Seq Counts: LCB of Fold Change between dilution levels", eval=F}

analysis_frm <- ddpcr_seq_dilution_frm %>% 
  dplyr::filter(bin_cov > 50) %>%
  dplyr::mutate(
   dd_rate = dd_pos/dd_tot,
   seq_rate = seq_pos/seq_tot,
 ) %>%
dplyr::arrange(sample, batch, Well_96)

 # store dilution fold-change summaries here
ddpcr_seq_dil_fc_sum_frm <- data.frame()



METRICS <- c("dd_pos", "dd_rate", "seq_pos", "seq_rate")

par(mfrow=c(2,2), mar = c(5.5, 3.5, 2, 1))

for(MTR in METRICS) {
   
  analysis_frm[, "metric"] <- analysis_frm[, MTR]
  metric_by_dil_level_lst <- with(analysis_frm, split(metric+0.25, dil_levelF))
  batch_by_dil_level_lst <- with(analysis_frm, split(batch, dil_levelF))
  Well_96_by_dil_level_lst <- with(analysis_frm, split(Well_96, dil_levelF))
  
  boxplot(metric_by_dil_level_lst, type='n',log='y'

 # set par() outside
 #par(mar = c(5, 4, 4, 6), oma=c(2,0,0,0))
 boxplot(Var_by_batch_lst, las=2, xaxt='n',
        border=1,
        staplewex = 0,       # remove horizontal whisker lines
        outline = F,         # remove outlying points
        whisklty = 0,        # remove vertical whisker lines
        staplecol = "white", # just to be totally sure :)
        whiskcol = "white"   # dito)
  )
  if(Points)
  for(JJ in seq_along(Var_by_batch_lst))
  points(x = jitter(rep(JJ, length(Var_by_batch_lst[[JJ]])), amount=.20),
       y = Var_by_batch_lst[[JJ]],
       pch = 1, cex=1.0)

 extr_plateID_vec <- sapply(strsplit(names(Var_by_batch_lst), split=':'), '[', 2)
 axis(side=1, outer=F, at = 1:length(extr_plateID_vec), extr_plateID_vec, line=-1, tick=F)

 extr_cons_batchID_vec <- sapply(strsplit(names(Var_by_batch_lst), split=':'), '[', 1)
 extr_cons_batchID_ndx <- match(unique(extr_cons_batchID_vec), extr_cons_batchID_vec)
 abline(v = seq_along(Var_by_batch_lst)[extr_cons_batchID_ndx[-1]] - 0.5, 
        col='green', lty=2, lwd=2)
  
 axis(side=1, outer=F, at = extr_cons_batchID_ndx+2,
      labels=sub("B002", '', unique(extr_cons_batchID_vec)),
       tick=F, line=0, las=1)
 
 #mtext(side=1, outer=T, "Extraction Consolidated Batch")
 title(Title)

 return(Var_by_batch_lst)
 

```


<br/>
<br/>



# Load Sequencing Data { .tabset}

* This section is for documentation only.  

## Home {-}

<br/>
<br/>


## Read Seq Data

* When binary coverage or fragment counts appear multiple times
in a dataset, the columns are duplicates

* `Sample Id` has no consistency across batches, with some
batches having multiple Ids. 
   - To consolidate, `SampleID` will be the concatenation of the
different Ids.



```{r m5a-seq-data, cache=T, cache.vars=c("common_columns_vec", "seq_data_lst"), results = "asis", fig.cap = "Sequencing Data"}


seq_sheets_vec <- readxl::excel_sheets(with(params,
  file.path(conta_ddPCR_root, analysis_dir, seq_data_file))
) 

seq_data_lst <- lapply(seq_sheets_vec, function(SHEET)
with(params, 
 readxl::read_excel(
 path = file.path(conta_ddPCR_root, analysis_dir, seq_data_file),
 sheet = SHEET)
))
names(seq_data_lst) <- seq_sheets_vec

 # Continue
Keep_ndx <- which(sapply(seq_data_lst, ncol) > 1)

seq_data_lst <- seq_data_lst[Keep_ndx]
names(seq_data_lst) <- seq_sheets_vec[Keep_ndx]

 # Add probes_m_unconv
seq_data_lst <- lapply(seq_data_lst, function(frm) 
  {
   frm  %<>% dplyr::mutate(
    probes_m_unconv = fragment_counts_on_target_probe_count -
  fragment_counts_on_target_unconverted_probe_count)
    frm})


 # Find common columns
common_columns_vec <- Reduce(intersect, lapply(seq_data_lst, 
function(tbl) colnames(tbl)))

if(F)
data.frame(NumCol = quantile(sapply(seq_data_lst, ncol))) %>%
t() %>%
knitr::kable(
caption = paste("Number of columns for seq data - Common =", length(common_columns_vec))
) %>%
kableExtra::kable_styling(full_width = F)


 # list of uncommon columns
 #if(F)
uncommon_columns_lst <-
lapply(seq_data_lst, function(frm) setdiff(colnames(frm),  common_columns_vec))

names(uncommon_columns_lst) <- names(seq_data_lst)

if(F)
seq_data_lst[["seq_B001"]] %>% DT::datatable()


```

```{r m5a-sort-out-sample-id, results="asis", fig.cap = "Sorting Out Columns in Seq Data"}


for(BatchID in names(seq_data_lst)[1:5]) {

   if(F)
   cat("\n\n## ", BatchID, " {-}\n")

   batch_seq_frm <- as.data.frame(seq_data_lst[[BatchID]])
   sample_id_cols <- grep("Sample ",  names(batch_seq_frm))

   binary_cov_cols <- grep("on_binary_target_coverage_stats_mean",  names(batch_seq_frm)) 
   fragnent_counts_cols <- grep("fragment_counts_mapq60_la_filtered_singleton_filtered_count",
   names(batch_seq_frm))
   sex_cols <- grep("^sex", names(batch_seq_frm))
   y_norm_count_cols <- grep("y_norm_count", names(batch_seq_frm))

   tmp <- batch_seq_frm[1:10, c(sample_id_cols,binary_cov_cols, fragnent_counts_cols,
                                sex_cols, y_norm_count_cols)]                      
   colnames(tmp) <- c(paste0("samp", seq_along(sample_id_cols)),
                      paste0("bin_cov", seq_along(binary_cov_cols)),
                      paste0("fragment_count", seq_along(fragnent_counts_cols)),
                      paste0("^sex", seq_along(sex_cols)),
                      paste0("y_norm_count", seq_along(y_norm_count_cols))
                     )
                      
if(F)
print(
   tmp %>%
   knitr::kable( 
   caption = paste("Values for ", BatchID)
) %>%
kableExtra::kable_styling(full_width = F)
)

}

```

* Fix duplicated columns
   - keep the first of everything, even though the value if sex differs
between columns

```{r m5a-fix-columns, cache=T, cache.vars=c("common_columns_vec", "seq_data_lst"), results = "asis", fig.cap = "Sequencing Data"}

seq_data_lst <- lapply(seq_data_lst, function(FRM) {
   binary_cov_cols <- grep("on_binary_target_coverage_stats_mean",  names(FRM))
   names(FRM)[binary_cov_cols[1]] <- "on_binary_target_coverage_stats_mean"

   fragnent_counts_cols <- grep("fragment_counts_mapq60_la_filtered_singleton_filtered_count",
   names(FRM))
   names(FRM)[fragnent_counts_cols[1]] <- "fragment_counts_mapq60_la_filtered_singleton_filtered_count"
   # y_norm_count
   y_norm_count_cols <- grep("y_norm_count",  names(FRM))
   names(FRM)[y_norm_count_cols[1]] <- "y_norm_count"

   # sex  - keep the first also
   sex_cols <- grep("^sex",  names(FRM))
   names(FRM)[sex_cols[1]] <- "sex"

   # Add empty Plate_24  if missing
   FRM$Plate_24 = paste0('', FRM$Plate_24)

   FRM})

 # Recompute common columns
common_columns_vec <- Reduce(intersect, lapply(seq_data_lst,
function(tbl) colnames(tbl)))

```


<br/>
<br/>

## Consolidated Data {.tabset}

* For many applications the reads of samples being analyzed are best analyzed as a set.

* With that in mind, we will consolidate the all of the batches together as a set 
and partition the data by batch on a case by case basis, as needed.

* In the consolidated dataset, we need to keep the usual qc metrics, a sample
key identifier, which could differ between sets of batches run for different purposes,
and all of the metrics relevant to contamination analysis.

### Home {-}

<br/>
<br/>

### Key Fields {- .tabset}

* A key field is a unique sample identifier which can distinguish between
all of the samples in the dataset.

```{r m5a-key-fields, cache=T, cache.vars="key_fields_lst", results="asis"}

for(BatchID in names(seq_data_lst)) {

   cat("\n\n### ", BatchID, " {-}\n")

   batch_seq_frm <- seq_data_lst[[BatchID]]
   Key_fields_vec <- colnames(batch_seq_frm)[
   apply(batch_seq_frm, 2, function(CC) length(unique(CC))) ==
   nrow(batch_seq_frm)]

if(F)
print(
   batch_seq_frm[1:10, Key_fields_vec[1:min(10,length(Key_fields_vec))]] %>% 
   knitr::kable(
   caption = paste("Key Values for ", BatchID)
) %>%
kableExtra::kable_styling(full_width = F)
) 
  
}

```

* Key can be "Sample Id" or "Sample Id...15", or "Well_96"

```{r m5a-key-field, cache=T, cache.vars=c("common_columns_vec", "seq_data_lst"), results = "asis", fig.cap = "Sequencing Data"}

DEGUG <- function() {
sapply(seq_data_lst, function(FRM) 
c("Sample Id" %in% colnames(FRM), "Sample Id...15" %in% colnames(FRM)))

sapply(seq_data_lst, function(FRM) 
cbind(sample_id = FRM$`Sample Id`, 
      sample_id15 = FRM$`Sample Id...15`, 
      well_96 = FRM$Well_96)[1:5,])
}#DEBUG


seq_data_lst <- lapply(seq_data_lst, function(FRM)
 { if("Sample Id" %in% colnames(FRM)) 
     FRM[, "sampleID"] = FRM$`Sample Id` else
     if("Sample Id...15" %in% colnames(FRM)) 
     FRM[, "sampleID"] = FRM$`Sample Id...15` else
     FRM[, "sampleID"] = FRM$Well_96
     FRM %<>% dplyr::relocate(sampleID)
   FRM})


 # Recompute common columns
common_columns_vec <- Reduce(intersect, lapply(seq_data_lst,
function(tbl) colnames(tbl)))
     

```
 
<br/>
<br/>

### Aliasing {-}

* Recall:  conta_frac = cf = num_contamination_fragments/num_hom_fragments_adj

```{r m5a-short-alias, cache=T, cache.vars="metric_alias_vec"}

metric_alias_vec <- unlist(list(
  bin_cov = "analysis_on_binary_target_coverage_stats_mean",                 # $\ge$ 50
  unmapped_frac = "analysis_fragment_counts_unmapped_fraction",              # $\le$ 0.08
  zero_cov_targ = "analysis_zero_cov_targets_binary",                        # $\le$ 25
  on_target_unconv = "fragment_counts_on_target_unconverted_probe_count",    # $\le$  600
  probes_m_unconv  = "probes_m_unconv",                                      # $\le$ 5000 
          
  bsc_ratio = "analysis_filtered_bisulfite_conversion_ratio",                # $\ge$ 0.991 
  unconv_frac = "fragment_counts_unconverted_fraction",                      # $\le$ 0.01
  mapq60_la_filt_conta_frac = "fragment_counts_mapq60_la_filtered_barcode_conta_fraction",   # $\le$ 0.08
  conta_frac = "analysis_cf",                    # $\le$ ifelse CANCER_NOT_DETECTED 0.05, 0.001
  n_contam_frag = "num_contamination_fragments",
  n_contam_frag_no_sb = "num_contamination_fragments_no_sb",
  n_hom_frag_adj = "num_hom_fragments_adj",
  n_hom_frag_adj_no_sb = "num_hom_fragments_adj_no_sb",

  #sex_matches2 = "sex_matches2",                                               #  = TRUE
  abn_cov = "analysis_linear_filtered_with_singletons_abnormal_coverage_cpg_mean",  # $\ge$ 0.5 if CANCER_DETECTED

  hyper_abn_cov = "linear_filtered_with_singletons_abnormal_coverage_hyper_cpg_mean",
  hypo_abn_cov = "linear_filtered_with_singletons_abnormal_coverage_hypo_cpg_mean",

  hyper_total_cov = "linear_filtered_with_singletons_total_coverage_hyper_cpg_mean",
  hypo_total_cov = "linear_filtered_with_singletons_total_coverage_hypo_cpg_mean",

  total_cov = "total_coverage_cpg_mean",

  frag_counts = "analysis_fragment_counts_probe_count", 
  seq_quant = "sequencing_quant_inferred_undiluted_enrich_concentration",
  frag_lengths = "analysis_fragment_lengths_stats_mean",
  lib_quant = "lib_quant_quant_sample_concentration",
  extr_quant = "extr_quant_quant_sample_concentration",
  duplicate_frac = "analysis_fragment_counts_duplication_fraction",
  TMeF_CR = "mvaf_af_posterior_median_colon_rectum"
))


if(!params$analysis_Prefix)
metric_alias_vec <-  sub("^analysis_", '', metric_alias_vec)


setdiff(metric_alias_vec, common_columns_vec) %>%
knitr::kable(
caption = "Missing aliases removed from set"
) %>%
kableExtra::kable_styling(full_width = F)

metric_alias_vec <- metric_alias_vec[-which(!is.element(metric_alias_vec, common_columns_vec))]


data.frame(
alias = names(metric_alias_vec),
metric_name = metric_alias_vec,
row.names=NULL) %>%
knitr::kable(
caption = "Aliases to Metric Table Columns"
) %>%
kableExtra::kable_styling(full_width = F)

```

### Consolidate (and Clean-up) {-}

<!--
Follow nomenclature used in
[MCED-I Test Plate Data Summarization - burn_in Data](https://rstudio-connect.ti-apps.aws.grail.com/connect/#/apps/2ac8eac9-28aa-47e4-b7a4-9f617db82451/access).
-->

```{r m5a-consolidate, cache=T,  cache.vars=c("conta_seq_frm")}

conta_seq_frm <- do.call('rbind',lapply(names(seq_data_lst), function(BatchID)
data.frame(BatchID = BatchID, 
           seq_data_lst[[BatchID]][, common_columns_vec])))

conta_seq_frm %<>% 
dplyr::relocate(Created.Time:Analysis.Id, .after = last_col())
conta_seq_frm %<>% 
dplyr::relocate(Labels:Result.Location, .after = last_col())

conta_seq_frm %<>% 
dplyr::relocate(sex_error:sex_z_score, .after = "Col_96")

conta_seq_frm %<>%
dplyr::relocate(c(p_contamination, num_contamination_fragments:num_targets_hypo), 
    .after = "Col_96")

conta_seq_frm %<>%
dplyr::relocate(c(cancer_lod, cf, cf_no_sb, conta_call, conta_call_no_sb), 
    .after = "Col_96")

conta_seq_frm %<>%
dplyr::rename(batch = Data_batch)

conta_seq_frm %<>%
dplyr::mutate(Plate_24 = sub("P",'', Plate_24))

conta_seq_frm %<>%
dplyr::mutate(Sample = ifelse(is.na(Sample), 'N/A', Sample)) %>%
dplyr::mutate(batch = ifelse(is.na(batch), 'N/A', batch)) %>%
dplyr::mutate(Plate_24 = ifelse(is.na(Plate_24), 'N/A', Plate_24))


sample_labels_vec <- names(with(conta_seq_frm, rev(sort(table(Sample, exclude=NULL)))))
sample_labels_ord <- c("NA12878", "NA24631", "F", "PC3", "N/A",
setdiff(sample_labels_vec, c("NA12878", "NA24631", "F", "PC3", "N/A")))

conta_seq_frm %<>%
dplyr::mutate(Sample = factor(Sample, levels=sample_labels_ord))


if(F)
with(conta_seq_frm,  
table(batch, Sample, exclude=NULL)
) %>% t() %>%
knitr::kable(
caption = "Seq Data (is.na(Data_batch) is B002_old)"
) %>%
kableExtra::kable_styling(full_width = F)

```

* No clean-up.  The clean-up which we have code for requires standard, un-adulterated
pipeline outputs.

* Shorten the names and add `Sample` from 
merged ddPCR & Seq data frame


```{r m5a-shorten-names, cache=T,  cache.vars=c("conta_seq_frm")}

names(conta_seq_frm) <-
sub("num_", "n_",
sub("contamination", "contam",
sub("fragments", "frag",
names(conta_seq_frm))))

conta_seq_frm <- merge(
seq_ddpcr_metadata_frm %>% 
  dplyr::select(
     batch, Well_96, Test, sampleName, sample, pctMale, 
     Positives, Negatives, Accepted_droplets,
     cf_ddpcr, pctConta_byRatio, pctConta_byYRS,
     Ratio, `Fractional Abundance`, 
     `Ch1+Ch2+`, `Ch1+Ch2-`, `Ch1-Ch2+`, `Ch1-Ch2-`) %>%
  dplyr::rename(
   chPP = `Ch1+Ch2+`,
   chMM = `Ch1-Ch2-`,
   chMP = `Ch1-Ch2+`,
   chPM = `Ch1+Ch2-`),
conta_seq_frm,
by = c("batch", "Well_96"),
all.y = T)


if(F)
with(conta_seq_frm,
table(batch, paste0(sample))
) %>% t() %>%
knitr::kable(
caption = "Sample Descripion"
) %>%
kableExtra::kable_styling(full_width = F)



```


<br/>
<br/>


Annotation ...


```{r m5a-annot, cache=F, echo=T}

 ########################################
 # Annotation 
 ########################################

 # Option 3
grail_color <- c("#3B1C52","#94724F","#628592","#666666","#546741","#B73D27",
               "#D29A22","royalblue4","plum4","maroon4", "turquoise4", "tomato4")
col_vector <- rev(grail_color)


 # Option 2
suppressPackageStartupMessages(require(RColorBrewer))
qual_col_pals <- brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector <- unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
col_vector[4] <- 'red'
col_vector[8] <- 'darkgreen'

 # Option 1
KellyColors.vec <- c(
  "#222222", "#F3C300", "#875692", "#F38400", "#A1CAF1",
  "#BE0032", "#C2B280", "#848482", "#008856", "#E68FAC", "#0067A5",
  "#F99379", "#604E97", "#F6A600", "#B3446C", "#DCD300", "#882D17",
  "#8DB600", "#654522", "#E25822", "#2B3D26"
)
col_vector <- KellyColors.vec

 # recycle for more levels
 col_vector <- rep(col_vector, 3)

 Pch_v <- setdiff(1:50, 26:31)


 # Plate_24
Plate_24Legend_vec <- sort(unique(conta_seq_frm$Plate_24))
Plate_24Col_vec <- col_vector[1:length(Plate_24Legend_vec)]
names(Plate_24Col_vec) <- Plate_24Legend_vec
  
Plate_24Pch_vec <- Pch_v[1:length(Plate_24Legend_vec)]
names(Plate_24Pch_vec) <- Plate_24Legend_vec


 # BatchID
BatchIDLegend_vec <-  rev(sort(table(conta_seq_frm$BatchID)))
BatchIDCol_vec <- col_vector[1:length(BatchIDLegend_vec)]
names(BatchIDCol_vec) <- BatchIDLegend_vec

BatchIDPch_vec <- Pch_v[1:length(BatchIDLegend_vec)]
names(BatchIDPch_vec) <- BatchIDLegend_vec

 # sample
sampleLegend_vec <- rev(sort(table(conta_seq_frm$sample)))
sampleCol_vec <- col_vector[1:length(sampleLegend_vec)]
names(sampleCol_vec) <- sampleLegend_vec

 #sampleCol_vec["P05LJZC"] <- 'black'

samplePch_vec <- Pch_v[1:length(sampleLegend_vec)]
names(samplePch_vec) <- sampleLegend_vec


```


<br/>
<br/>

## Sample QC  {#sample-qc .tabset}

* Incomplete.  QC metrics have been added to the dataset but no
accounting of the verious modes of failure has been done.

* Return to this later if necessary.

### Home {-}

<br/>
<br/>

### Intro

* Before anything, we run the usual QC analyses.
   - This is critically important as the assay operates on the
sample prep; not the individual read groups.  
   - Shared variability due to systematic effects can often
be greater than the variability of interest.
   - Even when one is only interested in the patterns exhibited by
a single donor or sample of origin, an analysis which incorporates all
of the reads produced within a run is almost always superior to
a micro focused analysis of a single read group^[
this paradigm is probably still true in the case of a pool
of commercial test samples, although in that scenario
there are good reasons not to combine samples in some analyses.].

(Analysis here follows the example from
[MCED-I Test Plate Data Summarization - burn_in Data](https://rstudio-connect.ti-apps.aws.grail.com/content/2ac8eac9-28aa-47e4-b7a4-9f617db82451).)

<br/>

### Sample QC Rules 

```{r m5a-qc-metrics, cache=T, cache.vars=c("QCM_METRICS", "QCM_THRESHOLD", "QCM_SIGN"), eval = T}

QCM_METRICS <- c(
  metric_alias_vec["bin_cov"],
  metric_alias_vec["unmapped_frac"],
  metric_alias_vec["zero_cov_targ"],
  metric_alias_vec["on_target_unconv"],
  metric_alias_vec["probes_m_unconv"],
  metric_alias_vec["bsc_ratio"],
  metric_alias_vec["unconv_frac"],
  metric_alias_vec["mapq60_la_filt_conta_frac"],
  metric_alias_vec["conta_frac"]
  #metric_alias_vec["sex_matches2"]
  #metric_alias_vec["abn_cov"]
)


QCM_THRESHOLD <- c(
  bin_cov = 50            ,### 120,           ### 150,
  unmapped_frac = 0.08    ,### 0.10,    ### 0.28,
  zero_cov_targ = 25      ,### 34,      ### 237,

  on_target_unconv = 600  ,
  probes_m_unconv = 5000 ,
  bsc_ratio = 0.991,

  unconv_frac = 0.01,
  mapq60_la_filt_conta_frac = 0.08,
  conta_frac = "ifelse(cancer_detected, .001, .05)"     ## .001 if CANCER_DETECTED
  ###sex_matches = 0
  ###abn_cov = 0.5        ## if CANCER_DETECTED 
  )



QCM_SIGN <- c(
  bin_cov = -1,
  unmapped_frac = 1,
  zero_cov_targ = 1,

  on_target_unconv = 1,
  probes_m_unconv = 1,
  bsc_ratio = -1,

  unconv_frac = 1, 
  mapq60_la_filt_conta_frac = 1,
  conta_frac = 1
  #sex_matches = 1
  #abn_cov = -1
)

```


```{r m5a-metrics, results = 'hold', eval=T}

qc_metrics_frm <- data.frame(do.call("rbind",lapply(1:length(QCM_METRICS),
function(JJ) data.frame(Num = JJ, name = names(QCM_METRICS)[JJ],
   rule = paste(QCM_METRICS[JJ], ifelse(QCM_SIGN[JJ]>0, '>', '<'), QCM_THRESHOLD[JJ])
))))

qc_metrics_frm %>%
knitr::kable(
caption = "Flagged samples will be included in plots but excluded from algorighm development"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>

* Add QC calls to  `conta_seq_frm`



```{r m5a-add-qc, cache=T, cache.vars=c('qc_error_code_frm', 'qc_error_modes_frm','v3a_burn_in_frm')}

QCM_THRESHOLD <- sapply(QCM_THRESHOLD, as.numeric)


qc_fail_mtx <- do.call('cbind', lapply(setdiff(names(QCM_METRICS),'cf'),
  ## do cf separately
  function(QCname) {
cat(QCname, '\n')
  ifelse(is.na(conta_seq_frm[, QCM_METRICS[QCname]]), TRUE,
  ((conta_seq_frm[, QCM_METRICS[QCname]] - QCM_THRESHOLD[QCname]) * QCM_SIGN[QCname]) > 0)
 }
 ))
colnames(qc_fail_mtx) <- paste0(setdiff(names(QCM_METRICS),'cf'), '_qcm')

failMode_vec <- apply(qc_fail_mtx, 1, function(RR) letters[which(RR)])
failMode_vec <- sapply(failMode_vec, paste, collapse='')

conta_seq_frm  <- 
data.frame(qc_fail_mtx, conta_seq_frm) %>%
dplyr::mutate(
  qcFail = rowSums(qc_fail_mtx, na.rm=T) > 0,
  failMode = failMode_vec
) %>%
dplyr::relocate(failMode, qcFail, dplyr::all_of(colnames(qc_fail_mtx)))

failMode_tbl <- with(conta_seq_frm, table(failMode, exclude = NULL))

qc_error_modes_frm <- as.data.frame(
failMode_tbl[order(failMode_tbl, decreasing = T)]) %>%
dplyr::filter(
  failMode != ''
) %>%
dplyr::mutate(
  failMode2 = ifelse(Freq > 40, as.character(failMode), "Other")
)

 
qc_error_modes_frm[1:10, ] %>% dplyr::select(-failMode2) %>%
t() %>%
knitr::kable(
caption = "Top Ten Error modes"
) %>%
kableExtra::kable_styling(full_width = F)

qc_error_code_frm <-
data.frame(Code = letters[1:ncol(qc_fail_mtx)], 
error = colnames(qc_fail_mtx))


qc_error_code_frm %>%
knitr::kable(
caption = "Error Codes - we are counting conta here"
) %>%
kableExtra::kable_styling(full_width = F)

```


<br/>
<br/>

### Failure Counts 


```{r m5a-troublshoot, eval=F}

colSums(is.na(conta_seq_frm %>% dplyr::select(
 pc3_batch_qc_fail,
 nc3_batch_qc_fail,
 binary_fail, 
 abnormal_fail,
 sex_fail
 )) , na.rm = T) %>%
knitr::kable(
caption = "Total Number of missing values "
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>

* qcFail Counts {- .tabset}

* RETURN TO THIS LATER

<!--
#### Table {-}
-->
```{r m5a-qc-counts-by-ecb, cache=T, cache.vars=c('qc_counts_by_ecb', 'exclSampleID_vec', 'analysis_frm'), eval=F}

qc_counts_by_ecb <- conta_seq_frm %>% 
  ####dplyr::filter(qcFail) %>%
  dplyr::group_by(batch,  Plate_24) %>%
  dplyr::summarize(
   N = dplyr::n() ,
   qcFail = sum(qcFail, na.rm=T),
   bin_cov_qcm = sum(bin_cov_qcm),
   unmapped_frac_qcm = sum(unmapped_frac_qcm),
   zero_cov_targ_qcm = sum(zero_cov_targ_qcm),
   on_target_unconv_qcm = sum(on_target_unconv_qcm),
   probes_m_unconv_qcm = sum(probes_m_unconv_qcm),
   bsc_ratio_qcm = sum(bsc_ratio_qcm),
   unconv_frac_qcm = sum(unconv_frac_qcm),
   mapq60_la_filt_conta_frac_qcm = sum(mapq60_la_filt_conta_frac_qcm)
)  %>% as.data.frame()

colSums(conta_seq_frm %>% dplyr::select(qcFail:mapq60_la_filt_conta_frac_qcm)) %>%
knitr::kable(
caption = paste("Total Number of samples failing QC.  N =", nrow(conta_seq_frm))
) %>%
kableExtra::kable_styling(full_width = F)

```


```{r m5a-table-qc-counts, cache=T, cache.vars='', fig.height=8, fig.width=11, fig.cap="tdg2 failure counts by batch", eval=F}

tmp <- qc_counts_by_ecb %>%
janitor::adorn_totals()

rbind(tail(tmp, 1), tmp) %>%
knitr::kable(
caption = paste("failing QC batch sample count")
) %>%
kableExtra::kable_styling(full_width = F)

```




<br/>
<br/>

```{r m5a-dt-datatable-qc-counts-by-ecb, eval = DT_DATATABLE, eval=F}
qc_counts_by_ecb %>% 
  DT::datatable(extensions = 'Buttons',
  caption = "Failure breakdown by ECB",
            options = list(dom = 'Blfrtip',
                           buttons = c('copy', 'csv', 'excel')))

```


<br/>
<br/>

## Define Functions and Annotations {-}


Define functions ...


```{r m5a-define-plot-fun, cache=T, cache.vars=c("plot_cov_by_batch_f",  "plot_cov_by_batch_f2")}
 ############################################
 ## adapted from panel.cor in ?pairs
panel.cor <- function(x, y, digits=2, cex.reg)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))

  r_vec <- sapply(split(data.frame(x, y), PlateNum),
    function(frm) cor(log(frm[,1]), log(frm[,2])))

legend("top",
  legend = paste(names(r_vec), '=', round(r_vec, 2)),
  text.col = as.numeric(names(r_vec)),
  title="plate corr", bty = 'n')

}

 ############################################
DEBUG <- function() {

   Metric = "bin_cov"
   Title =  "bin_cov"
   Ylim = c(2, 250)

  data_frm = v3a_burn_in_frm %>% 
   dplyr::filter(!is.na(abn_cov), !is.na(bin_cov)) %>%
   dplyr::filter(sample_type == "STUDY")   %>%
   dplyr::mutate(abn_cov = pmax(abn_pos_q05, abn_cov))

   Metric = "abn_cov"
   Title =  "abn_cov"
   Ylim = c(0.001, 2)
   Xaxis = T
   LvlLegend=T
 addControls=F
annotLegend = F

addCancerNotDetected=F

 annotCol = "Plate_24"; annotPch = "sample"
}#DEBUG


plot_cov_by_batch_f <- function(
  data_frm,
  Metric = "bin_cov", Title = NULL, 
  Xaxis=F, annotLegend = T,
  Ylim = NULL, 
  annotCol = "Plate_24", annotPch = "sample") {

  data_frm[, "metric"] <- data_frm[, Metric]
  data_frm[, "annotCol"] <- data_frm[, annotCol]
  data_frm[, "annotPch"] <- data_frm[, annotPch]

  data_frm %<>% dplyr::mutate(metric = pmax(metric, 25))

  metric_by_batch_lst <- with(data_frm,
  split(metric, batch))

  annotCol_by_batch_lst <- with(data_frm,
  split(annotCol, batch))
  annotPch_by_batch_lst <- with(data_frm,
  split(annotPch, batch))

   # annotCol
  annotColLegend_vec <- sort(unique(data_frm$annotCol))
  annotCol_vec <- col_vector[1:length(annotColLegend_vec)]
  names(annotCol_vec) <- annotColLegend_vec

   # annotPch
  tmp <- rev(sort(table(data_frm$annotPch, exclude=NULL)))
  annotPchLegend_vec <- names(tmp)
  annotPch_vec <- Pch_v[1:length(annotPchLegend_vec)]
  names(annotPch_vec) <- annotPchLegend_vec

  annotPch_vec <- ifelse(annotPch_vec == 8, 13, annotPch_vec)
  annotPch_vec <- ifelse(annotPch_vec == 11, 14, annotPch_vec)

  boxplot(metric_by_batch_lst, las=2, xaxt='n', log='y',  ### ylim=Ylim,
          border=1, col = 0,   # do print the box; a mixture  is ok for bin cov
          staplewex = 0,       # remove horizontal whisker lines
          outline = F,         # remove outlying points
          whisklty = 0,        # remove vertical whisker lines
          staplecol = "white", # just to be totally sure :)
          whiskcol = "white"   # dito)
    )
  x_jit_lst <- lapply(metric_by_batch_lst, function(x) jitter(rep(0, length(x)), amount=0.20))

  title(Title)
  
  for(JJ in seq_along(metric_by_batch_lst))
  points(x = x_jit_lst[[JJ]] + JJ,
         y = metric_by_batch_lst[[JJ]],
         pch = annotPch_vec[annotPch_by_batch_lst[[JJ]]],
         col = annotCol_vec[annotCol_by_batch_lst[[JJ]]]
  )
  
  axis(side=1, at = seq_along(metric_by_batch_lst), names(metric_by_batch_lst))
  


   if(annotLegend){
old_par =  par(xpd = NA)
   legend("topright", inset=c(-.20, 0), ncol=2,
   title = annotCol,
   legend = names(annotCol_vec),
   pch = 19, 
   col = annotCol_vec, bty='n'
   )
   legend("bottomright", inset=c(-.35, .0), ncol=2,
   title = annotPch,
   legend = names(annotPch_vec),
   pch = annotPch_vec, 
   col = 1, bty='n'
   )
par(old_par)
  }

  return(invisible(metric_by_batch_lst))
}# plot_cov_by_batch_f
 ##############################################

DEBUG <- function() {

  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(abn_cov), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(cfPlus = cf + 1e-6)
 # cf
  Metric = "cfPlus"; Title = "cf+1e-6";
  Xaxis=T;  
  #Ylim = c(.001;2.00);
  annotColLegend = F; annotPchLegend = F;
  annotOneLegend = T;
  annotCol = "sample"; annotPch = "sample"
  
}

plot_cov_by_batch_f2 <- function(
  data_frm, Metric, Title = '', Xaxis = F, 
  Yaxt = 's', Ylim=NULL,
  annotColLegend = T, annotPchLegend = T,
  annotOneLegend = F, legend_cx = 1,
  annotCol = "Plate_24", annotPch = "sample") {

  min_pos <- min(data_frm[, Metric][data_frm[, Metric]>0])
  if(F)
  data_frm[, "metric"] <- pmin(0.2, pmax(min_pos/10, data_frm[, Metric]))

  data_frm[, "metric"] <- data_frm[, Metric]

  data_frm[, "annotCol"] <- data_frm[, annotCol]
  data_frm[, "annotPch"] <- data_frm[, annotPch]

  ###data_frm %<>% dplyr::mutate(metric = pmax(metric, 25))

  metric_by_batch_lst <- with(data_frm,
  split(metric, batch))

  annotCol_by_batch_lst <- with(data_frm,
  split(annotCol, batch))
  annotPch_by_batch_lst <- with(data_frm,
  split(annotPch, batch))

   # annotCol
  annotColLegend_vec <- sort(unique(data_frm$annotCol))
  annotCol_vec <- col_vector[1:length(annotColLegend_vec)]
  names(annotCol_vec) <- annotColLegend_vec

   # annotPch
  tmp <- rev(sort(table(data_frm$annotPch, exclude=NULL)))
  annotPchLegend_vec <- names(tmp)
  
  if(annotCol == annotPch)
  annotPchLegend_vec <- annotColLegend_vec

  annotPch_vec <- Pch_v[1:length(annotPchLegend_vec)]
  names(annotPch_vec) <- annotPchLegend_vec

  annotPch_vec <- ifelse(annotPch_vec == 8, 13, annotPch_vec)
  annotPch_vec <- ifelse(annotPch_vec == 11, 14, annotPch_vec)

  boxplot(metric_by_batch_lst, las=2, xaxt='n', log='y',  ### ylim=Ylim,
          yaxt = Yaxt,
          border=1, col = 0,   # do print the box; a mixture  is ok for bin cov
          staplewex = 0,       # remove horizontal whisker lines
          outline = F,         # remove outlying points
          whisklty = 0,        # remove vertical whisker lines
          staplecol = "white", # just to be totally sure :)
          whiskcol = "white"   # dito)
    )
  x_jit_lst <- lapply(metric_by_batch_lst, function(x) jitter(rep(0, length(x)), amount=0.20))

  title(Title)
  
  for(JJ in seq_along(metric_by_batch_lst))
  points(x = x_jit_lst[[JJ]] + JJ,
         y = metric_by_batch_lst[[JJ]],
         pch = annotPch_vec[annotPch_by_batch_lst[[JJ]]],
         col = annotCol_vec[annotCol_by_batch_lst[[JJ]]]
  )

  if(Xaxis)  
  axis(side=1, at = seq_along(metric_by_batch_lst), names(metric_by_batch_lst))

old_par =  par(xpd = NA)
 if(annotOneLegend) 
   legend("topright", inset=c(-.22, 0), ncol=2,
   title = annotCol, cex = legend_cx,
   legend = names(annotCol_vec),
   pch = annotPch_vec,
   col = annotCol_vec, bty='n'

   ) else 
{
 if(annotColLegend)
   legend("topright", inset=c(-.20, 0), ncol=2,
   title = annotCol,
   legend = names(annotCol_vec),
   pch = 19, 
   col = annotCol_vec, bty='n'
   )
Pch_vec <- sort(names(annotPch_vec))
 if(annotPchLegend)
   legend("bottomright", inset=c(-.20, .00), ncol=2,
   title = annotPch,
   legend = Pch_vec,
   pch = annotPch_vec[Pch_vec], 
   col = 1, bty='n'
   )
}
par(old_par)


  return(invisible(metric_by_batch_lst))
}# plot_cov_by_batch_f2
##########################################

``` 

```{r m5a-check-density, eval=F}

 # uv is unform with variability =
Uv_f = function(x, v) 1 - v/2  + v*x


summary_frm <- do.call('rbind', lapply(c(0, 5, 10, 20, 40, 50)/100,
function(vv) {
   
  density_v = integrate(Uv_f, 0, 1, v = vv)$value
  if(density_v != 1) stop("density check error")

  mean_v = integrate(function(x,v) x * Uv_f(x, v), 0 , 1, v = vv)$value
  secondMoment_v = integrate(function(x,v) x^2 * Uv_f(x, v), 0 , 1, v = vv)$value
  var_v = secondMoment_v - mean_v^2

  data.frame(v = vv,  mean = round(mean_v, 3), var = round(var_v, 3))
 }
))


 #
Uv_20_vec <- GoFKernel::random.function(n=10000, f = function(x) Uv_f(x, .20),
   lower=0, upper=1)
hist(Uv_20_vec)
qqplot(Uv_20_vec, runif(10000))
abline(0, 1, col ='red')

Uv_40_vec <- GoFKernel::random.function(n=10000, f = function(x) Uv_f(x, .40),
   lower=0, upper=1)
hist(Uv_40_vec)
qqplot(Uv_40_vec, runif(10000))
abline(0, 1, col ='red')

plot(density(Uv_40_vec))
lines(density(runif(10000)), col="blue")

Uv_10_vec <- GoFKernel::random.function(n=10000, f = function(x) Uv_f(x, .10),
   lower=0, upper=1)
hist(Uv_10_vec)

```


<br/>
<br/>


Shorten names ,,,
```{r m5a-shorten-names-2, cache=T, cache.vars = "conta_seq_frm"}

conta_seq_frm %<>% dplyr::mutate(
  bin_cov = on_binary_target_coverage_stats_mean,                 # $\ge$ 50
  unmapped_frac = fragment_counts_unmapped_fraction,              # $\le$ 0.08
  zero_cov_targ = zero_cov_targets_binary,                        # $\le$ 25
  on_target_unconv = fragment_counts_on_target_unconverted_probe_count,    # $\le$  600
  probes_m_unconv  = probes_m_unconv,                                      # $\le$ 5000 
          
  bsc_ratio = filtered_bisulfite_conversion_ratio,                # $\ge$ 0.991 
  unconv_frac = fragment_counts_unconverted_fraction,                      # $\le$ 0.01
  mapq60_la_filt_conta_frac = fragment_counts_mapq60_la_filtered_barcode_conta_fraction,   # $\le$ 0.08
  conta_frac = cf,                    # $\le$ ifelse CANCER_NOT_DETECTED 0.05, 0.001
  #n_contam_frag = n_contam_frag,
  #n_contam_frag_no_sb = n_contam_frag_no_sb,
  #n_hom_frag_adj = n_hom_frag_adj ,
  #n_hom_frag_adj_no_sb = n_hom_frag_adj_no_sb,

  #sex_matches2 = sex_matches2,                                               #  = TRUE
  abn_cov = linear_filtered_with_singletons_abnormal_coverage_cpg_mean,  # $\ge$ 0.5 if CANCER_DETECTED

  hyper_abn_cov = linear_filtered_with_singletons_abnormal_coverage_hyper_cpg_mean,
  hypo_abn_cov = linear_filtered_with_singletons_abnormal_coverage_hypo_cpg_mean,

  hyper_total_cov = linear_filtered_with_singletons_total_coverage_hyper_cpg_mean,
  hypo_total_cov = linear_filtered_with_singletons_total_coverage_hypo_cpg_mean,

  total_cov = total_coverage_cpg_mean,

  frag_counts = fragment_counts_probe_count, 
  #seq_quant = sequencing_quant_inferred_undiluted_enrich_concentration,
  frag_lengths = fragment_lengths_stats_mean,
  #lib_quant = lib_quant_quant_sample_concentration,
  #extr_quant = extr_quant_quant_sample_concentration,
  duplicate_frac = fragment_counts_duplication_fraction,
  TMeF_CR = mvaf_af_posterior_median_colon_rectum,
  dummy = "DUMMY:"
)


```

<br/>
<br/>

# Examine Sequencing Data {#exam-seq-data .tabset}

* In this section we just include various data displays.
See Section \@ref(ddpcr-seq-dilution) for tabulations of summaries.

## Home {-}

```{r m5a-consolidated-data, fig.cap = "Seq Data Batch Composition"}

batch_tbl <- with(conta_seq_frm,  
table(batch, sample, exclude=NULL)
) %>% t() 

if(F)
batch_tbl[rev(order(rowMeans(batch_tbl))),] %>%
knitr::kable(
caption = "Seq Data (is.na(Data_batch) is B002_old)"
) %>%
kableExtra::kable_styling(full_width = F)

```

## List of Studies {-}


```{r m5a-list-studies, cache=T, cache.vars='', fig.height=6, fig.width=8, fig.cap="List of Studies", results = "asis"}

conta_seq_studies_frm <- conta_seq_frm %>%
dplyr::select(batch, Test) %>%
dplyr::distinct()  %>%
dplyr::filter(batch != "N/A", !is.na(Test))

conta_seq_studies_frm %>% t() %>%
knitr::kable(
caption = "List of Studies (batches)"
) %>%
  kableExtra::kable_styling(full_width = F)



```



## CF Pairs Plots {.tabset}

```{r m5a-cf-pairs, cache=T, cache.vars='', fig.height=6, fig.width=8, fig.cap="Contamination Fraction Features", results = "asis"}

 # Plate_24
Plate_24Legend_vec <- sort(unique(conta_seq_frm$Plate_24))
Plate_24Col_vec <- col_vector[1:length(Plate_24Legend_vec)]
names(Plate_24Col_vec) <- Plate_24Legend_vec
  
Plate_24Pch_vec <- Pch_v[1:length(Plate_24Legend_vec)]
names(Plate_24Pch_vec) <- Plate_24Legend_vec

 # sample
sampleLegend_vec <- sort(unique(conta_seq_frm$sample))
sampleCol_vec <- col_vector[1:length(sampleLegend_vec)]
names(sampleCol_vec) <- sampleLegend_vec
  
samplePch_vec <- Pch_v[1:length(sampleLegend_vec)]
names(samplePch_vec) <- sampleLegend_vec
 
  data_frm = conta_seq_frm %>%
    dplyr::filter(!is.na(abn_cov), !is.na(bin_cov), cf < 0.2) 
  data_frm$cf <- pmax(1e-6, data_frm$cf)

for(BATCH in setdiff(unique(data_frm$batch), "N/A")) {
  batch_data_frm <- data_frm %>% 
     dplyr::filter(batch==BATCH) %>%
   dplyr::mutate( # recompute cf - missing at times
     cf0 = cf,
     cf = n_contam_frag / n_hom_frag_adj
   )

  TEST <- paste(unique(batch_data_frm$Test), collapse = '; ')

  cat("\n\n### ", BATCH, "  {- .tabset}\n")
  cat("\n\n* ",  TEST, "\n")

  cat("\n\n#### Plot {-}\n")
if(!(BATCH %in% c("B012", "B013"))){
  with(batch_data_frm, 
    pairs(cbind(cf, n_contam_frag = n_contam_frag+0.25, n_hom_frag_adj, n_hom_frag),
    log='xy', cex.labels = 1.5, 
    lower.panel=NULL,
    panel=function(x,y){
     na.flg <- is.na(x) | is.na(y)
      points(
        x[!na.flg], y[!na.flg],
        col=Plate_24Col_vec[Plate_24[!na.flg]],
        pch=samplePch_vec[sample[!na.flg]]
      )
    lines(lowess(x[!na.flg],y[!na.flg]), col='green', lwd=2)
  }
 ))


old_par <- par(xpd=NA)

  legend(1, 5, title="Plate_24", legend=names(Plate_24Col_vec), 
  col = Plate_24Col_vec, pch=19, bty='n', ncol=2)

  Pch_vec <- with(batch_data_frm, sort(unique(sample)))

  legend(1, 3, title="sample", legend=Pch_vec, ncol=2,
         col = 1, pch = samplePch_vec[Pch_vec], bty='n')
par(old_par)
} else {  # batch 12, 13
 with(batch_data_frm,
    pairs(cbind(cf, n_contam_frag = n_contam_frag+1/2, n_hom_frag_adj, n_hom_frag),
    log='xy', cex.labels = 1.5,
    lower.panel=NULL,
    panel=function(x,y){
      na.flg <- is.na(x) | is.na(y)
      points(x[!na.flg], y[!na.flg],
      pch=Plate_24Pch_vec[Plate_24[!na.flg]],
      col=sampleCol_vec[sample[!na.flg]])
    lines(lowess(x[!na.flg],y[!na.flg]), col='green', lwd=2)
    }
 ))

SAMPLES <- with(batch_data_frm,  unique(sample))
old_par <- par(xpd=NA)
  legend(1, 5, title="sample", legend=SAMPLES,
  col = sampleCol_vec[SAMPLES], pch=19, bty='n', ncol=2)

# Pch_vec <- with(batch_data_frm, sort(unique(sample)))
# legend(1, 3, title="sample", legend=Pch_vec, ncol=2,
#        col = 1, pch = samplePch_vec[Pch_vec], bty='n')
par(old_par)
}


mtext( side = 1, outer = T, line = -3, 
paste0(BATCH, '-', TEST)) 

if(BATCH == "B002")
mtext( side = 1, outer = T, line = -2,
"Note: Plate_24 == 3 has lower n_hom_frag_adj values, leading to inflated cf estimates")

if(BATCH == "B005")
mtext( side = 1, outer = T, line = -2,
"Note: Plate_24 == 3 has higher n_hom_frag_adj values")


cat("<br/>\n")
cat("<br/>\n")

  
    cat("\n\n#### Tables {- .tabset}\n")

    cat("\n\n##### Summary {-} \n")

   batch_data_sum_frm <- batch_data_frm %>%
   dplyr::select(batch, Well_96, sampleName, sample,
          cf, n_contam_frag, n_hom_frag_adj, bin_cov) %>%
   dplyr::group_by(sample) %>%
   dplyr::summarize(
     N = dplyr::n(),
     cf_mean  = exp(mean(log(cf + 1e-6), na.rm=T)),
     cf_mad = mad(cf, na.rm=T),
     contam_frag_mean = exp(mean(log(n_contam_frag+0.5), na.rm=T)), 
     contam_frag_mad = mad(n_contam_frag, na.rm=T), 
     contam_frag_root_mean = sqrt(mean(n_contam_frag, na.rm=T)) 
    )
         
print(
     batch_data_sum_frm %>%
     knitr::kable(
       caption = paste("Data for Batch - Test", BATCH, '-', TEST)
     ) %>%
       kableExtra::kable_styling(full_width = F)
)

   
    cat("\n\n##### Detailed {-} \n")

print(
with(batch_data_frm,
data.frame(row = 1:nrow(batch_data_frm), batch, Well_96, sampleName, sample, 
cf, n_contam_frag, n_hom_frag_adj, bin_cov)
) %>%
knitr::kable(
#caption = paste("Data for Batch - Test", BATCH, '-', TEST)
) %>%
kableExtra::kable_styling(full_width = F)
)

}

```

<br/>
<br/>

## CF, n_contam_frag  

```{r m5a-cf-by-Data-batch, cache=F, cache.vars='bin_by_batch_lst', fig.height=6, fig.width=10, fig.cap="CF: By Extraction Consolidated Batch"}
 ###

par(mfrow=c(2, 1), mar = c(2, 4.5, 2, 1), oma=c(4,0,2,12))

cfPlus_seq_frm <- plot_cov_by_batch_f2(
  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(abn_cov), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(
      cfPlus = cf + 1e-6,
      n_contam_fragPlus = n_contam_frag + 0.25
),
 # cf
  Metric = "cfPlus", Title = "cf+1e-6",
  Xaxis=T, 
  #Ylim = c(.001,2.00), 
  annotColLegend = F, annotPchLegend = F,
  annotOneLegend = T, legend_cx = 0.7,
  annotCol = "sample", annotPch = "sample")

cfPlus_seq_frm <- plot_cov_by_batch_f2(
  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(abn_cov), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(
      cfPlus = cf + 1e-6,
      n_contam_fragPlus = n_contam_frag + 0.25
),
 # n_contam_fragPlus
  Metric = "n_contam_fragPlus", Title = "n_contam_frag + 0.25",
  Xaxis=T,
  #Ylim = c(.001,2.00),
  annotColLegend = F, annotPchLegend = F,
  annotOneLegend = F,
  annotCol = "sample", annotPch = "sample")




```

<br/>
<br/>

## CF, n_contam, n_hom_frag_adj

<p><p/>
* The plate effects which are apparent in the figure showing the binary coverage variability
are not discernible in the figure displaying the CF variability as shown below.
   - this is due to the fact that CF is a ratio which in effect removes
the technical variability which may be most clearly seen in 
binary coverage variability
   - CF is an adjusted contaminant count which might be improved by a regression
analysis.


```{r m5a-cf-n-contam-bin-by-Data-batch, cache=F, cache.vars='bin_by_batch_lst', fig.height=6, fig.width=10, fig.cap="CF: By Extraction Consolidated Batch"}
 ###

DEBUG <- function() {
with(conta_seq_frm %>%
   dplyr::filter(!is.na(n_contam_frag), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(n_contam_fragPlus =n_contam_frag+0.25),
split(n_contam_fragPlus, batch))

  min_pos <- min(data_frm[, Metric][data_frm[, Metric]>0])
  if(F)
  data_frm[, "metric"] <- pmin(0.2, pmax(min_pos/10, data_frm[, Metric]))

}#DEBUG

par(mfrow=c(3, 1), mar = c(0, 4.5, 2, 1), oma=c(4,0,2,10))

plot_cov_by_batch_f2(
  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(n_contam_frag), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(cfplus = cf + 1e-6),
 # cf
  Metric = "cfplus", Title = "cf + 1e-6",
  Xaxis=F, 
  #Ylim = c(.001,2.00), 
  annotColLegend = F, annotPchLegend = T,
  annotCol = "Plate_24", annotPch = "sample")

plot_cov_by_batch_f2(
  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(n_contam_frag), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(n_contam_fragPlus =n_contam_frag+0.25),
 # n_contam_frag
  Metric = "n_contam_fragPlus", Title = "n_contam_frag + 0.25",
  Xaxis=F,  Yaxt='n',
  #Ylim = c(.001,2.00), 
  annotColLegend = T, annotPchLegend = F,
  annotCol = "Plate_24", annotPch = "sample")

plot_cov_by_batch_f2(
  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(n_contam_frag), !is.na(bin_cov), cf < 0.2) %>%
   dplyr::mutate(
     n_contam_fragPlus = n_contam_frag+0.25,
     n_hom_frag_adjPlus = n_hom_frag_adj+0.25,
   ),          
 # n_contam_frag
  Metric = "n_hom_frag_adjPlus", Title = "n_hom_frag_adj + 0.25",
  Xaxis=T,
  #Ylim = c(.001,2.00),
  annotColLegend = F, annotPchLegend = F,
  annotCol = "Plate_24", annotPch = "sample")

```

<br/>
<br/>


## Binary Cov {-}


```{r m5a-cov-by-Data-batch, cache=F, cache.vars='bin_by_batch_lst', fig.height=6, fig.width=10, fig.cap="Binary Cov: By Extraction Consolidated Batch"}
 ###

par(mfrow=c(1, 1), mar = c(0, 4.5, 2, 1), oma=c(4,0,2,10))


 #plot_cov_by_batch_f(

  data_frm = conta_seq_frm %>%
   dplyr::filter(!is.na(abn_cov), !is.na(bin_cov)) 
   
 # bin_cov
  Metric = "bin_cov"; Title = "bin_cov (floored at 25)"; 
  Xaxis=F; CtlLegend=F;  annotLegend = T; 
  #Ylim = c(.001;2.00); 
  addControls=F; addCancerNotDetected=F
  

  annotLegend = T; annotCol = "Plate_24"; annotPch = "sample";

  data_frm[, "metric"] <- data_frm[, Metric]
  data_frm[, "annotCol"] <- data_frm[, annotCol]
  data_frm[, "annotPch"] <- data_frm[, annotPch]

  data_frm %<>% dplyr::mutate(metric = pmax(metric, 25))

  metric_by_batch_lst <- with(data_frm,
  split(metric, batch))

  annotCol_by_batch_lst <- with(data_frm,
  split(annotCol, batch))
  annotPch_by_batch_lst <- with(data_frm,
  split(annotPch, batch))

   # annotCol
  annotColLegend_vec <- sort(unique(data_frm$annotCol))
  annotCol_vec <- col_vector[1:length(annotColLegend_vec)]
  names(annotCol_vec) <- annotColLegend_vec

   # annotPch
  tmp <- rev(sort(table(data_frm$annotPch, exclude=NULL)))
  annotPchLegend_vec <- names(tmp)
  annotPch_vec <- Pch_v[1:length(annotPchLegend_vec)]
  names(annotPch_vec) <- annotPchLegend_vec

  annotPch_vec <- ifelse(annotPch_vec == 8, 13, annotPch_vec)
  annotPch_vec <- ifelse(annotPch_vec == 11, 14, annotPch_vec)

  boxplot(metric_by_batch_lst, las=2, xaxt='n', log='y',  ### ylim=Ylim,
          border=1, col = 0,   # do print the box; a mixture  is ok for bin cov
          staplewex = 0,       # remove horizontal whisker lines
          outline = F,         # remove outlying points
          whisklty = 0,        # remove vertical whisker lines
          staplecol = "white", # just to be totally sure :)
          whiskcol = "white"   # dito)
    )
  x_jit_lst <- lapply(metric_by_batch_lst, function(x) jitter(rep(0, length(x)), amount=0.20))

  title(Title)
  
  for(JJ in seq_along(metric_by_batch_lst))
  points(x = x_jit_lst[[JJ]] + JJ,
         y = metric_by_batch_lst[[JJ]],
         pch = annotPch_vec[annotPch_by_batch_lst[[JJ]]],
         col = annotCol_vec[annotCol_by_batch_lst[[JJ]]]
  )
  
  axis(side=1, at = seq_along(metric_by_batch_lst), names(metric_by_batch_lst))
  

   if(annotLegend){
old_par =  par(xpd = NA)
   legend("topright", inset=c(-.20, 0), ncol=2,
   title = annotCol,
   legend = names(annotCol_vec),
   pch = 19, 
   col = annotCol_vec, bty='n'
   )
   legend("bottomright", inset=c(-.30, .0), ncol=2,
   title = annotPch,
   legend = names(annotPch_vec),
   pch = annotPch_vec, 
   col = 1, bty='n'
   )
par(old_par)
  }

 #  return(invisible(metric_by_batch_lst))
 #}# plot_cov_byECB_f

```


<br/>
<br/>

# Sequencing Data Analysis: Background Batches {#seq-data-analysis-backgound .tabset}


<br/>
<br/>


# Sequencing Data Analysis: Dilution Series {#seq-data-analysis-dil-series .tabset}

* Gather statistical summaries from bathes 12 and 13.

* RETURN TO THIS LATER - for now just plot cf vs dilution level


## Home {-}

<br/>
<br/>

## CF vs Dilution Levels {- .tabset}

```{r m5a-cf-vs-dilutions, cache=T, cache.vars='', fig.height=6, fig.width=8, fig.cap="Contamination Fraction Features", results = "asis"}

 # Plate_24
Plate_24Legend_vec <- sort(unique(conta_seq_frm$Plate_24))
Plate_24Col_vec <- col_vector[1:length(Plate_24Legend_vec)]
names(Plate_24Col_vec) <- Plate_24Legend_vec
  
Plate_24Pch_vec <- Pch_v[1:length(Plate_24Legend_vec)]
names(Plate_24Pch_vec) <- Plate_24Legend_vec

 # sample
sampleLegend_vec <- sort(unique(conta_seq_frm$sample))
sampleCol_vec <- col_vector[1:length(sampleLegend_vec)]
names(sampleCol_vec) <- sampleLegend_vec
  
samplePch_vec <- Pch_v[1:length(sampleLegend_vec)]
names(samplePch_vec) <- sampleLegend_vec
 
  data_frm = conta_seq_frm %>%
    dplyr::filter(!is.na(abn_cov), !is.na(bin_cov), cf < 0.2) 
  data_frm$cf <- pmax(1e-6, data_frm$cf)

for(BATCH in c("B012", "B013")){
  cat("\n\n### ", BATCH, "  {- .tabset}\n")
  batch_data_frm <- data_frm %>% 
     dplyr::filter(batch==BATCH) %>%
     dplyr::mutate( dil_level = 2^as.numeric(sub("L",'',sample)))
    
  cat("\n\n#### Plot {-}\n")
  with(batch_data_frm, 
    pairs(cbind(cf, n_contam_frag, n_hom_frag_adj, dil_level),
    log='xy', cex.labels = 1.5, 
    lower.panel=NULL,
    panel=function(x,y){
      points(x, y,
      col=Plate_24Col_vec[Plate_24],
      pch=samplePch_vec[sample]
  )}
 ))

old_par <- par(xpd=NA)
  legend(1, 5, title="Plate_24", legend=names(Plate_24Col_vec), 
  col = Plate_24Col_vec, pch=19, bty='n', ncol=2)
  Pch_vec <- with(batch_data_frm, sort(unique(sample)))
  legend(1, 3, title="sample", legend=Pch_vec, ncol=2,
         col = 1, pch = samplePch_vec[Pch_vec], bty='n')
par(old_par)

mtext( side = 1, outer = T, line = -3, 
paste0(BATCH)) 
  
cat("<br/>\n")
cat("<br/>\n")

    cat("\n\n#### Table {-}\n")

print(
with(batch_data_frm,
data.frame(batch, Well_96, sampleName, sample, Sample,
cf, n_contam_frag, n_hom_frag_adj, n_hom_frag)
) %>%
knitr::kable(
caption = paste("Data for Batch", BATCH)
) %>%
kableExtra::kable_styling(full_width = F)
)

cat("<br/>\n")
cat("<br/>\n")

}

```

<br/>
<br/>


# Load ddPCR Data

* Using the ddPCR data as input instead of the merged file used up to here.

```{r m5a-ddpcr-data, cache=T, cache.vars=c("common_columns_vec", "ddPCR_data_lst"), results = "asis", fig.cap = "ddPCR Data"}


ddpcr_sheets_vec <- readxl::excel_sheets(with(params,
  file.path(conta_ddPCR_root, analysis_dir, ddpcr_data_file))
) 

ddpcr_data_lst <- lapply(ddpcr_sheets_vec, function(SHEET)
with(params, 
 readxl::read_excel(
 path = file.path(conta_ddPCR_root, analysis_dir, ddpcr_data_file),
 sheet = SHEET)
))
names(ddpcr_data_lst) <- ddpcr_sheets_vec

sapply(ddpcr_data_lst, dim) %>%
knitr::kable(
caption = "ddPCR table dimensions"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>

# ddPCR Data Analysis: Background Batches {#ddpcr-data-analysis-bckgrnd-batches.tabset}

<br/>
<br/>


# ddPCR Data Analysis: Dilution Series  {#ddpcr-data-analysis-dil-series .tabset}

* In this section we read in abd analyze the raw ddPCR data.
   - In the analysios above we used a previously merged file.

## Home {-}

<br/>
<br/>

## Load ddpcr_B012-13 

```{r m5a-ddpcr-dilution-series, cache=T, cache.vars='ddpcr_B012_B013_frm', fig.cap = "Dilution Series ddPCR Data"}

ddpcr_B012_B013_frm <- ddpcr_data_lst[["ddpcr_B012-13"]]

ddpcr_B012_B013_frm %<>%
dplyr::rename(
  batch = Data_batch,
  sample = Sample,
  DyeName = `DyeName(s)`)


ddpcr_B012_B013_frm %<>%
dplyr::mutate(
  mixtureLevel = ifelse(grepl("^L", sample),
         paste0("L", formatC(as.numeric(sub("L", '', sample)), width=2, flag='0')),
         NA)
  )

ddpcr_B012_B013_frm %<>%
dplyr::mutate(
sample = ifelse(grepl('^L', sample), mixtureLevel, sample)
  )

with(ddpcr_B012_B013_frm, 
table(paste0(batch,':', Run, ':Target_', Target, ':', DyeName), sample)
) %>%
knitr::kable(
caption = "ddpcr_B012_B013_frm composition"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>


### Datatable {-}


```{r m5a-ddpcr-dilution-datatble, cache=T, cache.vars='', fig.cap = "Dilution Series ddPCR Data"}

ddpcr_B012_B013_frm %>%
  DT::datatable(extensions = 'Buttons',
  caption = "ddpcr_B012_B013_frm",
            options = list(dom = 'Blfrtip',
                           buttons = c('copy', 'csv', 'excel')))


```

<br/>
<br/>

## `ddPCR` package

<br/>
<br/>


## `twoddPCR` package

<br/>
<br/>

# Appendix 

<p><p/>
* In this case truth could come in the form of samples with known contamination levels,
and the ideal dataset would have samples having known levels of contamination,
with levels in the range of interest, and with replication.
   - the samples from the titration set (batches 12 and 13) at the
levels of contamination near the critical level of say 90% detection rate^[
90%]
form a good set for assessment purposes.
   -  lower levels would not be detectable and higher level would easily be detectable
so neither of these are useful for metric selection (ie. which metric is best at detection).

## Study Design {#data-collect-design .tabset -}

* For documentation only.
   - this section contains various printout which were use
to understand the data structure in the data files.
   - this is kept only for debugging purposes

<!--
* Are the ddPCR and Seq data run side by side in the
same material? (up to the point of divergence ddPCR/Seq)
-->

<br/>

### Home {-}

<br/>
<br/>


### Design Info {.tabset}


* Partial - need information in machine readable format.


#### Home {-}


```{r m5a-design-info, results = "asis", fig.cap = "Design Structure of Data for Conta ddPCR/Seq"}

analysis_files <- with(params,
list.files(file.path(conta_ddPCR_root, analysis_dir))
 )


sheets_vec <- readxl::excel_sheets(with(params,
  file.path(conta_ddPCR_root, analysis_dir, cp_design_file))
) 

if(F)
sheets_vec %>% 
knitr::kable(
caption = "Sheets of the design workbook"
) %>%
kableExtra::kable_styling(full_width = F)


design_file_wb <- with(params, openxlsx::loadWorkbook(
   file.path(conta_ddPCR_root, analysis_dir, cp_design_file))
   )

```

<br/>
<br/>


#### Image 1 {-}

`r CAPTION <-
paste(   
"Description of design_file_wb@.xData$media[1]"
)`       
       
```{r m5a-design-img1, fig.cap = "Design Image - Describe Img1"} 
knitr::include_graphics(design_file_wb@.xData$media[1], dpi=100)
 
``` 

<br/>
<br/>

#### Image 2 {-}

`r CAPTION <-
paste(   
"Description of design_file_wb@.xData$media[2]"
    
)`       
       
```{r m5a-design-img2, fig.cap = "Design Image - Describe Img2"} 
knitr::include_graphics(design_file_wb@.xData$media[2], dpi=100)
 
``` 

<br/> 
<br/> 

#### Metadata 

* We have metadata in 
`r with(params, file.path(conta_ddPCR_root, analysis_dir, seq_ddpcr_metadata_file))`

```{r m5a-metadata, cache=T, cache.vars ="seq_ddpcr_metadata_frm"}

seq_ddpcr_metadata_frm <- with(params, readxl::read_excel(
path = file.path(conta_ddPCR_root, analysis_dir, seq_ddpcr_metadata_file),
sheet = seq_ddpcr_metadata_sheet)
)

seq_ddpcr_metadata_frm %<>%
dplyr::rename(batch = Data_batch)

if(
with(seq_ddpcr_metadata_frm,
nrow(seq_ddpcr_metadata_frm) -
nrow(unique(cbind(batch, Well_96)))
) ) cat("\n\n blah-blah-blah \n")

 
 # Shorten Sample_name
seq_ddpcr_metadata_frm %<>%
dplyr::mutate(
sampleName = sub("Female Pool", 'FP',
             sub("Streck-", '',
             sub("Male Pool", 'MP', Sample_name)))
         ) %>%
dplyr::rename(sample = Sample)

seq_ddpcr_metadata_frm %<>%
dplyr::mutate(
  mixtureLevel = ifelse(grepl("^L", sample),
         paste0("L", formatC(as.numeric(sub("L", '', sample)), width=2, flag='0')),
         NA)
  )

if(F)
with(seq_ddpcr_metadata_frm,
table(paste0(mixtureLevel, ':', sample))) %>% t() %>% t()

seq_ddpcr_metadata_frm %<>%
dplyr::mutate(
sample = ifelse(grepl('^L', sample), mixtureLevel, sample)
  )

if(F)
with(seq_ddpcr_metadata_frm, 
table(batch, paste0(sample))
) %>% t() %>%
knitr::kable(
caption = "Sample Descripion"
) %>%
kableExtra::kable_styling(full_width = F)


```

<br/> 
<br/> 


## ddPCR & Seq Dilution Series Data {- .tabset}

### Home {-}

<br/>
<br/>

### Pairs ddPCR {-}

<!--
* Apply quality filters to get better resolution and color by pctDummy to see tot vs pos 
better.
-->

<p><p/>
* At the low end of the mixture/contamination range: 
   - dd_tot and dd_pos are uncorrelated
      - variability in the number of positive droplets does not depend on the
total number of droplets
   - seq_tot and seq_pos are weakly correlated

* To plot on the log scale, which is necessary if we want to see anything at all
in the region of interest, cf_ddpcr, pctConta_byRatio and pctConta_byYRS have to be 
shifted to remove negative values.

* In doing so, the lower tail of the distribution of values is truncated giving the
impression that the associated metrics have curtailed dynamic range.
   - this is not the case and is immaterial as this only happens at the lowest
titration level, which is well below the level of blank - the upper limit
that a contamination index can reach for non-cancer samples.

```{r m5a-ddpcr-seq-counts-pairs-ddPCR-only, cache=T, cache.vars='', fig.height= 8, fig.width = 11, fig.cap ="ddPCR: Examine all indicators"}

with(ddpcr_seq_dilution_frm %>% 
     dplyr::filter(bin_cov > 50),
pairs(cbind(
       dd_pos = dd_pos+.25, 
       dd_posRate = (dd_pos+.25)/dd_tot,
       cf_ddpcr = cf_ddpcr - 2*min(cf_ddpcr), 
       Conta_byRatio = pctConta_byRatio - 2 * min(pctConta_byRatio), 
       ###Conta_byYRS = pctConta_byRatio - 2 * min(pctConta_byYRS),
       pctDummy),
    log='xy', cex.labels = 1.5,
    lower.panel=NULL,
    panel=function(x,y){
      na.flg <- is.na(x) | is.na(y)
      points(x[!na.flg], y[!na.flg],
      pch=batchPch_vec[batch[!na.flg]],
      col=dil_levelFCol_vec[dil_levelF[!na.flg]])
    lines(lowess(x[!na.flg],y[!na.flg]), col='green', lwd=2)
  }
 ))


old_par =  par(xpd= NA)

legend(0.9, 5, legend = names(dil_levelFCol_vec), ncol = 2, 
text.col = dil_levelFCol_vec, title = "dil_levelF", title.col = 1, bty = 'n')


legend(1.0, 2.8, legend = names(batchPch_vec), ncol = 1,
pch = batchPch_vec, title = "batch", bty = 'n')

par(old_par)


```

<br/>
<br/>


### rePairs {-}

*  rePairs plots with cf_seq and cf_ddpcr
   - cf_seq = n_contam_frag/n_hom_frag_adj
   - cf_ddpcr = npositive/total

```{r m5a-ddpcr-seq-counts-re-pairs-qf, cache=T, cache.vars='', fig.height= 8, fig.width = 11, fig.cap ="ddPCR and Seq Counts: total, pos"}

with(ddpcr_seq_dilution_frm %>% 
     dplyr::filter(bin_cov > 50),
pairs(cbind(dd_tot, dd_pos = dd_pos+.25, seq_tot, seq_pos=seq_pos+0.25, pctDummy),
    log='xy', cex.labels = 1.5,
    lower.panel=NULL,
    panel=function(x,y){
      na.flg <- is.na(x) | is.na(y)
      points(x[!na.flg], y[!na.flg],
      pch=batchPch_vec[batch[!na.flg]],
      col=dil_levelFCol_vec[dil_levelF[!na.flg]])
    lines(lowess(x[!na.flg],y[!na.flg]), col='green', lwd=2)
  }
 ))


old_par =  par(xpd = NA)

legend(1, 4, legend = names(dil_levelFCol_vec), ncol = 3, 
text.col = dil_levelFCol_vec, title = "dil_levelF", title.col = 1, bty = 'n')


legend(1, 2.0, legend = names(batchPch_vec), ncol = 2,
pch = batchPch_vec, title = "batch", bty = 'n')

par(old_par)

dd_seq_tot_pos_cor_frm <- rbind(
seq = seq_tot_pos_cor_vec, dd = dd_tot_pos_cor_vec)


dd_seq_tot_pos_cor_frm %>%
knitr::kable(
caption = "pos vs tot corr by dil_levelF"
) %>%
kableExtra::kable_styling(full_width = F)

```

* At the low end of the mixture/contamination range,  dd_tot and dd_pos are
uncorrelated.
   - variability in the number of positive droplets does not depent on the
number of droplets *because they sre in great excess.

<br/>
<br/>



### Profiles {-}

*<span style="color:blue;"> pctMale is missing - will use dummy mixture
which assumes each dilution step is 1/2.</span> 
   - FIX THIS

```{r m5a-ddpcr-seq-counts-profiles-plot, cache=T, cache.vars='', fig.height= 8, fig.width = 11, fig.cap ="ddPCR and Seq Counts: total, pos"}


 ###############################
par(mfrow=c(2, 1), mar=c(2, 2, 2, 2), oma=c(2, 2, 2, 4))

 # ddPCR
with(ddpcr_seq_dilution_frm %>% dplyr::mutate(rep = 1),
{
plot(x = log(pctDummy), y = log(dd_tot),  log='', ylim=range(log(pmax(1, c(dd_tot, dd_pos, dd_neg))), na.rm=T),
     col = batchCol_vec[batch],
     pch = Plate_24Pch_vec[Plate_24],
    xaxt = 'n', yaxt = 'n', xlab='', ylab='')
if(F)
points(log(pctDummy), log(dd_neg),
     col = 2, ###batchCol_vec[batch],
     pch = batchPch_vec[batch],
      )
points(log(pctDummy), log(dd_pos),
     col = batchCol_vec[batch],
     pch = Plate_24Pch_vec[Plate_24],
      )
axis(side=2, at = pretty(log(c(dd_tot, dd_pos, dd_neg))), 
label = round(exp( pretty(log(c(dd_tot, dd_pos, dd_neg)))), 0))
axis(side=1, at = log(unique(pctDummy)), label = round(unique(pctDummy), 3))

}
)
title("ddPCR")

for(BATCH in unique(ddpcr_seq_byDil_frm$batch)) 
with(ddpcr_seq_byDil_frm %>% dplyr::filter(batch==BATCH), 
lines(log(pctDummy), log(dd_tot_med), col = batchCol_vec[BATCH], lwd=2))

for(BATCH in unique(ddpcr_seq_byDil_frm$batch)) 
with(ddpcr_seq_byDil_frm %>% dplyr::filter(batch==BATCH), 
lines(log(pctDummy), log(dd_pos_med), col = batchCol_vec[BATCH], lwd=2))

old_par =  par(xpd = NA)

legend('topright', inset=c(-.10, 0), legend = names(batchCol_vec), 
text.col = batchCol_vec, title = "batch", title.col = 1, bty = 'n')


legend('topright', inset=c(-.10, .3), legend = names(Plate_24Pch_vec), 
pch = Plate_24Pch_vec, title = "Plate_24", bty = 'n')

par(old_par)


 # seq
with(ddpcr_seq_dilution_frm %>% dplyr::mutate(rep = 1),
{
plot(x = log(pctDummy), y = log(seq_tot),  log='', ylim=range(log(pmax(1, c(seq_tot, seq_pos, seq_neg))), na.rm=T),
     col = batchCol_vec[batch],
     pch = Plate_24Pch_vec[Plate_24],
    xaxt = 'n', yaxt = 'n', xlab='', ylab='')
if(F)
points(log(pctDummy), log(seq_neg),
     col = 2, ###batchCol_vec[batch],
     pch = batchPch_vec[batch],
      )
points(log(pctDummy), log(seq_pos),
     col = batchCol_vec[batch],
     pch = Plate_24Pch_vec[Plate_24],
      )
axis(side=2, at = pretty(log(c(seq_tot, seq_pos, seq_neg))), 
label = round(exp(pretty(log(c(seq_tot, seq_pos, seq_neg)))), 0))
axis(side=1, at = log(unique(pctDummy)), label = round(unique(pctDummy), 3))

}
)
title("Seq")

for(BATCH in unique(ddpcr_seq_byDil_frm$batch)) 
with(ddpcr_seq_byDil_frm %>% dplyr::filter(batch==BATCH), 
lines(log(pctDummy), log(seq_tot_med), col = batchCol_vec[BATCH], lwd=2))

for(BATCH in unique(ddpcr_seq_byDil_frm$batch)) 
with(ddpcr_seq_byDil_frm %>% dplyr::filter(batch==BATCH), 
lines(log(pctDummy), log(seq_pos_med), col = batchCol_vec[BATCH], lwd=2))

mtext(side = 1, outer = T, "pctMix")
mtext(side = 2, outer = T, "counts")
mtext(side = 3, outer = T, "Sequencing and ddPCR Data - Showing Total and Positive (contam) Counts")


```

<br/>
<br/>

### Pre-pair Plots {- .tabset}

* Some relationships to examine before diving in.  

#### Home {-} 

<br/>
<br/>

#### `n_hom_frag_adj` vs `n_hom_frag` {-}

<p><p/>
* One question to resolve is whether scaling the number of contaminants detected, 
`n_contam_frag`, by the to total number of homozygous fragments shared with
adjacent samples, `n_hom_frag_adj`, a measure of risk or exposure.
   - here we look at `n_hom_frag_adj` vs `n_hom_frag`

```{r m5a-pre-pairs-hom-frag, cache=T, cache.vars='', fig.height= 5, fig.width = 7, fig.cap ="n_hom_frag_adj vs n_hom_frag"}

with(ddpcr_seq_dilution_frm %>% 
     dplyr::filter(bin_cov > 50),
pairs(cbind(n_contam_frag=n_contam_frag+0.25, n_hom_frag, n_hom_frag_adj),
    log='xy', cex.labels = 1.5,
    lower.panel=NULL,
    panel=function(x,y){
      na.flg <- is.na(x) | is.na(y)
      points(x[!na.flg], y[!na.flg],
      pch=batchPch_vec[batch[!na.flg]],
      col=dil_levelFCol_vec[dil_levelF[!na.flg]])
    lines(lowess(x[!na.flg],y[!na.flg]), col='green', lwd=2)
  }
 ))

old_par =  par(xpd = NA)

legend(1, 4, legend = names(dil_levelFCol_vec), ncol = 3, 
text.col = dil_levelFCol_vec, title = "dil_levelF", title.col = 1, bty = 'n')


legend(1, 2.0, legend = names(batchPch_vec), ncol = 2,
pch = batchPch_vec, title = "batch", bty = 'n')

par(old_par)

```


```{r m5a-pre-pairs-hom-frag-2, cache=T, cache.vars='', fig.height= 5, fig.width = 7, fig.cap ="n_hom_frag_adj vs n_hom_frag"}

with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50),
summary(n_hom_frag/n_hom_frag_adj))

with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50),
plot(n_hom_frag, n_hom_frag_adj))
abline(0, 1.60)
abline(lm(n_hom_frag_adj ~ n_hom_frag, data = ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50)))

with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50),
plot(exp((log(n_hom_frag_adj) + log(n_hom_frag))/2),
     exp(log(n_hom_frag_adj) - log(n_hom_frag)))
)

```


```{r m5a-pre-pairs-hom-frag-3, cache=T, cache.vars='', fig.height= 5, fig.width = 7, fig.cap ="n_hom_frag_adj vs n_hom_frag"}
par(mfrow=c(2, 1), mar=c(4.5, 4.0, 2, 1))
with(ddpcr_seq_dilution_frm %>%  
     dplyr::filter(bin_cov > 50),
plot(x= (n_hom_frag_adj + n_hom_frag)/2,
     y= n_hom_frag - n_hom_frag_adj , col="grey")
)
with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50) %>%
     dplyr::mutate(M = n_hom_frag - n_hom_frag_adj, A = (n_hom_frag_adj + n_hom_frag)/2),
abline(lm(M ~ A), col='blue', lwd=2)
)
with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50) %>%
     dplyr::mutate(M = n_hom_frag - n_hom_frag_adj, A = (n_hom_frag_adj + n_hom_frag)/2),
lines(lowess(A, M), col="green", lwd=2)
)
title("Additive Scale")

 # Log scale
 ################################
with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50),
plot(x= n_hom_frag,  ####(n_hom_frag * n_hom_frag)^(1/2),
     y= n_hom_frag / n_hom_frag_adj , col="grey")
)    
with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50) %>%
     dplyr::mutate(M = n_hom_frag / n_hom_frag_adj, A = (n_hom_frag * n_hom_frag)^(1/2)),
abline(lm(M ~ A), col='blue', lwd=2)
)
with(ddpcr_seq_dilution_frm %>%
     dplyr::filter(bin_cov > 50) %>%
     dplyr::mutate(M = n_hom_frag / n_hom_frag_adj, A = (n_hom_frag * n_hom_frag)^(1/2)),
lines(lowess(A, M), col="green", lwd=2)
)
title("Multiplicative Scale")

```

<br/>
<br/>

### Pairs {-}

```{r m5a-ddpcr-seq-counts-pairs, cache=T, cache.vars='ddpcr_seq_byDil_frm', fig.height= 8, fig.width = 11, fig.cap ="ddPCR and Seq Counts: total, pos"}


 # ddPCR
with(ddpcr_seq_dilution_frm %>% 
     dplyr::mutate(rep = 1),
pairs(cbind(pctDummy, dd_tot, dd_pos = dd_pos+.25, seq_tot, seq_pos),
    log='xy', cex.labels = 1.5,
    lower.panel=NULL,
    panel=function(x,y){
      points(x, y,
      col=batchCol_vec[batch],
      pch=Plate_24Pch_vec[Plate_24]
  )}
 ))



old_par =  par(xpd = NA)

legend(1, 5, legend = names(batchCol_vec),
text.col = batchCol_vec, title = "batch", title.col = 1, bty = 'n')


legend(1, 3.5, legend = names(Plate_24Pch_vec), ncol = 2,
pch = Plate_24Pch_vec, title = "Plate_24", bty = 'n')

par(old_par)


```

<br/>
<br/>

## Count - Profiles  Table    {-}

* pctMale is missing - will use dummy mixture assuming each dilution step is 1/2.

```{r m5a-ddpcr-seq-counts-profiles-table, cache=T, cache.vars='ddpcr_seq_byDil_frm', fig.height= 8, fig.width = 11, fig.cap ="ddPCR and Seq Counts: total, pos"}

ddpcr_seq_byDil_frm <- ddpcr_seq_dilution_frm %>%
dplyr::group_by(batch, dil_level) %>%
dplyr::summarize(
  #Vp = median(Vp),
  pctDummy = round(median(pctDummy), 4),

  dd_tot = median(dd_tot),
  dd_pos = median(dd_pos),
  dd_p = round(100*dd_pos/dd_tot, 3),

  seq_tot = median(seq_tot, na.rm=T),
  seq_pos = median(seq_pos, na.rm=T),
  seq_p = round(100*seq_pos/seq_tot, 3),
) %>%
tidyr::pivot_wider(
 id_cols = c(dil_level, pctDummy),
 names_from = batch,
 values_from = c(dd_tot, dd_pos, dd_p, seq_tot, seq_pos, seq_p),
 names_vary = "fastest"
 ) %>% as.data.frame()


ddpcr_seq_byDil_frm %>% t() %>%
knitr::kable(
caption = "ddPCR, Seq Dilution Data - Summarized over reps"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>



## LW Figures  {- .tabset}

* Reconstruct figures from 
[20241007 ddPCR for conta assessment [WIP]](https://docs.google.com/presentation/d/1I9xdVgxyAc5WDbhbqi_IO2ncIZnuuTU1HHftrEve9OY/edit#slide=id.g30ef6cc0c1a_0_11).


### Home {-}

<br/>
<br/>

### Batch Contents {-}

```{r m5a-lw-plots-4, fig.height=6, fig.width=8, fig.cap ='', eval=T}
#library(readr)
#library(tidyverse)
#library(ggpubr)
#library(ggplot2)
#library(hrbrthemes)

  ###20241028 B012-B013 titration

###ddpcr_seq <- read_csv("/Users/lwang/Documents/R/ContaQC/20241017_metadata_conta_ddpcr_seq - data w B002new.csv")

 # seq_ddpcr_metadata_frm 
ddpcr_seq_frm  <- with(params, readxl::read_excel(
path = file.path(conta_ddPCR_root, analysis_dir, seq_ddpcr_metadata_file),
sheet = seq_ddpcr_metadata_sheet)
)


with(ddpcr_seq_frm, 
table(Test, Data_batch), exclude=NULL
) %>% t() %>%
knitr::kable(
caption = "Content of ddpcr_seq_frm"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>

### Coverage Plots {-}

* Seq ~ binary
* ddPCR ~ Accpeted droplets

(ddK = 1000 dd)

```{r m5a-ddpcr-seq-cov-plots, fig.height=6, figf.width =8, fig.cap = "ddpcr_seq_frm: Coverage"}

BIN_THRESH <- 50
DD_THRESH <-  1e4

df01 <- dplyr::filter(ddpcr_seq_frm, 
on_binary_target_coverage_stats_mean >50 & Accepted_droplets > 10000)


par(mfrow=c(2, 1), mar=c(2, 3, 3, 2), oma = c(2, 2, 2, 0))
 
 # seq summaries
boxplot(with(ddpcr_seq_frm %>% dplyr::filter(on_binary_target_coverage_stats_mean > 5), 
       split(on_binary_target_coverage_stats_mean, Data_batch)),
  log='y')
abline(h=50, col='red')
title(paste("on_binary_target_coverage_stats_mean - Threshold :", BIN_THRESH))

seq_sum_by_batch <- with(ddpcr_seq_frm %>% dplyr::filter(on_binary_target_coverage_stats_mean > 5),
       sapply(split(on_binary_target_coverage_stats_mean, Data_batch),
   function(LL) 
   c(binQ = quantile(LL, prob=(1:3)/4), seq_mad = mad(LL), 
     pctBinLtThr = 100*mean(LL < BIN_THRESH)))) %>%
   t() %>% round(1) %>% as.data.frame() %>%
   dplyr::mutate(seq_pctcv = round(100*seq_mad/`binQ.50%`, 1))

seq_sum_by_batch %<>%
dplyr::mutate(
  batch = rownames(seq_sum_by_batch)
 )


 # dd summaries
boxplot(with(ddpcr_seq_frm, split(Accepted_droplets, Data_batch)),
  log='y')
abline(h=10000, col='red')
title(paste("Accepted_droplets - Threshold :", DD_THRESH))

dd_sum_by_batch <- with(ddpcr_seq_frm %>% dplyr::filter(Accepted_droplets > 5),
       sapply(split(Accepted_droplets, Data_batch),
   function(LL) 
   c(ddK = quantile(LL/1e3, prob=(1:3)/4), ddK_mad = mad(LL/1e3), pctDdLtThr = 100*mean(LL < DD_THRESH)))) %>%
t() %>% round(1) %>% as.data.frame() %>%
dplyr::mutate(ddK_pctcv =round(100*ddK_mad/`ddK.50%`, 1))

dd_sum_by_batch %<>%
dplyr::mutate(
  batch = rownames(dd_sum_by_batch)
)

```

<br/>
<br/>

### Coverage Tables {-}

```{r m5a-ddpcr-seq-cov-tables, fig.height=6, figf.width =8, fig.cap = "ddpcr_seq_frm: Coverage"}

ddpcr_seq_sum_by_batch_frm <- merge(
  seq_sum_by_batch, 
  dd_sum_by_batch, 
  by='batch', 
  all=T)

ddpcr_seq_sum_by_batch_frm %>%
knitr::kable(
  caption = "ddPCR and Sequencing Sample Size Summaries by Batch"
) %>%
kableExtra::kable_styling(full_width = F)


```

* Using binary coverage and accepted droplets aqs proxy for "size",
we see that seq platform is 4-5 times more variable than ddPCR.


<br/>
<br/>

### LW Plots {- .tabset}

* Reproducing figures from 
[20241007 ddPCR for conta assessment [WIP]](https://docs.google.com/presentation/d/1I9xdVgxyAc5WDbhbqi_IO2ncIZnuuTU1HHftrEve9OY/edit#slide=id.g31066045df9_0_0)

<p><p/>
* We have a titration run on each platform with duplicate runs.
   - This provides much information to evaluate each platform in terms of 
its ability to track changes in proportions of the two sources of cfDNA in the
mixture.
      - This should be the primary analysis.
   - As a secondary analysis we can compare the two, keeping in mind what
was uncovered in the primary analysis.
      - for example  if there is bias in one platform where the other stays
in target, we expect the two to diverge at the same point when compared
directly.

<p><p/>
* An alternative to linearity of relationship between cf and mixture or dd equivalent,
we can look at the separation between titration levels.
   - this may be a more easily interpretable assessment.
   - we can characterize the distance between groups, or titration levels,
by an estimated fold change and standard error or standard deviation.
      - SE is more useful for inference, but it is dependent on the number of 
data points.
      - SD is more descriptive, but doesn't depend on n.  It can be used to convert 
distances into standard units.



#### Home {-}

<br/>
<br/>

#### CF_seq vs ratio_dd {-}

<p><p/>
* Should Replot - coloring by ditration level
   - cf cutoff is in the middle of a dilution level

<p><p/>
* No matter how we color it, this plot is not as interesting as the
next one clearly showing the paltform's abiluty to disciminate between input levels.

```{r m5a-LW-concordance-plots, cache=T, cache.vars='titration_df', fig.height=5, fig.width=7, fig.cap = "Concordance figure from 20241007 - cf_seq vs ddpcr_ratio"}

 # this is unnecessary - missing values will drop out naturally.
 # df02 <- filter(df01, !is.na(cf_seq))
 # df03 <- filter(df02, !is.na(Ratio))

titration_df <- subset(ddpcr_seq_frm, Test =="titration")

require(ggplot2)

ggplot2::ggplot(titration_df, 
  aes(x=Ratio, y=cf_seq, col=ExtConsol), size = 5, alpha = 0.5) +
  geom_point(alpha = 0.5, size = 3) +
  geom_smooth(method=lm, level=0.95, size = 1.2, aes()) + 
  scale_x_continuous(trans='log10')+
  scale_y_continuous(trans='log10')+
  geom_abline(linetype = "dashed", size = 0.3) +
  geom_hline(yintercept = 1.0e-03, col = "blue", size = 0.3)+
  ### stat_regline_equation(label.x= -2.25, label.y= -0.015, col = "blue", size = 3.5) +
  ### stat_cor(aes(label=..rr.label..),label.x=-2.25, label.y= -0.25, col = "blue", size = 3.5) +
  labs(title = "Concordance: YRS/HBB ratio vs. cf_seq", subtitle =
         "- NA24631 titrated in NA12878; cf_seq cutoff 1.0e-03 in blue line") +
  theme_bw()+ theme(legend.position = "bottom")
  ##theme_ipsum() 

```

#### CF_seq boxplots by titration {-}

```{r m5a-LW-boxplot-by-titration, cache=T, cache.vars='', fig.height=5, fig.width=7, fig.cap = "Concordance figure from 20241007 - cf_seq vs ddpcr_ratio"}

ggplot(data = titration_df, 
  aes(x=Sample_name, y = cf_seq)) + geom_boxplot(aes(col=pctMale))+
  geom_jitter(aes(col=pctMale),width=0.15, size = 2, alpha = 0.3) +
  coord_cartesian(ylim = c(1.0e-06, 1)) +
  scale_y_continuous(trans='log10')+
  geom_hline(yintercept = 1.0e-03, col = "black", size = 0.3, linetype = "dashed")+
  labs(title = "NA24631 titrated in NA12878 - cf_seq", subtitle =
         "(cf cutoff 1.0e-03 in dashed line)") +
  theme_bw() + theme(axis.text.x=element_text(angle=45,hjust=1,size=10))
  ##theme_ipsum()

```



<br/>
<br/>

####  pos vs tot {-}

* 

<br/>
<br/>

###  outcome vs MixPct  {-}

with regression fit and Dx


<br/>
<br/>

###  ????  {-}

<br/>
<br/>

## Mathematics Relating to Binomial Variates  {- .tabest}


### Hone {-}

<br/>
<br/>

### Limit of non-detection {-}

If X = 0 we can claim p $le$ 3/n.  This is really a limit of non-detection -
if we don't detect any error in n cases, can we claim that the limit
of detection is 3/n?

From HS calculus, or the Stack Exchange:

\begin{equation}

\lim\limits_{n \to \infty} \left( 1 + \frac{x}{n} \right)^n = e^x
 
(\#eq:exp-as-limit)
\end{equation}


   
1. Assume that we observe N iid  Bernoulli trials each with probability
of success `p`.  In our example N is the number of fragments belonging to
the conta estimation set, and `success` is observing a non-hom  fragment.
Then 


\begin{equation}
P(X=0|N, p) = (1 - p)^N
(\#eq:prob-zero-success)
\end{equation}

 
The probability, $P(X=0|N, p)$ can be thought of as a likelihood of `p`
given that X = 0 is observed.  To obtain an $\alpha$ confidence interval
for `p`, the following must hold:

\begin{equation}

(1 - p)^N = \alpha

(\#eq:low-p-CI)
\end{equation}

   
<br/>
<br/>


### Rule of 3 {-}

When the binomial distribution is applicable, the Rule of
3 comes about this way.


Assume that we observe N iid  Bernoulli trials each with probability
of success `p`.  In our example N is the number of fragments belonging to
the conta estimation set, and `success` is observing a non-hom  fragment.
Then 


\begin{equation}
P(X=0|N, p) = (1 - p)^N
(\#eq:prob-zero-success)
\end{equation}

 
The probability, $P(X=0|N, p)$ can be thought of as a likelihood of `p`
given that X = 0 is observed.  To obtain an $\alpha$ confidence interval
for `p`, the following must hold:

\begin{equation}

(1 - p)^N = \alpha

(\#eq:low-p-CI)
\end{equation}

<br/>
<br/>



We are focusing on X = 0 because the set {X = 0} is the identical
same event as {The sample is not contaminated}.

A one sided confidence interval of level $\alpha$ is given by
$${p: (1 - p)^n > \alpha}$$
and solving for $p$ gives
$$p \le  1 - \alpha^{1/n}$$

By Taylor expansion,
$$\alpha^{1/n} = 1 + ln(\alpha)/n + [ln(\alpha)]^2/2n^2 + ...$$

Retaining just the linear term,
$$p \le - ln(\alpha)/n$$


In other words when X = 0, $-ln(\alpha)/n$ is an upper bound for 
p, which we may think of as a (**lower bound for**) Limit of Detection - we can never
claim a lower p (but detection still needs to be demonstrated).

```{r m5a-rule-of-ln-alpha-over-n,cache=T, cache.vars='',fig.cap="The Rules of -ln(a) over n"}

data.frame(
  alpha = c(.05, .01, .001), 
  `-ln(alpha)` = round(-log(c(.05, .01, .001)),2),
  check.names=F) %>%  t() %>%
knitr::kable(
caption = "X=0 UCB for p is -ln(alpha)/n"
) %>%
kableExtra::kable_styling(full_width = F)

```

<br/>
<br/>

## ddPCR -  Pinheiro  et al. (2012)  {- .tabset}

Pinheiro et al. (2012) [@Pinheiro:2012aa] evaluate the ddPCR platform.
The methodology is similar to Basu et al. (2017) [@Basu:2017aa].
The notation in this article is the following.

### Home {-}

<br/>
<br/>


### Notation {-}

* Fraction of positive partitions $\Leftrightarrow$ sample concentration:

\begin{equation}

M = - ln \left( 1 - \frac{P}{R}\right)

(\#eq:prop-det-conc)
\end{equation}

where M is the average number of target molecules per partition,
P is the number of partitions containing amplified product (not empty), 
and R is the number of partitions or reactions analyzed.


* Factors influencing variability:
   - R - the number of reeactions
   - the number of template molecules in the assay
   - partition volume (Vd)



`r CAPTION <-
paste(   
"Schematic showing the ddPCR workflow (Anal. Chem. 2012, 84, 2, 1003-1011)"
)`
 
```{r m5a-Pinheiro-2012-fig1, fig.cap = CAPTION,  out.width = "800px", fig.align = 'center'}

knitr::include_graphics("img/Pinheiro_2012_Fig1.jpeg")
 
``` 


<br/>
<br/>

### Experimental Design  {-}


`r CAPTION <-
paste(
"Schematic diagram of experimental design for assessing linearity and precision.(Anal. Chem. 2012, 84, 2, 1003-1011)"
)`

```{r m5a-Pinheiro-2012-fig2, fig.cap = CAPTION,  out.width = "800px", fig.align = 'center'}

knitr::include_graphics("img/Pinheiro_2012_Fig2.jpeg")

```

* DNA were prepared using a gravimetric protocol to 
minimize the uncertainty due to pipetting.  

* The ddPCR reagents, except DNA, were premixed and the final reaction mix was prepared gravimetrically by combining the DNA and PCR components (Supporting Information S-1). 

<br/>
<br/>

### Linearity and Precision {-}

* Linearity and Precision over the Theoretical Dynamic Range of ddPCR Instrument. 

<p><p/>
* Three independent **gravimetric serial dilutions** of Lambda DNA were prepared 
to produce three sets of seven solutions containing an average of
(based on absorbance measurements)
   - 26, 105, 409, 1 630, 6 495, 25 700, and 103 000 predicted copies of Lambda DNA 
per 20 μL ddPCR 

* For two of the serial dilution sets (gravimetric dilution series 2 and 3), 
eight replicate ddPCRs were prepared from a single eight-channel droplet generator
cartridge for each of the seven solutions. 

* For gravimetric dilution series 1, 24 replicate ddPCRs were prepared from 
three eight-channel droplet generator cartridges for each of the seven solutions. 

* The ddPCRs were analyzed using assay 2 under simplex conditions.
A complete eight channel cartridge of NTC was prepared by adding 1× TE0.1
buffer in place of DNA template. 

* NTCs showed alow-level background signal of approximately three positive droplets
per NTC assay, which could possibly be attributed to low-level
template contamination during the preparation of the reaction mixture.


<br/>
<br/>

#### Comparison of DNA Concentration and Ratio Measurements  {-}

* Using Two Different dPCR Formats under Simplex and Duplex Conditions. 

<br/>
<br/>


# References {#references}
    
<p><p/>
* [google statistics of balls in boxes and bins](https://www.google.com/search?q=statisrtics+of+balls+in+boxes+and+bns&sca_esv=c744cb070de47b7e&rlz=1C5GCEM_en&ei=BGFEZ_PxMsif5NoPkqmNcA&ved=0ahUKEwjzg5CLsveJAxXID1kFHZJUAw4Q4dUDCA8&uact=5&oq=statisrtics+of+balls+in+boxes+and+bns&gs_lp=Egxnd3Mtd2l6LXNlcnAiJXN0YXRpc3J0aWNzIG9mIGJhbGxzIGluIGJveGVzIGFuZCBibnMyCBAAGIAEGKIEMggQABiABBiiBEiEIlCwDViZFHABeACQAQCYAf4BoAHDBqoBBTAuNC4xuAEDyAEA-AEBmAIEoAK0A8ICChAAGLADGNYEGEfCAgoQIRigARjDBBgKmAMA4gMFEgExIECIBgGQBgiSBwMxLjOgB7QW&sclient=gws-wiz-serp)

* [wiki Balls into bins problem](https://en.wikipedia.org/wiki/Balls_into_bins_problem).

<div id="refs"></div>

<br/>
<br/>


```{r , eval=T}
pander::pander(sessionInfo())
```

  * WRKDIR = `r normalizePath(WRKDIR)`
  * FN = `r FN`
  * Scripts = Scripts
  * RUN DATE = `r date()`

```{r , echo=FALSE}
  knit_exit()
```

############################################################
## ARCHIVED CODE BELOW {-}
############################################################
## A Simple Model for Dilution Series Data {#a-simple-model}


<p><p/>
*  In a dilution/titration series the amount of input material is
progressively reduced and the impact or effect this has on some output
is measured.  In the test plate titration, the amount of cancer cfDNA that
is spiked into non-cancer background is progressively reduced.  
   - The sample metric that comes the closest to directly measuring cancer cfDNA is
`TMeF_CR`.
   - Another metric that comes close to  directly measuring cancer cfDNA
of `abnormal coverage`.
   - A less obvious sample metric that could be measuring
cancer cfDNA is  `binary coverage`.  

* Let `M` stand for a sequencing metric thought to be tracking the amount
of cancer cfDNA in solution and
let `I` stand for the amount of cancer cfDNA input in a non-cancer background.
The simplest plausible model relating a sequencing metric to an input
amount of cancer cfDNA is the following:


\begin{equation}
Y_R = log(M) = \alpha + \beta \cdot  log(I) + \epsilon    \\
\text{where } \epsilon \sim (0, \sigma^2)
(\#eq:regression)
\end{equation}

* Model \@ref(eq:regression) says that the magnitude of the
sequencing metric rises smoothly and  proportionally
to the amount of input cancer cfDNA.

* We use $Y_R$ here to distinguish the regresion model
from the ANOVA model, where we will use $Y_A$ to denote
the response.


* Note that for the proportionality coefficient in \@ref(eq:regression)
to be meaningful, $\sigma$ must be reasonably small 
(if $\left||\beta \right| \ll \sigma$ the regression equation 
buys very little. 


<p><p/>
* The fit of the model to the data, or the lack of fit, provides a 
useful way to describe a dilution series:
   - under optimal assay conditions we should expect a steady rise in
the sequencing metric to match the rise in input, with little 
variation around the incline.


<p><p/>
* One of the ways that the data may not fit the model described in
Equation \@ref(eq:regression) is that the general trend could be in
agreement with the model, but the variability  may be excessive - ie
the data are noisy.  
   - this deviation is captured by the model fit residual error
or root men squared error, the usual estimator of $\sigma$.

<p><p/>
* Another way the data may not fit is through systematic deviation -
there is a relationship between the sequencing metric and the amount 
cancer cfDNA input, but it is not a simple proportionality
constant that explains the relationship.
An analysis of variance model would fit such
a pattern.  An **analysis of variance** model  explains the
variability in log(Y) in terms of the input level groups - L1, ..., L6.

\begin{equation}
log(M) =  L_g + \epsilon      \\
\text{where } \epsilon \sim (0, \sigma^2)
(\#eq:anova)
\end{equation}

where the parameters $L_g$ stand for the expecred sequence metric
level for the samples with the g'th inpout group.  

<p><p/>
* As with Model \@ref(eq:regression), $\sigma$ must be smaller than the differences
between groups for the model to be meaningful.

<p><p/>
* We can use the difference between the anova fit and the linear fit 
to obtain a measure of lack of fit for the linear model,
or **modeling error**.

<br/>

<p><p/>
For each metric this analysis produces the following diagnostics for each Data_batch:

<p><p/>
* **Modeling Error (ME)**

\begin{equation}
ME = \sqrt{\sum_{d=1}^D(\hat{Y_{R_d}} - \hat{Y_{A_d}})^2/D}
(\#eq:ME)
\end{equation}

where the mean squared difference is taken over the design points.  The green
and blue points in the plots below.

<p><p/>
* **Residual Error (RE)** (rmse)
   - this is analogous to what is sometimes referred to as the `noise`
of the system
   - it is essentially unexplained variability which is taken to be
random (until we identify some specific sources of systematic variability contributing
to what today we call noise)

\begin{equation}
RE = \sqrt{\sum_{t=1}^T(Y_{A_t} - \hat{Y_{A_t}})^2/(T-G)}
(\#eq:RE)
\end{equation}

where t is the total number of samples across the G groups.


<p><p/>
* **Signal to Noise Ratio (SNR)**

   -  The slope in Model \@ref(eq:regression) is an indicator of how much the
metric values change for each unit change of input.  This is analogous to a `signal`.

   - to get a signal to noise ratio indicator, we can take the quotient of
the estimated slope to the residual variance:

\begin{equation}
SNR = \frac{\hat{\beta}}{{\hat{\sigma}}}
(\#eq:SNR)
\end{equation}


where $\hat{\beta}$ is the estimate of parameter $\beta} in
\@ref{eq:regression}, and $\hat{\sigma}$ is the 
estimate of the standard deviation of the distribution of $\epsilon$ -
typically the root mean squared error of the model fit to the data.

<br/>
<br/>

* **Important Note** In these evaluations  it is important
to use robust procedures.
   - TO DO: get robust versions of total_ss, reg_ss and res_ss



<br/>
<br/>

## Plate fits {.tabset #example-fits}

* All test plates from the burn-in dataset are analyzed to examine
the relationship between 4 metrics and the designed dilution levele:
binary coverage, abnormal coverage, total coverage, abnormal content, TMeP_CR.



<p><p/>
* In the figures under the various results tabs below:
   - the blue points and lines are from the **fitted linear regression models**.
      - <span style="color:blue;">blue points </span> are 
$\hat{Y}_{R_d}$ in \@ref(eq:ME) and \@ref(eq:regression).
   - the green points are from the fitted **fitted ANOVA model**
      - <span style="color:green;">green points </span> are 
$\hat{Y}_{A_d}$ in \@ref(eq:ME) and \@ref(eq:anova).

   - **Modeling error**, \@ref(eq:ME),  is the root mean squared difference between the blue
and the green points.
   - the **residual error** (also known as **pure error**), \@ref(eq:RE),
is the  root mean squared residuals from the anova fit,  


<p><p/>
* In the plots, the vertical red lines at input = 5.5 are 95% prediction intervals.
The blue wings are 95% confidence intervals.
   - CI $\approx \pm 2 \cdot \frac{\hat{\sigma}}{\sqrt{n}}$^[this is the
formula for a simple mean.  In a regression contex there is another scaling factor
which depends in the location of the predicted point from the center of
the fitting set]
   - PI $\approx \pm 2 \cdot (\sigma + \frac{\hat{\sigma}}{\sqrt{n}})$ 

<p><p/>
* **Note on inferential context**. What is the inferential context here?    Statistical
inference let's us make inference beyond a sample set to an imagined context
in which either the sample collection is vastly expanded to include more
sample points, or the measurement exercise is replicated a large number of times.
   -  we should keep in mind that we cannot make any inference beyond the
sampled data sampling frame.  If we have only one plate, we cannot make inferences 
beyond that one plate.
   - statistical inference is useful in informing us about the expected level of
reproducibility of the observed results; from that it is up to us to decide
what actions to take.

<p><p/>
* Note that some regression diagnostics  are not directly comparable when the response
is expressed in different units, but SNR values are comparable.

<p><p/>
* Binary vs Abnormal coverage:
   - a large fraction of variability in binary coverage is often
within group variability unrelated to input (or TMeF)
   - when this variability is shared with abnormal coverage (ie.
abnornal and binary coverage valuesa are correlated) abnormal coverage
values can be adjusted to remove this component of variabiliry and thus
reduce the abnormal coverage variability.
   - This can be done by regression analysis as described in Section
\@ref(model-for-coverage).
   - `abnormal content` is a special case of this sort of adjusted coverage.

<p><p/>
*  This variability is unwanted in the sense that abnormal
coverage variability would ideally be solely driven by biological
variability of interest.
   - for QC purposes, though, we do watn to track this,
and any other source of unwanted variability


### Home {-}

<br/>
<br/>


<!-- no need
### Plate Selection {-}

**Plates were not selected - all plates with data are shown**
-->

```{r m5a-pick-four-plates, cache=T, cache.vars = c("TPD_Plate_vec", "four_TPD_plates", "all_TPD_plates_frm"), include=F, eval=F}
 ### CLEARE CACH

TPD_Plate_vec <- with(v3a_structure_frm %>% dplyr::filter(TPD),
unique(Plate))

 # four plates
set.seed(101)
four_TPD_plates <- sample(TPD_Plate_vec, size = 4)


 # basic info for all test plates with flag for the four selected
 ##############################
all_TPD_plates_frm <- conta_seq_frm %>%
dplyr::filter(Plate %in% TPD_Plate_vec) %>%
dplyr::select(flowcell_no, flowcell, CCB_no, CCB, ECB_no, ECB,
   PlateNum, Plate)  %>%
unique() %>%
dplyr::mutate(
  plate_set = ifelse(Plate %in% four_TPD_plates, 1, 2)
) %>%
as.data.frame()  %>%
dplyr::arrange(plate_set, flowcell_no, CCB_no, ECB_no, PlateNum)


all_TPD_plates_frm %>%
knitr::kable(
caption = "Plate Information"
) %>%
kableExtra::kable_styling(full_width = F)



```

<br/>
<br/>


```{r m5a-examples-bin-abn-tmef, cache=T, cache.vars = "", fig.cap="Example plate fit and Dx", results = "asis", eval=F}

 # in here we need standardize and plot all on a common 
 # scale in standard units but on each metric's true scale
 # do the analysis first; plot second

for(JJ in 1:nrow(all_TPD_plates_frm)) {
  
 with(all_TPD_plates_frm[JJ,],
 cat("\n\n### ", ECB_no,'-', PlateNum,  "  {- .tabset}\n")
 )

  TP <- all_TPD_plates_frm[JJ, "Plate"]

  TP_frm <- conta_seq_frm %>%
  dplyr::filter(
     !is.na(abn_cov), 
     !is.na(bin_cov),
     TMeF_CR < 0.5) %>%
  dplyr::filter(Plate == TP,
                Level %in% paste0("L", 1:6)
   ) %>%
   dplyr::mutate(
     abn_cov = pmax(abn_pos_q05, abn_cov),
     bin_cov = pmax(bin_pos_q05, bin_cov),
     abn_content = abn_cov/bin_cov * 1e3
   )
  ####################################################
  # - Before anything, get Metrics mean, sd and values in 
  # SU (standsrd units)
  # all of the analyses are on the log scale 
  TP_frm#   - otherwise we run into problems with negative predictions
  # - get range from all metrics on SU log scale
  # - convert to each metric's log scale to specify Ylim.
  #####################################################
  METRICS <- c("bin_cov", "abn_cov", "abn_content", "TMeP_CR")
  
  metrics_logscale_sum_mtx <- sapply(METRICS, function(MTRC) 
   {# for MTRC in METRICS
    # get min and max of MRTC in SUs
     Mean = mean(log(TP_frm[,MTRC]), na.rm=T)
     Sd = sd(log(TP_frm[,MTRC]), na.rm=T)
     min_su = min((log(TP_frm[,MTRC]) - Mean)/Sd, na.rm=T)
     max_su = max((log(TP_frm[,MTRC]) - Mean)/Sd, na.rm=T)
     return(c(Mean=Mean, 
              Sd=Sd, 
              min_su=min_su, 
              max_su=max_su
          ) )
   })

   # min/max  of min/max across METRICS
   Min_su <- min(metrics_logscale_sum_mtx["min_su",])
   Max_su <- max(metrics_logscale_sum_mtx["max_su",])

   # WON'T USE THIS
   metrics_logscale_sum_frm <- data.frame(t(metrics_logscale_sum_mtx)) %>%
   dplyr::mutate(
    Metric = colnames(metrics_logscale_sum_mtx),
    Ymin = Mean + Sd * Min_su,
    Ymax = Mean + Sd * Max_su
   )

  #####################################################
   tmp <-unique( TP_frm %>%
   dplyr::select(Plate, flowcell_no, CCB_no, ECB_no, PlateNum))

   mtext3 <- with(tmp,
     sprintf("Plate = %s - %s,  %s,  %s, Num = %s",
     Plate, flowcell_no, CCB_no, ECB_no, PlateNum)
         )
   
  # will pass pch and col for well through TP_frm

  par(mfrow = c(2, 2), mar=c(2.5, 2.5, 2, 2.5), oma=c(2,2.5,2,2) )

   # Binary Cov
   ################################
   TP_bin_cov_res_frm <- plot_metric_vs_input_f(
      TP_frm, Metric="bin_cov", 
      Pch = ep_wellPch_vec[TP_frm$ep_well], 
      Col = ep_wellCol_vec[TP_frm$ep_well], 
      Ylim_log_su = c(Min_su, Max_su),
      axis_4 = T,
      add_median=T, Legend=F, add_snr=F, Title = '')
   title("bin_cov")
   mtext(side=3, outer = T, mtext3)

   with(TP_bin_cov_res_frm, cex = 0.8,
   legend("topleft", bty='n', cex=0.8, # title = "Binary Coverage",
   legend = c(
              paste("Mod. Err =", round(ME, 3)),
              paste("Res. Err =", round(RE, 3)),
              paste("beta =", round(slope, 3)),
              paste("SNR = ", round(SNR, 3))
           ),
   ncol = 1)
   )
  
   # Abnormal Cov
   ################################
   TP_abn_cov_res_frm <- plot_metric_vs_input_f(
      TP_frm, Metric="abn_cov", 
      Pch = ep_wellPch_vec[TP_frm$ep_well], 
      Col = ep_wellCol_vec[TP_frm$ep_well], 
      Ylim_log_su = c(Min_su, Max_su),
      axis_4 = T,
      add_median=T, Legend=F, add_snr=F, Title = '')
   title("abn_cov") 

   with(TP_abn_cov_res_frm,
   legend("topleft", bty='n', cex=0.8, # title = "Abnormal Coverage",
   legend = c(
              paste("Mod. Err =", round(ME, 3)),
              paste("Res. Err =", round(RE, 3)),
              paste("beta =", round(slope, 3)),
              paste("SNR = ", round(SNR, 3))
           ),
   ncol = 1)
   )
  
   # Abnormal Content
   ################################
   TP_abn_content_res_frm <- plot_metric_vs_input_f(
      TP_frm, Metric="abn_content", 
      Pch = ep_wellPch_vec[TP_frm$ep_well], 
      Col = ep_wellCol_vec[TP_frm$ep_well], 
      Ylim_log_su = c(Min_su, Max_su),
      axis_4 = T,
      add_median=T, Legend=F, add_snr=F, Title = '')
   title("abn_content")
 
   with(TP_abn_content_res_frm,
   legend("topleft", bty='n',  cex=0.8, # title = "Abnormal Content",
   legend = c(
              paste("Mod. Err =", round(ME, 3)),
              paste("Res. Err =", round(RE, 3)),
              paste("beta =", round(slope, 3)),
              paste("SNR = ", round(SNR, 3))
           ),
   ncol = 1)
   )

   # TMeP_CR
   ################################
   TP_frm %<>%
   dplyr::mutate(TMeK = 10 * TMeP_CR)

   TP_tmep_res_frm <- plot_metric_vs_input_f(
      TP_frm, Metric="TMeK", 
      Pch = ep_wellPch_vec[TP_frm$ep_well], 
      Col = ep_wellCol_vec[TP_frm$ep_well], 
      Ylim_log_su = c(Min_su, Max_su),
      axis_4 = T,
      add_median=T, Legend=F, add_snr=F, Title = '')
   title("TMeF_CR x 1000")

   with(TP_tmep_res_frm, 
   legend("topleft", bty='n',  cex=0.8, # title = "TMeP_CR",
   legend = c(
              paste("Mod. Err =", round(ME, 3)),
              paste("Res. Err =", round(RE, 3)),
              paste("beta =", round(slope, 3)),
              paste("SNR = ", round(SNR, 3))
           ),
   ncol = 1)
   )

   mtext(side=1, outer = T, "Input Amount")
   mtext(side=2, outer = T, line= 1, "Coverage in the usual units")
   mtext(side=4, outer = T, "Coverage in standard units")
   
   cat("<br/>\n")  
   cat("<br/>\n")  


}#for(JJ

```

<br/>
<br/>


# Examine ddPCR Data {#exam-ddpcr-data .tabset}

* Going to the ddPCR data instead of the merged file used up to here.

<br/>
<br/>

## Read ddPCR Data {-}

<br/>
<br/>

* pos vs tot

<br/>
<br/>

## LW Plots {-}

```{r ARC-m5a-lw-plots, fig.height=6, fig.width=8, fig.cap ='', eval=F}
library(readr)
library(tidyverse)
library(ggpubr)
library(ggplot2)
library(hrbrthemes)

###20241028 B012-B013 titration

ddpcr_seq <- read_csv("/Users/lwang/Documents/R/ContaQC/20241017_metadata_conta_ddpcr_seq - data w B002new.csv")
view(ddpcr_seq)

df01 <- filter(ddpcr_seq, on_binary_target_coverage_stats_mean >50 & Accepted_droplets > 10000)
df02 <- filter(df01, !is.na(cf_seq))
df03 <- filter(df02,!is.na(Ratio))
titration_df <- subset(df03, Test =="titration")

ggplot(titration_df, aes(x=Ratio, y=cf_seq, col=ExtConsol), size = 5, alpha = 0.5) +
  geom_point(alpha = 0.5, size = 3) +
  geom_smooth(method=lm, level=0.95, size = 1.2, aes()) + 
  scale_x_continuous(trans='log10')+
  scale_y_continuous(trans='log10')+
  geom_abline(linetype = "dashed", size = 0.3) +
  geom_hline(yintercept = 1.0e-03, col = "blue", size = 0.3)+
  stat_regline_equation(label.x= -2.25, label.y= -0.015, col = "blue", size = 3.5) +
  stat_cor(aes(label=..rr.label..),label.x=-2.25, label.y= -0.25, col = "blue", size = 3.5) +
  labs(title = "Concordance: YRS/HBB ratio vs. cf_seq", subtitle =
         "- NA24631 titrated in NA12878; cf_seq cutoff 1.0e-03 in blue line") +
  theme_bw()+ theme(legend.position = "bottom")

ggplot(data = titration_df, aes(x=Sample_name, y = cf_seq)) + geom_boxplot(aes(col=pctMale))+
  geom_jitter(aes(col=pctMale),width=0.15, size = 2, alpha = 0.3) +
  coord_cartesian(ylim = c(1.0e-06, 1)) +
  scale_y_continuous(trans='log10')+
  geom_hline(yintercept = 1.0e-03, col = "black", size = 0.3, linetype = "dashed")+
  labs(title = "NA24631 titrated in NA12878 - cf_seq", subtitle =
         "(cf cutoff 1.0e-03 in dashed line)") +
  theme_ipsum() + theme_bw() + theme(axis.text.x=element_text(angle=45,hjust=1,size=10))

```



<br/>
<br/>



```{r ARC-m5a-plot-2-dorazio-dilution, cache=T, cache.vars='', fig.height= 6, fig.width = 8, fig.cap ="Dorazio Dilution Data"}

par(mar=par("mar") + c(0, 0 , 0, 4))

with(dorazio_dilution_frm,
plot(x = log(dd_tot), y = log(dd_pos),  log='',
     col = concFCol_vec[concF],
     pch = repPch_vec[rep],
    xaxt = 'n', yaxt = 'n', xlab='dd_tot', ylab='dd_pos' )
)
with(dorazio_dilution_frm,
{
axis(side=1, at = pretty(log(dd_tot)), label = round(exp( pretty(log(dd_tot))), 0))
axis(side=2, at = pretty(log(dd_pos)), label = round(exp( pretty(log(dd_pos))), 0))
})

concF_dil_frm_lst <- split(dorazio_dilution_frm, dorazio_dilution_frm$concF)
concF_dil_lm_lst <- lapply(concF_dil_frm_lst, function(frm)
  lm(log(dd_pos) ~  log(dd_tot), data = frm))


for(CC in names(concF_dil_lm_lst)) {
  dil_lm <- concF_dil_lm_lst[[CC]]
  abline(dil_lm, col = concFCol_vec[CC])
}

slopes_vec <- sapply(concF_dil_lm_lst, function(LM) coefficients(LM)[[2]])

old_par =  par(xpd = NA)

legend('topright', inset = c(-.2, 0), title = "ConcF - slope",
legend = paste0(names(slopes_vec), '=', round(slopes_vec, 1)),
text.col = concFCol_vec[names(slopes_vec)], bty='n')

par(old_par)

title("Dilution Data from Dorazio et al. (2015) ")

```

<p><p/>
<span style="color: brown">
* For the binomial distributiuon to be mathematically correct, ...
Confluence page.)  </span>


* Note that the CF is mostly a function of the "adjacent" samples. It is
the proportion of adjacent SNP cites finding their way into the contaminated
sample.
   - these values will not be independent between samples in the same run.

<p><p/>
* Stochasticity:
   - We are examining **all of the molecules** in the sample which
overlap with ContaV3 variants.
   - We can classify each unique molecule  as contaminant or not.
   - The unknown is the level of contamination in the entirety of the
sample.  
   - The current approach assumes that the ContaV3 variants are a random
sample of all variants so that we can use proportions computed on the
ContaV3 variants as estimates of proportions of all variants.
      - this is a convenient assumption which requires validation

<br/>

* [ Conta V3: MCED-I Contamination Detection](https://docs.google.com/presentation/d/1Hr0XL50yPqQ0d1PsV19gig0IHL-MXXAs5vSCtqU2EYo/edit#slide=id.g112ab0dabe1_0_35)

   -   Relies on coverage across 1000 ultra low noise target regions: 
      -  500 pairs of SNPs close to each other
      -  500 long indels in high-complexity sequence context
   -  Theoretical LoD ~2.4 x 10-5, assuming:
      - Aggregate coverage ~500,000
      - Half of markers are homozygous in destination sample
      - Half of contamination fragments will have opposite haplotype
   - One observed event → contamination is certain (factor of 3 for 95% CI)
   - Key learnings from Doppler Conta assessment (same conta probes/targets)
      - Due to (1) lower than expected aggregate coverage (likely due to lower GC content among conta targets) & (2) conservative filtering → theoretical LoD ~8 x 10-5
      - **Empirical LoD estimate observed in titration experiment** is consistent with
theoretical LoD


```{r ARC-m5a-define-plot-fun, cache=T, cache.vars=c("plot_cov_by_batch_f")}
 ############################################
DEBUG <- function() {

   Metric = "bin_cov"
   Title =  "bin_cov"
   Ylim = c(2, 250)

  data_frm = v3a_burn_in_frm %>% 
   dplyr::filter(!is.na(abn_cov), !is.na(bin_cov)) %>%
   dplyr::filter(sample_type == "STUDY")   %>%
   dplyr::mutate(abn_cov = pmax(abn_pos_q05, abn_cov))

   Metric = "abn_cov"
   Title =  "abn_cov"
   Ylim = c(0.001, 2)
   Xaxis = T
   LvlLegend=T
 addControls=F
annotLegend = F

addCancerNotDetected=F

 annotCol = "Plate_24"; annotPch = "sample"
}#DEBUG



plot_cov_by_batch_f <- function(data_frm, Metric, Title ='',  
  Ylim = NULL, Xaxis = F, 
  annotLegend = F, annotCol = "Plate_24", annotPch = "sample",
  CtlLegend = F, LvlLegend = F, addControls = T, addCancerNotDetected=F) {

  data_frm[, "metric"] <- data_frm[, Metric]
  data_frm[, "annotCol"] <- data_frm[, annotCol]
  data_frm[, "annotPch"] <- data_frm[, annotPch]

  split(metric, sub("FC_", '', sub("CCB_", '', sub("ECB_", '',
  paste0(flowcell_no, ':', CCB_no, ':', ECB_no)))))
  

  annotCol_by_ECB_lst <- with(data_frm,
  split(annotCol, sub("FC_", '', sub("CCB_", '', sub("ECB_", '',
  paste0(flowcell_no, ':', CCB_no, ':', ECB_no)))))
  )

  annotPch_by_batch_lst <- with(data_frm,
  split(annotPch, sub("FC_", '', sub("CCB_", '', sub("ECB_", '',
  paste0(flowcell_no, ':', CCB_no, ':', ECB_no)))))
  )


  FC_vec <- sapply(strsplit(names(metric_by_batch_lst), split=':'),'[', 1)
  CCB_vec <- sapply(strsplit(names(metric_by_batch_lst), split=':'),'[', 2)
  ECB_vec <- sapply(strsplit(names(metric_by_batch_lst), split=':'),'[', 3)
  
  FC_ndx <- match(unique(FC_vec), FC_vec)
  CCB_ndx <- match(unique(CCB_vec), CCB_vec)
  ECB_ndx <- match(unique(ECB_vec), ECB_vec)

   # annotCol
  annotColLegend_vec <- sort(unique(data_frm$annotCol))
  annotCol_vec <- col_vector[1:length(annotColLegend_vec)]
  names(annotCol_vec) <- annotColLegend_vec

   # annotPch
  annotPchLegend_vec <- sort(unique(data_frm$annotPch))
  annotPch_vec <- Pch_v[1:length(annotPchLegend_vec)]
  names(annotPch_vec) <- annotPchLegend_vec

  annotPch_vec <- ifelse(annotPch_vec == 8, 13, annotPch_vec)
  annotPch_vec <- ifelse(annotPch_vec == 11, 14, annotPch_vec)

  boxplot(metric_by_batch_lst, las=2, xaxt='n', log='y',  ### ylim=Ylim,
          border=1, col = 0,   # do print the box; a mixture  is ok for bin cov
          staplewex = 0,       # remove horizontal whisker lines
          outline = F,         # remove outlying points
          whisklty = 0,        # remove vertical whisker lines
          staplecol = "white", # just to be totally sure :)
          whiskcol = "white"   # dito)
    )
  x_jit_lst <- lapply(metric_by_batch_lst, function(x) jitter(rep(0, length(x)), amount=0.20))

  title(Title)
  #abline(h = QCM_THRESHOLD["bin_cov"], col='blue')
  abline(v = FC_ndx[-1]-.5, col='grey', lty = 1, lwd=2)
  abline(v = CCB_ndx[-1]-.5, col='grey', lty = 3, lwd=2)
  
  for(JJ in seq_along(metric_by_batch_lst))
  points(x = x_jit_lst[[JJ]] + JJ,
         y = metric_by_batch_lst[[JJ]],
         pch = annotPch_vec[annotPch_by_batch_lst[[JJ]]],
         col = annotCol_vec[annotCol_by_batch_lst[[JJ]]]
  )
  
    # Add controls
  if(addControls)
  for(JJ in seq_along(metric_by_batch_lst))
  points(x = JJ + x_jit_lst[[JJ]],
         y = metric_by_batch_lst[[JJ]],
         pch = controlPch_vec[sampSub_by_batch_lst[[JJ]]],
         col = controlCol_vec[sampSub_by_batch_lst[[JJ]]],
         cex = 0.5
  )
  
  
  
   if(Xaxis) {
old_par =  par(xpd = NA)
     axis(side=1, at = c(-1, ECB_ndx), line = -1, tick=F,
          labels=c("ECB", unique(ECB_vec)), 
          las = 1, cex.axis = 0.8)
     axis(side=1, at = c(-2, CCB_ndx), line = 0, tick=F,
          labels=c("CCB", unique(CCB_vec)), 
          las = 1, cex.axis = 0.8)
     axis(side=1, at = c(-2, FC_ndx), line = 1, tick=F,
          labels=c("FC", unique(FC_vec)), 
          las = 1, cex.axis = 0.8)
     mtext(side=1, line=2.75, "FC : CCB : ECB")
par(old_par)
   }

   if(CtlLegend){
old_par =  par(xpd = NA)
   legend("topright", inset=c(-.15, 0), title="Controls",
   legend = substring(names(controlCol_vec), 1, 3),
   pch = controlPch_vec, col = controlCol_vec, bty='n'
   )
par(old_par)
  }
   if(annotLegend){
old_par =  par(xpd = NA)
   legend("topright", inset=c(-.15, 0), 
   legend = names(annotCol_vec),
   pch = annotPch_vec, 
   col = annotCol_vec, bty='n'
   )
par(old_par)
  }

  return(invisible(metric_by_batch_lst))
}# plot_cov_by_batch_f

``` 


```{r , eval=T}
pander::pander(sessionInfo())
```

  * WRKDIR = `r normalizePath(WRKDIR)`
  * FN = `r FN`
  * Scripts = Scripts
  * RUN DATE = `r date()`

```{r , echo=FALSE}
  knit_exit()
```

<p><p/>
* The first approach explored is to simply look for correlation between
an index of contamination produced by ddPCR with the index of contamination
obtained from the sequencing data, contam.
   - once we have an index of contamination derived from the
ddPR platform, it can be calibrated to the seq platform
contamination index.
      - it would be good to understand how conta is calibrated
and how it performs.
      - it would also be good to understand the ddPCR plaform's
performance profile.

<!--
<p><p/>
* On both platforms, we want to know what fraction of the captured abnormal
fragments are contamitant.  This quantity cannot be computed as we can't 
readily distinguish between native and contaminating abnormal fragments.
-->

<br/>
<br/>


* In this section, we use a standard regression analysis approach to summarize 
the cell line titration data series in a way that should be telling of the
state of operation of the sample processing pipeline.  

<p><p/>
* In the next section we describe a regression approach to
characterizing the salient features of a titration series dataset which


# Glossary {#glossary}

## Collaboration

> the action of working with someone to produce or create something.

* In a collaboration greater success can be achieved than in independent
work, if the collaboration is done correctly.  Collaborations between
scientists and statisticians can be especially fruitful, if they are
done correctly.  

* A scientist working independently on a problem, only to consult a statistician
After finding a solution (at least in their mind),
is not a collaboration in any sense of that word, and no statistician would
be able to provide useful advice without retracing the scientist's steps
back to the point when the problem was conceptualized. 

 ################################
 ################################

are expected to reflect the state of the processing system and the quality of
the geneated data.

* We then apply the approach to 4 randomly selected plates using 4 meeasures of coverage:
   - binary coverage
   - abnormal coverage
   - abnormal content = abn cov/bin cov
   - TMeP_CR = TMeF_CR * 100

<p><p/>
* The results here are consistent with previous findings:
   - TMeP_CR values are propportional to the input amount of cancer cfDNA,
in accordance to a simple model that assumes that in a well 
functioning system - ie. in the absence of high impact event (missing reagent) which could shock the 
system or a sustained disturbance (suboptimal reagents or conditions) preventing homoeostasis -
the yield of captured targets coming from the diluted sample should be reduced proportionally.
   - Amomng the metrics examined, the pattern in the TMeP_CR values is closest to what is predicted by 
such a model, followed by abnormal content, and abnormal coverage.
   - The pattern predicted by the simple model does not fit binary coverage data very well.

<p><p/>
* In section \@ref(process-all-burn-in) we analyze all of the test plates in the burn-in
dataset and tabulate the results.

<p><p/>
* In the last section, the same analyses are performed on the previous dataset with test plates.



<!-- To run
# nohup Rscript -e "knitr::knit2html('_M5A-cf_ddPCR_seq.Rmd')" > _M5A-cf_ddPCR_seq.log  &

# Or
# nohup Rscript -e "rmarkdown::render('_M5A-cf_ddPCR_seq.Rmd')" > _M5A-cf_ddPCR_seq.log  &

