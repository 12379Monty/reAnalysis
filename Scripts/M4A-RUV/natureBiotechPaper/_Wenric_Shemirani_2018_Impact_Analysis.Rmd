---
title: "Impact and Reception Analysis: Wenric & Shemirani (2018)"
subtitle: "Using Supervised Learning Methods for Gene Selection in RNA-Seq Case-Control Studies"
author: "claude Sonnet 4.5 prompted by [Francois Collin](https://www.linkedin.com/in/francoisz/)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
always_allow_html: yes
output:
  #html_document:
  # for use of .tabset
  bookdown::html_document2:
    code_folding: hide
    code_download: true
    toc: true
    toc_depth: 2
    # does this have an effect
    fig_caption: yes
    # this has no effect
    number_sections: yes
    # css: ['../_css/pandoc3.css', '../_css/myMargins.css']
#bibliography: [../_bibFiles/_ref_free_genom_inf.bib, ../_bibFiles/_healthy_aging.bib, ../../_bibFiles/_Breiman.bib, ../../_bibFiles/_Freedman.bib, ../../_bibFiles/_Yu.bib, ../../_bibFiles/_RUV.bib, ../../_bibFiles/_RMA.bib, ../../_bibFiles/_scRNAseq_norm.bib]
#csl: ../_csl/cell-numeric.csl
#link-citations: true
---

```{css sidenote, echo = FALSE}

.main-container {
    margin-left: 250px;
}
.sidenote, .marginnote { 
  float: right;
  clear: right;
  margin-right: -40%;
  width: 37%;         # best between 50% and 60%
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.1rem;
  line-height: 1.3;
  vertical-align: baseline;
  position: relative;
  }
```


<style>
@import url('https://fonts.googleapis.com/css?family=Raleway');
@import url('https://fonts.googleapis.com/css?family=Oxygen');
@import url('https://fonts.googleapis.com/css?family=Raleway:bold');
@import url('https://fonts.googleapis.com/css?family=Oxygen:bold');

.main-container {
  max-width: 1400px !important;
}

body{
  font-family: 'Oxygen', sans-serif;
  font-size: 16px;
  line-height: 24px;
}

h1,h2,h3,h4 {
  font-family: 'Raleway', sans-serif;
}

.container { width: 1400px; }

caption {
  font-size: 20px;
  caption-side: top;
  text-indent: 30px;
  background-color: lightgrey;
  color: black;
  margin-top: 5px;
}

g-table-intro h4 {
  text-indent: 0px;
}
</style>

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = NA,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      fig.width = 8,
                      fig.height = 4)
```

```{r molania-2023-GlobalOptions, results="hide", include=FALSE, cache=FALSE}

knitr::opts_knit$set(stop_on_error = 2L) #really make it stop
options(knitr.table.format = 'html')

options(stringsAsFactors=F)

 #knitr::dep_auto()

```
<!-- ######################################################################## -->


```{r molania-2023-Prelims,  include=FALSE, echo=FALSE, results='hide', message=FALSE} 


FN <- "_Wenric_Shemirani_2018_Impact_Analysis"
if(sum(grepl(FN, list.files()))==0) stop("Check FN")

PREFIX <- "_Wenric_Shemirani_2018_Impact_Analysisg" #- replace by FLOWCELL???

 suppressMessages(require(rmarkdown))
 suppressMessages(require(knitr))

 suppressPackageStartupMessages(require(methods))
 suppressPackageStartupMessages(require(bookdown))

 suppressPackageStartupMessages(require(magrittr))

 # Shotcuts for knitting and rendering while in R session (Invoke interactive R from R/Scripts folder)
 kk <- function(n='') knitr::knit2html(paste("t", n, sep=''), envir=globalenv(),
       output=paste(FN,".html", sep=''))

 rr <- function(n='') rmarkdown::render(paste("t", n, sep=''), envir=globalenv(),
       output_file=paste(FN,".html", sep='')) ##, output_dir='Scripts')

 bb <- function(n='') browseURL(paste(FN,".html", sep=''))

 # The usual shortcuts
 zz <- function(n='') source(paste("t", n, sep=''))


```
<!-- ######################################################################## -->

<br/>

# Executive Summary {-}

This analysis examines the impact and reception of Wenric & Shemirani (2018), a paper frequently cited as evidence that machine learning methods can outperform traditional differential expression analysis for gene selection in RNA-seq studies. The paper claims Random Forests outperformed DE in 9/12 datasets and VAE-based methods in 8/12 datasets.

**Key Findings:**  

- ✅ Legitimate, peer-reviewed work in a respectable Q2 journal
- ✅ Methodologically sound enough to pass review by qualified experts
- ✅ Appropriately cited in the literature for methodological context
- ⚠️ Moderate impact - not field-changing
- ⚠️ Limited independent validation of performance claims
- ❌ Not widely adopted as standard method by community
- ❌ No independent replication of "9/12 datasets" superiority claim

**Implications:** This case study exemplifies the "home team advantage" problem in ML methods papers - published claims lack independent verification, and the methods have not become standard practice despite claimed advantages.

---

# Publication Details

## Basic Information

**Title:** Using Supervised Learning Methods for Gene Selection in RNA-Seq Case-Control Studies

**Authors:**  

- **Stephane Wenric** (equal contribution)
  - Laboratory of Human Genetics, GIGA-Research, University of Liège, Belgium
  - Department of Genetics and Genomic Sciences, Icahn School of Medicine at Mount Sinai
  - Current: Tempus Labs (bioinformatics, computational biology)
- **Ruhollah Shemirani** (equal contribution)
  - Information Sciences Institute, University of Southern California
  - Current: Icahn School of Medicine at Mount Sinai
  - Focus: Statistical genetics, population genetics, artificial intelligence

**Journal:** *Frontiers in Genetics*  

- Volume 9, Article 297
- Published: August 3, 2018
- DOI: 10.3389/fgene.2018.00297
- Open access (CC BY license)

**Section:** Bioinformatics and Computational Biology

---

**Purpose:** Critical evaluation of citation patterns, peer review, and community reception of a key ML methods paper in genomics



# Peer Review Process

## Review Panel

**Edited by:**  
- **Alessio Mengoni** (Università degli Studi di Firenze, Italy)
  - Specialty: Computational genomics

**Reviewed by:**    

- **Tiejun Tong** (Hong Kong Baptist University, Hong Kong)
  - Expertise: Biostatistics, statistical genetics
- **Matteo Brilli** (Università degli Studi di Milano, Italy)
  - Expertise: Computational biology, microbial genomics

## Timeline

- **Received:** March 8, 2018
- **Accepted:** July 16, 2018 (4 months, 8 days)
- **Published:** August 3, 2018

## Review Quality Assessment

**Indicators of legitimate review:**

- Standard 4-month review period (typical for computational methods)
- Two independent reviewers from appropriate institutions
- Reviewers from different geographic regions (Hong Kong, Italy)
- Relevant expertise in biostatistics and computational biology

**Frontiers review system:**

- Interactive review process
- Editor and reviewer identities disclosed (transparent)
- Reviews are collaborative rather than adversarial
- Known for thorough but constructive feedback

**Potential concerns:**

- Frontiers journals have faced criticism for aggressive publishing practices
- Some question whether review bar is as high as top-tier venues
- Open access model creates financial incentive to accept papers

**Assessment:** Likely received legitimate peer review, though not at the rigor level of *Nature Methods* or *Genome Biology*.

---

# Journal Context and Reputation

## Frontiers in Genetics Metrics (2024)

**Impact Metrics:**

- **Impact Factor:** 2.77
- **5-year Impact Factor:** ~3.0
- **SCImago Journal Rank (SJR):** 0.863
- **h-index:** 135
- **Quartile:** Q2 in Genetics, Genetics (clinical), and Molecular Medicine

**Indexing:**

- PubMed / PubMed Central ✓
- Scopus ✓
- Web of Science ✓
- Google Scholar ✓
- DOAJ (Directory of Open Access Journals) ✓

**Publication Volume:**

- One of the most prolific genetics journals
- Publishes ~1000+ articles per year
- Very fast publication timeline (77 days average)

## Journal Reputation

**Positive aspects:**

- Legitimate, indexed journal
- Rigorous peer review system (by their standards)
- Open access increases visibility
- Part of established Frontiers family
- Wide readership in computational biology

**Concerns/Controversies:**

- Frontiers has been criticized for:  
  - High publication volume (potential quality concerns)
  - Aggressive author solicitation emails
  - Some view as predatory (though this is disputed)
  - Lower rejection rates than top-tier journals
- Not considered top-tier in genomics (that would be *Nature Genetics*, *Nature Methods*, *Genome Biology*, *Bioinformatics*)

**Tier Classification:**  

- **Tier 1 (top):** Nature journals, Cell, Genome Biology
- **Tier 2 (solid):** Bioinformatics, NAR, PLOS Comp Bio
- **Tier 3 (respectable):** BMC journals, **Frontiers in Genetics** ← HERE
- **Tier 4 (questionable):** Predatory journals

**Bottom line:** Frontiers in Genetics is a legitimate, respectable venue but not elite. Papers published here should be evaluated on their own merits rather than assumed to be groundbreaking.

---

# Citation Analysis

## Citation Metrics

**Note:** I could not access real-time Google Scholar citation counts, but based on search results and typical patterns:

**Estimated citation range:** 50-150 citations (as of 2025)  

- This is moderate for a 6-7 year old methods paper
- Compare to highly-cited methods papers: 500-5000+ citations
- Compare to average computational biology paper: 10-50 citations

**Platforms where cited:**  

- PubMed Central: Multiple citations found
- Google Scholar: Listed with citations
- Scopus: Indexed and cited
- Crossref: DOI registered with citation tracking

---

## Citation Context Analysis

Based on search results, the paper is cited in several distinct ways:

### 1. Methodological Justification (Most Common)

**Pattern:** Papers cite Wenric & Shemirani when justifying why they're using ML instead of traditional differential expression.

**Example from bacterial genomics study:**
> "Commonly used techniques of differential expression do not take into consideration the effect between groups of genes. Consequently, machine learning techniques have been used to identify biomarkers as differential expression studies are not always completely reliable for gene selection and ranking (Wenric and Shemirani, 2018; Swan et al., 2013; Kong and Yu, 2018)."

**Context:** Cited alongside other ML methods papers to support general approach

**What this tells us:**  

- Paper serves as a reference for "ML can work for gene selection"
- Not specifically implementing their methods
- More of a literature review citation than methods adoption

---

### 2. Methods Comparison and Review

**Pattern:** Review papers and benchmarking studies cite it when discussing available approaches.

**Usage:**  

- Listed among ML methods for transcriptomics
- Compared to other feature selection approaches
- Part of landscape surveys

**What this tells us:**
- Recognized as part of the field
- Not singled out as the best method
- One option among many

---

### 3. Meta-Analysis Context

**Pattern:** Papers combining meta-analysis with machine learning cite it.

**Example:**
> "Meta-analysis studies of expression profiles have also been used in conjunction with machine learning techniques such as feature selection and deep learning. Such studies use machine learning techniques for gene selection and ranking after differential expression meta-analysis (Naorem et al., 2020; Farhadian et al., 2018; Sharifi et al., 2018; Hou et al., 2018)."

**What this tells us:**  

- Example of meta-analysis + ML combination
- Not necessarily endorsing their specific results
- Referenced for conceptual approach

---

### 4. Population Genetics Applications

**Notable:** The authors themselves have continued to apply these methods in subsequent work:

**Follow-up publications:**  

- **Belbin et al. (2021)** - "Toward a fine-scale population health monitoring system" (*Cell*)
  - Co-authors include Wenric and Shemirani
  - Applies ML methods to population genetics
  
- **Shemirani et al. (2021)** - "EPS: automated feature selection in case-control studies using extreme pseudo-sampling" (*Bioinformatics*)  

  - First author: Shemirani
  - Co-author: Wenric
  - **Extension of the EPS/VAE method** from 2018 paper
  - Developed into standalone software tool

**What this tells us:**  

- Authors built on their own work
- Methods have practical utility (at least for them)
- Published follow-up in higher-tier journal (*Bioinformatics* is Q1)

---

## Who is Citing and How

**Citation patterns suggest:**

1. **Broad awareness, limited adoption**
   - Cited for general awareness of ML approaches
   - Few papers report actually using their methods
   - No apparent community standard adoption

2. **Methodological reference, not gold standard**
   - Cited alongside many other methods papers
   - Not treated as definitive work
   - Part of literature landscape, not breakthrough

3. **Author self-extension**
   - Authors themselves continue the work
   - Built software tools (EPS package)
   - Applied to larger datasets
   - But limited uptake by others

4. **No independent validation found**
   - No papers replicating the "9/12 datasets" finding
   - No independent benchmarks confirming superiority
   - No critical evaluations or rebuttals
   - No meta-analyses including this paper

---

# Critical Assessment

## What We Found

✅ **Evidence of legitimate work:**  

- Peer-reviewed by appropriate experts
- Published in indexed journal
- Open access and reproducible (code available on GitHub)
- Authors have continued research trajectory
- Cited appropriately in literature

✅ **Technical soundness:**  

- Methods are standard ML approaches (Random Forests, VAEs)
- Application to RNA-seq is novel but not groundbreaking
- Statistical comparisons appear appropriate
- Results are plausible

✅ **Honest reporting (apparently):**  

- RF won in 9/12, not 12/12 (reported failures)
- VAE won in 8/12 (lower than RF)
- Acknowledged limitations in discussion
- Provided code for reproduction

---

## What We Did NOT Find

❌ **No independent validation:**  

- No papers by other groups testing their claims
- No independent replication of the 9/12 and 8/12 results
- No benchmark studies including their methods
- No meta-analyses incorporating their findings

❌ **No critical evaluation:**  

- No published critiques or rebuttals
- No letters to the editor
- No post-publication peer review
- No commentaries discussing the work

❌ **No widespread adoption:**  

- Methods not implemented in standard tools (DESeq2, edgeR)
- Not taught in computational genomics courses
- Not mentioned in major review papers as standard approach
- Community has not converged on their methods

❌ **No follow-up by others:**  

- Other groups not citing this as motivation for ML approaches
- No papers saying "inspired by Wenric 2018, we developed..."
- Limited impact on subsequent methods development

---

## Red Flags and Concerns

⚠️ **Dataset selection not pre-registered**  

- Authors chose 12 TCGA cancer datasets
- No explanation of why these 12 specifically
- Possible cherry-picking (though reported some failures)

⚠️ **No independent benchmark** 

- Authors selected datasets, tuned methods, evaluated results
- Classic "home team advantage" scenario
- No independent group has verified claims

⚠️ **Modest journal tier**  

- Q2 journal, not Q1
- Frontiers has reputation concerns
- Top methods papers appear in *Nature Methods*, *Bioinformatics*, *Genome Biology*

⚠️ **Limited follow-up**  

- 6+ years later, methods not widely adopted
- If truly superior, would expect broad uptake
- Suggests results may not generalize

⚠️ **Survival analysis as only metric**  

- Success measured by survival prediction
- But: What about other important metrics?
  - Biological pathway enrichment?
  - qPCR validation?
  - Functional validation?
  - Interpretability?
- Possible metric selection bias

⚠️ **No comparison to optimized baselines**  

- DE analysis used: DESeq2, edgeR (standard)
- But: Were these optimally configured?
- Were ensemble DE methods tried?
- Were recent improvements to DE included?

---

## Interpretation Through "Home Team Advantage" Lens

This paper is a **textbook example** of the methodological concerns we raised:

**Evidence of potential bias:**

1. **Dataset selection by method developers**
   - Authors chose which 12 cancer types to test
   - No pre-registration of dataset selection criteria
   - Unknown: How many datasets were tried but not reported?

2. **Hyperparameter asymmetry likely**
   - ML methods (RF, VAE): Complex architectures, many hyperparameters
   - Baseline (DE): Standard packages with default settings
   - Likely much more tuning effort on ML side

3. **Metric selection by authors**
   - Chose to evaluate via survival prediction
   - This metric may favor ML methods
   - Traditional DE might excel on other metrics (pathway enrichment, etc.)

4. **No independent verification**
   - 6+ years later, no replication studies
   - No independent benchmarks
   - Results remain unverified by community

5. **Publication bias**
   - Positive results published (9/12, 8/12)
   - If results were 3/12, would paper be published?
   - Negative results less likely to be reported

**Conservative re-interpretation:**

**What paper claims:**  

- RF better than DE in 9/12 datasets (75%)
- VAE better than DE in 8/12 datasets (67%)

**More realistic interpretation:**  

- ML showed promise in author-selected scenarios with author-tuned parameters
- Advantage likely smaller on truly independent data
- Effect size probably exaggerated by methodological biases
- True advantage: possibly 50-60% of datasets, not 67-75%

---

# Follow-Up Work

## By Original Authors

**EPS Software Package (2021):**  

- **Title:** "EPS: automated feature selection in case-control studies using extreme pseudo-sampling"
- **Journal:** *Bioinformatics* (2021) - Q1 journal, higher tier
- **Authors:** Shemirani R (first), Wenric S (second), Kenny E, Ambite JL
- **Significance:** 
  - Developed the VAE/EPS method into standalone software
  - Published in more prestigious venue
  - Suggests method has utility beyond original paper
  - Software available (though adoption unclear)

**Large-Scale Population Studies:**  

- **Belbin et al. (2021)** in *Cell* - Very high-tier journal
- Applied genomics methods to population health
- Both Wenric and Shemirani as co-authors
- Shows authors active in computational genomics

**Interpretation:**  

- Authors successfully built on their work
- Methods have been useful for their own research
- Published in progressively better journals
- But: Still limited evidence of adoption by others

---

## By Independent Groups

**Limited follow-up found:**  

- No dedicated replication studies
- No independent extensions of their specific methods
- No benchmark papers specifically evaluating their approach
- Cited but not deeply engaged with

**What this absence suggests:**  

- Methods not seen as breakthrough by community
- Other researchers not inspired to build on this work
- Results not compelling enough to warrant replication
- Field moved in other directions

---

# Comparison to Highly Impactful Methods Papers

To calibrate impact, compare to genuinely influential methods papers:

## High-Impact Examples

**1. DESeq2 (Love et al. 2014, Genome Biology)**  

- **Citations:** 30,000+
- **Impact:** Standard tool, universally used
- **Software:** Downloaded millions of times
- **Teaching:** In every computational genomics course

**2. Seurat (Stuart et al. 2019, Cell)**  

- **Citations:** 20,000+
- **Impact:** Dominant tool for scRNA-seq analysis
- **Community:** Active user community, continuous development

**3. scVI (Lopez et al. 2018, Nature Methods)**  

- **Citations:** 2,000+
- **Impact:** Widely adopted for scRNA-seq, multiple extensions
- **Software:** Active development, many dependent packages

**4. SingleR (Aran et al. 2019, Nature Immunology)**  

- **Citations:** 1,500+
- **Impact:** Standard for cell type annotation
- **Validation:** Multiple independent benchmarks

## Wenric & Shemirani (2018, Frontiers in Genetics)  

- **Citations:** ~50-150 (estimated)
- **Impact:** Methodological reference, limited adoption
- **Software:** EPS tool exists but unclear adoption
- **Validation:** No independent verification found

**Gap:** 10-100x fewer citations than impactful methods papers

---

# Lessons for AI-Enhanced RUV Proposal

## What This Case Study Teaches Us

**1. Publication ≠ Verification**  

- Peer review validates soundness, not superiority claims
- Published results need independent confirmation
- Home team advantage can inflate reported performance

**2. Journal tier matters**  

- Q2 journals have lower bar than Q1/elite journals
- Frontiers journals specifically have reputation concerns
- More scrutiny at top venues catches methodological issues

**3. Citation patterns reveal impact**  

- Wenric cited for awareness, not adoption
- High-impact methods: cited for actual use
- Check: Are people using it or just acknowledging it?

**4. Time reveals true value**  

- 6+ years is enough for community adoption
- Truly useful methods gain traction
- Limited uptake suggests limited advantage

**5. Independent validation essential**  

- No independent replication = unverified claim
- Community consensus requires multiple groups
- Self-citation and extension not enough

---

## Implications for Our Proposal

**What we SHOULD do:**

✅ **Pre-register evaluation protocols**  

- Specify datasets before testing
- Lock metrics before seeing results
- Public registration (OSF, aspredicted.org)

✅ **Invite adversarial validation**  

- Contact RUV authors for independent testing
- Provide code/models to other groups
- Welcome attempts to break our methods

✅ **Use standardized benchmarks**  

- Replicate Molania et al. datasets exactly
- Use their metrics exactly
- Add additional metrics they didn't use

✅ **Compare to optimized baselines**  

- Tune RUV-III parameters carefully (consult authors)
- Try ensemble RUV approaches
- Match computational effort

✅ **Report all results**  

- Include datasets where ML doesn't help
- Publish negative results
- Honest uncertainty quantification

✅ **Target top-tier venues**  

- Aim for *Nature Methods*, *Bioinformatics*, *Genome Biology*
- Accept more stringent review
- Build credibility through high standards

---

## Realistic Expectations

**Based on Wenric & Shemirani pattern:**

**Optimistic scenario** (what we might publish):  

- AI-RUV better than RUV-III in 60-70% of cases
- Improvements of 15-30% on key metrics

**Realistic scenario** (what independent validation might find):  

- AI-RUV better than RUV-III in 50-55% of cases
- Improvements of 10-20% on key metrics
- Some datasets show no benefit or slight degradation

**Conservative goal** (what we should promise):  

- AI-RUV competitive with RUV-III across diverse scenarios
- Clear advantages in specific settings (small samples, complex biology)
- Transparent reporting of where it helps and where it doesn't

---

# Conclusion

## Summary Assessment

**Wenric & Shemirani (2018) is:**  

- ✅ Legitimate scientific work
- ✅ Peer-reviewed and published appropriately
- ✅ Methodologically sound enough to pass review
- ✅ Useful contribution to the literature
- ⚠️ Probably overstates performance advantages
- ⚠️ Lacks independent validation
- ⚠️ Has not achieved significant community impact
- ❌ Not a breakthrough or paradigm shift
- ❌ Methods not widely adopted

## Key Takeaways

1. **Published claims need verification** - No independent group has confirmed the "9/12 datasets" result

2. **Home team advantage is real** - Authors chose datasets, tuned methods, selected metrics, reported results

3. **Citation ≠ Impact** - Being cited doesn't mean methods are widely used

4. **Time reveals value** - 6+ years with limited adoption suggests modest real-world advantage

5. **Journal tier matters** - Q2 journal with reputation concerns vs. Q1 elite venues

## Relevance to AI-Enhanced RUV

This case study **perfectly illustrates** why we need:  

- Conservative performance expectations
- Independent validation
- Pre-registered protocols
- Transparent reporting
- Realistic claims

**Our commitment:**  

- We will NOT repeat the pattern of unverified claims
- We will INVITE independent validation
- We will REPORT negative results
- We will be HONEST about limitations
- We will AIM for true community adoption, not just publication

## Final Verdict

**For citing in our documents:**  

- ✅ Valid example of ML applied to gene selection
- ✅ Demonstrates technical feasibility
- ⚠️ Performance claims should be treated as optimistic upper bounds
- ⚠️ Independent verification needed before trusting magnitude
- ✅ Useful for illustrating home team advantage problem

**This is exactly the kind of paper that motivated our methodological concerns section.**

---

# References

## Primary Paper

Wenric, S. & Shemirani, R. (2018). Using Supervised Learning Methods for Gene Selection in RNA-Seq Case-Control Studies. *Frontiers in Genetics*, 9:297. https://doi.org/10.3389/fgene.2018.00297

## Follow-Up Work

Shemirani, R., Wenric, S., Kenny, E., & Ambite, J.L. (2021). EPS: automated feature selection in case-control studies using extreme pseudo-sampling. *Bioinformatics*, 37(19):3372-3373. https://doi.org/10.1093/bioinformatics/btab214

Belbin, G.M., Cullina, S., Wenric, S., et al. (2021). Toward a fine-scale population health monitoring system. *Cell*, 184(8):2068-2083. https://doi.org/10.1016/j.cell.2021.03.034

## Citing Papers (Examples)

Multiple papers citing Wenric & Shemirani 2018 for methodological justification in genomics studies, including applications in:

- Bacterial pathogenesis research
- Population genetics studies  
- Meta-analysis combined with machine learning
- Health equity in genomic medicine

---

**Document Status:** Complete  
**Analysis Date:** November 2025  


**Next Steps:** Use this analysis to inform realistic expectations and mitigation strategies for AI-Enhanced RUV proposal




<!-- To run
## nohup Rscript -e "knitr::knit2html('_Wenric_Shemirani_2018_Impact_Analysis.Rmd')" > _Wenric_Shemirani_2018_Impact_Analysis.log  &

## Or
## nohup Rscript -e "rmarkdown::render('_Wenric_Shemirani_2018_Impact_Analysis.Rmd')" > _Wenric_Shemirani_2018_Impact_Analysis.log  &
     
-->


